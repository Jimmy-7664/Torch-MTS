PEMS08
Trainset:	x-(10700, 12, 170, 1)	y-(10700, 12, 170, 2)
Valset:  	x-(3567, 12, 170, 1)  	y-(3567, 12, 170, 2)
Testset:	x-(3566, 12, 170, 1)	y-(3566, 12, 170, 2)

--------- MegaCRN ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "y_time_of_day": true,
    "runner": "megacrn",
    "loss": "megacrn",
    "loss_args": {
        "l1": 0.01,
        "l2": 0.01
    },
    "lr": 0.01,
    "eps": 0.001,
    "milestones": [
        50,
        100
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 30,
    "model_args": {
        "num_nodes": 170,
        "input_dim": 1,
        "output_dim": 1,
        "horizon": 12,
        "rnn_units": 64,
        "num_layers": 1,
        "cheb_k": 3,
        "ycov_dim": 1,
        "mem_num": 20,
        "mem_dim": 64,
        "tf_decay_steps": 2000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MegaCRN                                  [64, 12, 170, 1]          12,176
├─ADCRNN_Encoder: 1-1                    [64, 12, 170, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─AGCRNCell: 3-1               [64, 170, 64]             75,072
│    │    └─AGCRNCell: 3-2               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-3               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-4               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-5               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-6               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-7               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-8               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-9               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-10              [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-11              [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-12              [64, 170, 64]             (recursive)
├─ADCRNN_Decoder: 1-2                    [64, 170, 128]            --
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-13              [64, 170, 128]            299,904
├─Sequential: 1-3                        [64, 170, 1]              --
│    └─Linear: 2-3                       [64, 170, 1]              129
├─ADCRNN_Decoder: 1-4                    [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-14              [64, 170, 128]            (recursive)
├─Sequential: 1-5                        [64, 170, 1]              (recursive)
│    └─Linear: 2-5                       [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-6                    [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-15              [64, 170, 128]            (recursive)
├─Sequential: 1-7                        [64, 170, 1]              (recursive)
│    └─Linear: 2-7                       [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-8                    [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-16              [64, 170, 128]            (recursive)
├─Sequential: 1-9                        [64, 170, 1]              (recursive)
│    └─Linear: 2-9                       [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-10                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-17              [64, 170, 128]            (recursive)
├─Sequential: 1-11                       [64, 170, 1]              (recursive)
│    └─Linear: 2-11                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-12                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-18              [64, 170, 128]            (recursive)
├─Sequential: 1-13                       [64, 170, 1]              (recursive)
│    └─Linear: 2-13                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-14                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-19              [64, 170, 128]            (recursive)
├─Sequential: 1-15                       [64, 170, 1]              (recursive)
│    └─Linear: 2-15                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-16                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-20              [64, 170, 128]            (recursive)
├─Sequential: 1-17                       [64, 170, 1]              (recursive)
│    └─Linear: 2-17                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-18                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-21              [64, 170, 128]            (recursive)
├─Sequential: 1-19                       [64, 170, 1]              (recursive)
│    └─Linear: 2-19                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-20                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-22              [64, 170, 128]            (recursive)
├─Sequential: 1-21                       [64, 170, 1]              (recursive)
│    └─Linear: 2-21                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-22                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-23              [64, 170, 128]            (recursive)
├─Sequential: 1-23                       [64, 170, 1]              (recursive)
│    └─Linear: 2-23                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-24                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-24              [64, 170, 128]            (recursive)
├─Sequential: 1-25                       [64, 170, 1]              (recursive)
│    └─Linear: 2-25                      [64, 170, 1]              (recursive)
==========================================================================================
Total params: 387,281
Trainable params: 387,281
Non-trainable params: 0
Total mult-adds (G): 48.88
==========================================================================================
Input size (MB): 1.04
Forward/backward pass size (MB): 50.22
Params size (MB): 1.50
Estimated Total Size (MB): 52.77
==========================================================================================

Loss: MegaCRNLoss

2023-10-18 21:06:02.914056 Epoch 1  	Train Loss = 22.12792 Val Loss = 30.73342
2023-10-18 21:06:24.558444 Epoch 2  	Train Loss = 15.01432 Val Loss = 29.77107
2023-10-18 21:06:46.103692 Epoch 3  	Train Loss = 14.59645 Val Loss = 27.94703
2023-10-18 21:07:08.184838 Epoch 4  	Train Loss = 14.59761 Val Loss = 24.64472
2023-10-18 21:07:28.535160 Epoch 5  	Train Loss = 14.33170 Val Loss = 20.18781
2023-10-18 21:07:48.974200 Epoch 6  	Train Loss = 14.13642 Val Loss = 21.20078
2023-10-18 21:08:09.367766 Epoch 7  	Train Loss = 13.95242 Val Loss = 20.63229
2023-10-18 21:08:30.599146 Epoch 8  	Train Loss = 13.98803 Val Loss = 33.79450
2023-10-18 21:08:52.613677 Epoch 9  	Train Loss = 13.87927 Val Loss = 19.56593
2023-10-18 21:09:12.747290 Epoch 10  	Train Loss = 13.84265 Val Loss = 31.84890
2023-10-18 21:09:31.397266 Epoch 11  	Train Loss = 13.72579 Val Loss = 21.94290
2023-10-18 21:09:50.472462 Epoch 12  	Train Loss = 13.50105 Val Loss = 19.71436
2023-10-18 21:10:10.945337 Epoch 13  	Train Loss = 13.18893 Val Loss = 21.26571
2023-10-18 21:10:32.634492 Epoch 14  	Train Loss = 13.47419 Val Loss = 20.06771
2023-10-18 21:10:54.343708 Epoch 15  	Train Loss = 13.26275 Val Loss = 18.83706
2023-10-18 21:11:14.579269 Epoch 16  	Train Loss = 13.20194 Val Loss = 17.73340
2023-10-18 21:11:35.158106 Epoch 17  	Train Loss = 13.21457 Val Loss = 23.39815
2023-10-18 21:11:55.151316 Epoch 18  	Train Loss = 13.12146 Val Loss = 27.19367
2023-10-18 21:12:14.344340 Epoch 19  	Train Loss = 13.09751 Val Loss = 22.89984
2023-10-18 21:12:34.768906 Epoch 20  	Train Loss = 13.07833 Val Loss = 20.10484
2023-10-18 21:12:56.671079 Epoch 21  	Train Loss = 13.08287 Val Loss = 17.56297
2023-10-18 21:13:15.281771 Epoch 22  	Train Loss = 12.76452 Val Loss = 17.66806
2023-10-18 21:13:34.012761 Epoch 23  	Train Loss = 12.81208 Val Loss = 18.01325
2023-10-18 21:13:52.529017 Epoch 24  	Train Loss = 12.63298 Val Loss = 17.72456
2023-10-18 21:14:12.012785 Epoch 25  	Train Loss = 12.75660 Val Loss = 19.97489
2023-10-18 21:14:31.705199 Epoch 26  	Train Loss = 12.68777 Val Loss = 17.57411
2023-10-18 21:14:52.132629 Epoch 27  	Train Loss = 12.64104 Val Loss = 16.54075
2023-10-18 21:15:12.099951 Epoch 28  	Train Loss = 12.71159 Val Loss = 17.14780
2023-10-18 21:15:32.085634 Epoch 29  	Train Loss = 12.73919 Val Loss = 18.59923
2023-10-18 21:15:51.950458 Epoch 30  	Train Loss = 12.55535 Val Loss = 18.78460
2023-10-18 21:16:12.457314 Epoch 31  	Train Loss = 12.52478 Val Loss = 19.50336
2023-10-18 21:16:33.018004 Epoch 32  	Train Loss = 12.57250 Val Loss = 16.28061
2023-10-18 21:16:54.333723 Epoch 33  	Train Loss = 12.40441 Val Loss = 18.76953
2023-10-18 21:17:14.254224 Epoch 34  	Train Loss = 12.49647 Val Loss = 17.19429
2023-10-18 21:17:34.677930 Epoch 35  	Train Loss = 12.18886 Val Loss = 16.18510
2023-10-18 21:17:55.054618 Epoch 36  	Train Loss = 12.30340 Val Loss = 16.44212
2023-10-18 21:18:15.513477 Epoch 37  	Train Loss = 12.23389 Val Loss = 17.36877
2023-10-18 21:18:36.024504 Epoch 38  	Train Loss = 12.23253 Val Loss = 16.28592
2023-10-18 21:18:57.318506 Epoch 39  	Train Loss = 12.29610 Val Loss = 16.86107
2023-10-18 21:19:18.124792 Epoch 40  	Train Loss = 12.20378 Val Loss = 16.42852
2023-10-18 21:19:37.485908 Epoch 41  	Train Loss = 12.10897 Val Loss = 16.72708
2023-10-18 21:19:57.222054 Epoch 42  	Train Loss = 12.07988 Val Loss = 16.18181
2023-10-18 21:20:16.865482 Epoch 43  	Train Loss = 12.17386 Val Loss = 16.23745
2023-10-18 21:20:38.580148 Epoch 44  	Train Loss = 12.18075 Val Loss = 15.95921
2023-10-18 21:20:59.327830 Epoch 45  	Train Loss = 12.15415 Val Loss = 16.80511
2023-10-18 21:21:20.689284 Epoch 46  	Train Loss = 12.20149 Val Loss = 16.47324
2023-10-18 21:21:42.205712 Epoch 47  	Train Loss = 12.11776 Val Loss = 18.14832
2023-10-18 21:22:02.517849 Epoch 48  	Train Loss = 11.95937 Val Loss = 16.22354
2023-10-18 21:22:23.129064 Epoch 49  	Train Loss = 11.99517 Val Loss = 20.52505
2023-10-18 21:22:45.086869 Epoch 50  	Train Loss = 12.00522 Val Loss = 16.30120
2023-10-18 21:23:06.767886 Epoch 51  	Train Loss = 11.66098 Val Loss = 15.45526
2023-10-18 21:23:26.357685 Epoch 52  	Train Loss = 11.61521 Val Loss = 15.48676
2023-10-18 21:23:45.521945 Epoch 53  	Train Loss = 11.59346 Val Loss = 15.56830
2023-10-18 21:24:06.563459 Epoch 54  	Train Loss = 11.58445 Val Loss = 15.45269
2023-10-18 21:24:28.125297 Epoch 55  	Train Loss = 11.56589 Val Loss = 15.56616
2023-10-18 21:24:49.852671 Epoch 56  	Train Loss = 11.56001 Val Loss = 15.67570
2023-10-18 21:25:09.361438 Epoch 57  	Train Loss = 11.55010 Val Loss = 15.68382
2023-10-18 21:25:29.167677 Epoch 58  	Train Loss = 11.53994 Val Loss = 15.59609
2023-10-18 21:25:50.826477 Epoch 59  	Train Loss = 11.55428 Val Loss = 15.41667
2023-10-18 21:26:11.273092 Epoch 60  	Train Loss = 11.54104 Val Loss = 15.44668
2023-10-18 21:26:33.405630 Epoch 61  	Train Loss = 11.52998 Val Loss = 15.50730
2023-10-18 21:26:55.486216 Epoch 62  	Train Loss = 11.53594 Val Loss = 15.50880
2023-10-18 21:27:17.666903 Epoch 63  	Train Loss = 11.53537 Val Loss = 15.45010
2023-10-18 21:27:39.005802 Epoch 64  	Train Loss = 11.54618 Val Loss = 15.59912
2023-10-18 21:28:00.424347 Epoch 65  	Train Loss = 11.56054 Val Loss = 15.61217
2023-10-18 21:28:20.285916 Epoch 66  	Train Loss = 11.53697 Val Loss = 15.56913
2023-10-18 21:28:42.185540 Epoch 67  	Train Loss = 11.55264 Val Loss = 15.50410
2023-10-18 21:29:03.064062 Epoch 68  	Train Loss = 11.56689 Val Loss = 15.49038
2023-10-18 21:29:24.480405 Epoch 69  	Train Loss = 11.56971 Val Loss = 15.54467
2023-10-18 21:29:44.879654 Epoch 70  	Train Loss = 11.56166 Val Loss = 15.50986
2023-10-18 21:30:04.514410 Epoch 71  	Train Loss = 11.57181 Val Loss = 15.50573
2023-10-18 21:30:24.713250 Epoch 72  	Train Loss = 11.56485 Val Loss = 15.66323
2023-10-18 21:30:45.623057 Epoch 73  	Train Loss = 11.61913 Val Loss = 15.94084
2023-10-18 21:31:06.741673 Epoch 74  	Train Loss = 11.59577 Val Loss = 15.52730
2023-10-18 21:31:27.900652 Epoch 75  	Train Loss = 11.60615 Val Loss = 15.53808
2023-10-18 21:31:47.803155 Epoch 76  	Train Loss = 11.64448 Val Loss = 15.47155
2023-10-18 21:32:07.630049 Epoch 77  	Train Loss = 11.64772 Val Loss = 15.63031
2023-10-18 21:32:26.698632 Epoch 78  	Train Loss = 11.68182 Val Loss = 15.53151
2023-10-18 21:32:46.681938 Epoch 79  	Train Loss = 11.72780 Val Loss = 15.56969
2023-10-18 21:33:04.887941 Epoch 80  	Train Loss = 11.72953 Val Loss = 15.47816
2023-10-18 21:33:25.250024 Epoch 81  	Train Loss = 11.72380 Val Loss = 15.46356
2023-10-18 21:33:45.174007 Epoch 82  	Train Loss = 11.77960 Val Loss = 15.57518
2023-10-18 21:34:05.027382 Epoch 83  	Train Loss = 11.76880 Val Loss = 15.53037
2023-10-18 21:34:25.077658 Epoch 84  	Train Loss = 11.81711 Val Loss = 15.58219
2023-10-18 21:34:44.852867 Epoch 85  	Train Loss = 11.86909 Val Loss = 15.44933
2023-10-18 21:35:03.898000 Epoch 86  	Train Loss = 11.82902 Val Loss = 15.63062
2023-10-18 21:35:23.214448 Epoch 87  	Train Loss = 11.89161 Val Loss = 15.43807
2023-10-18 21:35:42.537969 Epoch 88  	Train Loss = 11.91253 Val Loss = 15.55436
2023-10-18 21:36:02.347633 Epoch 89  	Train Loss = 11.89709 Val Loss = 15.61851
Early stopping at epoch: 89
Best at epoch 59:
Train Loss = 11.55428
Train RMSE = 23.14456, MAE = 13.59124, MAPE = 8.68108
Val Loss = 15.41667
Val RMSE = 26.17949, MAE = 15.61298, MAPE = 13.47109
--------- Test ---------
All Steps RMSE = 23.88451, MAE = 14.90846, MAPE = 9.35059
Step 1 RMSE = 19.45961, MAE = 12.50040, MAPE = 7.97025
Step 2 RMSE = 20.95250, MAE = 13.29867, MAPE = 8.40954
Step 3 RMSE = 21.96405, MAE = 13.85025, MAPE = 8.69250
Step 4 RMSE = 22.74148, MAE = 14.26472, MAPE = 8.93342
Step 5 RMSE = 23.40315, MAE = 14.62403, MAPE = 9.14802
Step 6 RMSE = 23.94586, MAE = 14.92446, MAPE = 9.34522
Step 7 RMSE = 24.44224, MAE = 15.20848, MAPE = 9.52625
Step 8 RMSE = 24.88132, MAE = 15.49876, MAPE = 9.70770
Step 9 RMSE = 25.30501, MAE = 15.76107, MAPE = 9.86764
Step 10 RMSE = 25.73924, MAE = 16.05285, MAPE = 10.03516
Step 11 RMSE = 26.11160, MAE = 16.30679, MAPE = 10.18824
Step 12 RMSE = 26.57231, MAE = 16.61105, MAPE = 10.38303
Inference time: 2.11 s
