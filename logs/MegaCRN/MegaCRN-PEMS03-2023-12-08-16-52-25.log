PEMS03
Trainset:	x-(15711, 12, 358, 1)	y-(15711, 12, 358, 2)
Valset:  	x-(5237, 12, 358, 1)  	y-(5237, 12, 358, 2)
Testset:	x-(5237, 12, 358, 1)	y-(5237, 12, 358, 2)

--------- MegaCRN ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "y_time_of_day": true,
    "runner": "megacrn",
    "loss": "megacrn",
    "loss_args": {
        "l1": 0.01,
        "l2": 0.01
    },
    "lr": 0.01,
    "eps": 0.001,
    "milestones": [
        50,
        100
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 30,
    "model_args": {
        "num_nodes": 358,
        "input_dim": 1,
        "output_dim": 1,
        "horizon": 12,
        "rnn_units": 64,
        "num_layers": 1,
        "cheb_k": 3,
        "ycov_dim": 1,
        "mem_num": 20,
        "mem_dim": 64,
        "tf_decay_steps": 2000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MegaCRN                                  [64, 12, 358, 1]          19,696
├─ADCRNN_Encoder: 1-1                    [64, 12, 358, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─AGCRNCell: 3-1               [64, 358, 64]             75,072
│    │    └─AGCRNCell: 3-2               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-3               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-4               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-5               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-6               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-7               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-8               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-9               [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-10              [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-11              [64, 358, 64]             (recursive)
│    │    └─AGCRNCell: 3-12              [64, 358, 64]             (recursive)
├─ADCRNN_Decoder: 1-2                    [64, 358, 128]            --
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-13              [64, 358, 128]            299,904
├─Sequential: 1-3                        [64, 358, 1]              --
│    └─Linear: 2-3                       [64, 358, 1]              129
├─ADCRNN_Decoder: 1-4                    [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-14              [64, 358, 128]            (recursive)
├─Sequential: 1-5                        [64, 358, 1]              (recursive)
│    └─Linear: 2-5                       [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-6                    [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-15              [64, 358, 128]            (recursive)
├─Sequential: 1-7                        [64, 358, 1]              (recursive)
│    └─Linear: 2-7                       [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-8                    [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-16              [64, 358, 128]            (recursive)
├─Sequential: 1-9                        [64, 358, 1]              (recursive)
│    └─Linear: 2-9                       [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-10                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-17              [64, 358, 128]            (recursive)
├─Sequential: 1-11                       [64, 358, 1]              (recursive)
│    └─Linear: 2-11                      [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-12                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-18              [64, 358, 128]            (recursive)
├─Sequential: 1-13                       [64, 358, 1]              (recursive)
│    └─Linear: 2-13                      [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-14                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-19              [64, 358, 128]            (recursive)
├─Sequential: 1-15                       [64, 358, 1]              (recursive)
│    └─Linear: 2-15                      [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-16                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-20              [64, 358, 128]            (recursive)
├─Sequential: 1-17                       [64, 358, 1]              (recursive)
│    └─Linear: 2-17                      [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-18                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-21              [64, 358, 128]            (recursive)
├─Sequential: 1-19                       [64, 358, 1]              (recursive)
│    └─Linear: 2-19                      [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-20                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-22              [64, 358, 128]            (recursive)
├─Sequential: 1-21                       [64, 358, 1]              (recursive)
│    └─Linear: 2-21                      [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-22                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-23              [64, 358, 128]            (recursive)
├─Sequential: 1-23                       [64, 358, 1]              (recursive)
│    └─Linear: 2-23                      [64, 358, 1]              (recursive)
├─ADCRNN_Decoder: 1-24                   [64, 358, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-24              [64, 358, 128]            (recursive)
├─Sequential: 1-25                       [64, 358, 1]              (recursive)
│    └─Linear: 2-25                      [64, 358, 1]              (recursive)
==========================================================================================
Total params: 394,801
Trainable params: 394,801
Non-trainable params: 0
Total mult-adds (G): 102.94
==========================================================================================
Input size (MB): 2.20
Forward/backward pass size (MB): 1269.14
Params size (MB): 1.50
Estimated Total Size (MB): 1272.84
==========================================================================================

Loss: MegaCRNLoss

2023-12-08 16:53:58.512392 Epoch 1  	Train Loss = 18.52183 Val Loss = 33.97952
2023-12-08 16:55:27.899672 Epoch 2  	Train Loss = 14.12512 Val Loss = 27.40855
2023-12-08 16:56:57.588961 Epoch 3  	Train Loss = 13.81342 Val Loss = 20.12074
2023-12-08 16:58:27.489865 Epoch 4  	Train Loss = 13.65882 Val Loss = 18.67447
2023-12-08 16:59:57.316280 Epoch 5  	Train Loss = 13.47798 Val Loss = 18.13192
2023-12-08 17:01:27.304719 Epoch 6  	Train Loss = 13.20658 Val Loss = 17.81737
2023-12-08 17:02:57.414595 Epoch 7  	Train Loss = 13.22544 Val Loss = 25.24430
2023-12-08 17:04:27.874771 Epoch 8  	Train Loss = 13.00503 Val Loss = 22.75698
2023-12-08 17:05:58.524237 Epoch 9  	Train Loss = 12.90859 Val Loss = 19.58572
2023-12-08 17:07:28.957013 Epoch 10  	Train Loss = 12.85576 Val Loss = 17.05889
2023-12-08 17:08:59.379873 Epoch 11  	Train Loss = 12.58167 Val Loss = 23.66401
2023-12-08 17:10:30.041374 Epoch 12  	Train Loss = 12.18789 Val Loss = 16.68682
2023-12-08 17:12:00.402864 Epoch 13  	Train Loss = 11.98354 Val Loss = 18.89480
2023-12-08 17:13:30.755333 Epoch 14  	Train Loss = 11.91203 Val Loss = 15.63778
2023-12-08 17:15:01.002695 Epoch 15  	Train Loss = 12.00009 Val Loss = 22.35117
2023-12-08 17:16:31.146880 Epoch 16  	Train Loss = 11.98404 Val Loss = 19.42145
2023-12-08 17:18:01.746759 Epoch 17  	Train Loss = 11.82444 Val Loss = 18.84074
2023-12-08 17:19:31.910261 Epoch 18  	Train Loss = 11.71856 Val Loss = 15.37762
2023-12-08 17:21:02.007546 Epoch 19  	Train Loss = 11.56722 Val Loss = 15.69279
2023-12-08 17:22:32.421057 Epoch 20  	Train Loss = 11.44313 Val Loss = 15.48231
2023-12-08 17:24:02.494682 Epoch 21  	Train Loss = 11.47764 Val Loss = 16.67081
2023-12-08 17:25:32.691875 Epoch 22  	Train Loss = 11.42507 Val Loss = 15.54637
2023-12-08 17:27:03.094665 Epoch 23  	Train Loss = 11.34225 Val Loss = 15.54274
2023-12-08 17:28:33.083305 Epoch 24  	Train Loss = 11.39761 Val Loss = 15.16159
2023-12-08 17:30:03.392133 Epoch 25  	Train Loss = 11.40230 Val Loss = 15.91331
2023-12-08 17:31:33.589319 Epoch 26  	Train Loss = 11.35692 Val Loss = 16.06703
2023-12-08 17:33:03.663798 Epoch 27  	Train Loss = 11.24681 Val Loss = 15.17768
2023-12-08 17:34:33.816988 Epoch 28  	Train Loss = 11.36136 Val Loss = 14.83018
2023-12-08 17:36:03.723136 Epoch 29  	Train Loss = 11.17891 Val Loss = 15.25837
2023-12-08 17:37:33.747526 Epoch 30  	Train Loss = 11.33238 Val Loss = 14.63470
2023-12-08 17:39:03.792378 Epoch 31  	Train Loss = 11.19652 Val Loss = 15.19848
2023-12-08 17:40:33.860608 Epoch 32  	Train Loss = 11.22041 Val Loss = 16.91399
2023-12-08 17:42:04.072706 Epoch 33  	Train Loss = 11.22948 Val Loss = 14.41409
2023-12-08 17:43:34.474363 Epoch 34  	Train Loss = 11.10990 Val Loss = 16.07911
2023-12-08 17:45:04.682631 Epoch 35  	Train Loss = 11.23007 Val Loss = 15.13926
2023-12-08 17:46:34.894557 Epoch 36  	Train Loss = 11.05663 Val Loss = 14.68039
2023-12-08 17:48:05.098589 Epoch 37  	Train Loss = 11.22641 Val Loss = 14.73933
2023-12-08 17:49:35.131989 Epoch 38  	Train Loss = 11.05178 Val Loss = 15.00986
2023-12-08 17:51:05.182617 Epoch 39  	Train Loss = 11.09415 Val Loss = 14.79239
2023-12-08 17:52:35.252708 Epoch 40  	Train Loss = 11.05351 Val Loss = 15.20872
2023-12-08 17:54:05.588593 Epoch 41  	Train Loss = 11.12909 Val Loss = 14.63605
2023-12-08 17:55:36.577452 Epoch 42  	Train Loss = 11.15037 Val Loss = 16.02165
2023-12-08 17:57:07.062389 Epoch 43  	Train Loss = 11.28891 Val Loss = 14.54950
2023-12-08 17:58:37.505984 Epoch 44  	Train Loss = 11.11093 Val Loss = 15.68542
2023-12-08 18:00:07.999312 Epoch 45  	Train Loss = 11.26437 Val Loss = 15.28929
2023-12-08 18:01:38.391501 Epoch 46  	Train Loss = 11.22289 Val Loss = 15.41572
2023-12-08 18:03:08.817579 Epoch 47  	Train Loss = 11.11751 Val Loss = 14.32973
2023-12-08 18:04:38.949330 Epoch 48  	Train Loss = 11.30543 Val Loss = 14.51857
2023-12-08 18:06:08.913458 Epoch 49  	Train Loss = 11.41397 Val Loss = 16.10930
2023-12-08 18:07:38.960450 Epoch 50  	Train Loss = 11.34531 Val Loss = 14.62021
2023-12-08 18:09:09.072910 Epoch 51  	Train Loss = 10.96129 Val Loss = 13.87973
2023-12-08 18:10:39.105612 Epoch 52  	Train Loss = 11.00257 Val Loss = 13.79995
2023-12-08 18:12:09.359565 Epoch 53  	Train Loss = 10.99213 Val Loss = 13.84920
2023-12-08 18:13:39.545273 Epoch 54  	Train Loss = 11.06839 Val Loss = 13.78551
2023-12-08 18:15:09.821705 Epoch 55  	Train Loss = 11.06303 Val Loss = 13.82916
2023-12-08 18:16:39.740439 Epoch 56  	Train Loss = 11.11071 Val Loss = 13.77545
2023-12-08 18:18:09.825728 Epoch 57  	Train Loss = 11.14119 Val Loss = 13.76044
2023-12-08 18:19:39.949836 Epoch 58  	Train Loss = 11.17515 Val Loss = 13.79843
2023-12-08 18:21:09.807494 Epoch 59  	Train Loss = 11.19056 Val Loss = 13.82776
2023-12-08 18:22:39.450491 Epoch 60  	Train Loss = 11.26902 Val Loss = 13.85974
2023-12-08 18:24:09.550232 Epoch 61  	Train Loss = 11.27869 Val Loss = 13.85215
2023-12-08 18:25:39.526389 Epoch 62  	Train Loss = 11.39756 Val Loss = 13.74029
2023-12-08 18:27:09.286793 Epoch 63  	Train Loss = 11.40624 Val Loss = 13.72713
2023-12-08 18:28:39.313468 Epoch 64  	Train Loss = 11.45602 Val Loss = 13.71731
2023-12-08 18:30:09.308599 Epoch 65  	Train Loss = 11.56459 Val Loss = 13.81587
2023-12-08 18:31:39.532709 Epoch 66  	Train Loss = 11.59933 Val Loss = 13.86937
2023-12-08 18:33:09.680152 Epoch 67  	Train Loss = 11.67531 Val Loss = 13.86165
2023-12-08 18:34:39.701339 Epoch 68  	Train Loss = 11.70315 Val Loss = 13.67733
2023-12-08 18:36:09.730579 Epoch 69  	Train Loss = 11.78227 Val Loss = 13.80607
2023-12-08 18:37:39.634919 Epoch 70  	Train Loss = 11.79628 Val Loss = 13.85152
2023-12-08 18:39:09.785705 Epoch 71  	Train Loss = 11.87654 Val Loss = 13.85965
2023-12-08 18:40:39.698687 Epoch 72  	Train Loss = 11.92483 Val Loss = 13.75560
2023-12-08 18:42:09.752540 Epoch 73  	Train Loss = 11.90407 Val Loss = 13.79221
2023-12-08 18:43:40.178282 Epoch 74  	Train Loss = 11.99297 Val Loss = 13.71870
2023-12-08 18:45:10.262366 Epoch 75  	Train Loss = 12.05872 Val Loss = 13.64684
2023-12-08 18:46:40.113773 Epoch 76  	Train Loss = 12.09486 Val Loss = 13.68132
2023-12-08 18:48:10.139748 Epoch 77  	Train Loss = 12.14376 Val Loss = 13.78102
2023-12-08 18:49:40.357865 Epoch 78  	Train Loss = 12.15610 Val Loss = 13.65981
2023-12-08 18:51:10.459969 Epoch 79  	Train Loss = 12.18568 Val Loss = 13.64232
2023-12-08 18:52:40.565046 Epoch 80  	Train Loss = 12.20655 Val Loss = 13.73845
2023-12-08 18:54:10.497829 Epoch 81  	Train Loss = 12.22545 Val Loss = 13.68589
2023-12-08 18:55:40.335085 Epoch 82  	Train Loss = 12.23677 Val Loss = 13.61908
2023-12-08 18:57:10.399114 Epoch 83  	Train Loss = 12.26649 Val Loss = 13.69783
2023-12-08 18:58:40.571828 Epoch 84  	Train Loss = 12.28116 Val Loss = 13.62057
2023-12-08 19:00:10.865863 Epoch 85  	Train Loss = 12.29526 Val Loss = 13.71293
2023-12-08 19:01:40.809430 Epoch 86  	Train Loss = 12.27139 Val Loss = 13.77104
2023-12-08 19:03:10.636634 Epoch 87  	Train Loss = 12.25435 Val Loss = 13.65607
2023-12-08 19:04:40.585816 Epoch 88  	Train Loss = 12.30718 Val Loss = 13.69464
2023-12-08 19:06:10.561486 Epoch 89  	Train Loss = 12.29153 Val Loss = 13.74637
2023-12-08 19:07:40.484569 Epoch 90  	Train Loss = 12.28914 Val Loss = 13.70527
2023-12-08 19:09:10.610892 Epoch 91  	Train Loss = 12.28785 Val Loss = 13.68436
2023-12-08 19:10:40.379830 Epoch 92  	Train Loss = 12.29289 Val Loss = 13.70788
2023-12-08 19:12:10.370346 Epoch 93  	Train Loss = 12.30566 Val Loss = 13.65547
2023-12-08 19:13:40.303678 Epoch 94  	Train Loss = 12.29039 Val Loss = 13.75883
2023-12-08 19:15:10.109960 Epoch 95  	Train Loss = 12.30411 Val Loss = 13.62472
2023-12-08 19:16:39.882912 Epoch 96  	Train Loss = 12.28802 Val Loss = 13.82150
2023-12-08 19:18:09.729915 Epoch 97  	Train Loss = 12.27671 Val Loss = 13.63955
2023-12-08 19:19:40.010759 Epoch 98  	Train Loss = 12.26715 Val Loss = 13.79867
2023-12-08 19:21:10.463385 Epoch 99  	Train Loss = 12.24879 Val Loss = 13.67374
2023-12-08 19:22:40.584249 Epoch 100  	Train Loss = 12.27184 Val Loss = 13.66746
2023-12-08 19:24:10.810673 Epoch 101  	Train Loss = 12.12469 Val Loss = 13.61493
2023-12-08 19:25:40.884509 Epoch 102  	Train Loss = 12.11340 Val Loss = 13.62324
2023-12-08 19:27:10.952594 Epoch 103  	Train Loss = 12.11108 Val Loss = 13.61521
2023-12-08 19:28:41.088571 Epoch 104  	Train Loss = 12.10064 Val Loss = 13.61503
2023-12-08 19:30:11.090252 Epoch 105  	Train Loss = 12.09958 Val Loss = 13.61650
2023-12-08 19:31:41.049353 Epoch 106  	Train Loss = 12.09818 Val Loss = 13.62509
2023-12-08 19:33:10.955678 Epoch 107  	Train Loss = 12.09807 Val Loss = 13.62729
2023-12-08 19:34:40.815453 Epoch 108  	Train Loss = 12.09332 Val Loss = 13.63233
2023-12-08 19:36:10.749896 Epoch 109  	Train Loss = 12.09193 Val Loss = 13.63187
2023-12-08 19:37:40.700848 Epoch 110  	Train Loss = 12.08704 Val Loss = 13.62149
2023-12-08 19:39:10.825627 Epoch 111  	Train Loss = 12.08852 Val Loss = 13.62501
2023-12-08 19:40:40.986643 Epoch 112  	Train Loss = 12.08723 Val Loss = 13.60806
2023-12-08 19:42:10.915371 Epoch 113  	Train Loss = 12.08368 Val Loss = 13.61638
2023-12-08 19:43:40.733208 Epoch 114  	Train Loss = 12.07892 Val Loss = 13.62345
2023-12-08 19:45:10.795242 Epoch 115  	Train Loss = 12.08164 Val Loss = 13.66227
2023-12-08 19:46:41.051726 Epoch 116  	Train Loss = 12.07980 Val Loss = 13.63804
2023-12-08 19:48:11.337646 Epoch 117  	Train Loss = 12.07924 Val Loss = 13.62616
2023-12-08 19:49:41.565654 Epoch 118  	Train Loss = 12.07533 Val Loss = 13.62294
2023-12-08 19:51:11.576149 Epoch 119  	Train Loss = 12.07216 Val Loss = 13.63793
2023-12-08 19:52:41.830716 Epoch 120  	Train Loss = 12.07335 Val Loss = 13.62999
2023-12-08 19:54:11.779989 Epoch 121  	Train Loss = 12.06689 Val Loss = 13.64325
2023-12-08 19:55:41.794940 Epoch 122  	Train Loss = 12.06861 Val Loss = 13.64765
2023-12-08 19:57:11.668090 Epoch 123  	Train Loss = 12.06879 Val Loss = 13.64671
2023-12-08 19:58:41.606920 Epoch 124  	Train Loss = 12.06617 Val Loss = 13.65142
2023-12-08 20:00:11.526086 Epoch 125  	Train Loss = 12.06271 Val Loss = 13.64328
2023-12-08 20:01:41.561601 Epoch 126  	Train Loss = 12.06111 Val Loss = 13.63984
2023-12-08 20:03:11.537280 Epoch 127  	Train Loss = 12.05926 Val Loss = 13.64520
2023-12-08 20:04:41.724941 Epoch 128  	Train Loss = 12.05682 Val Loss = 13.65136
2023-12-08 20:06:11.745392 Epoch 129  	Train Loss = 12.05441 Val Loss = 13.65180
2023-12-08 20:07:41.593461 Epoch 130  	Train Loss = 12.05256 Val Loss = 13.64339
2023-12-08 20:09:11.391777 Epoch 131  	Train Loss = 12.05411 Val Loss = 13.64270
2023-12-08 20:10:41.429077 Epoch 132  	Train Loss = 12.05280 Val Loss = 13.64871
2023-12-08 20:12:11.535539 Epoch 133  	Train Loss = 12.04776 Val Loss = 13.64891
2023-12-08 20:13:41.467664 Epoch 134  	Train Loss = 12.04792 Val Loss = 13.63351
2023-12-08 20:15:11.544836 Epoch 135  	Train Loss = 12.04269 Val Loss = 13.67526
2023-12-08 20:16:41.715618 Epoch 136  	Train Loss = 12.04381 Val Loss = 13.67694
2023-12-08 20:18:11.735368 Epoch 137  	Train Loss = 12.04455 Val Loss = 13.65443
2023-12-08 20:19:41.752444 Epoch 138  	Train Loss = 12.03749 Val Loss = 13.66443
2023-12-08 20:21:11.810094 Epoch 139  	Train Loss = 12.03773 Val Loss = 13.67283
2023-12-08 20:22:41.813492 Epoch 140  	Train Loss = 12.03237 Val Loss = 13.68169
2023-12-08 20:24:11.825869 Epoch 141  	Train Loss = 12.03468 Val Loss = 13.66513
2023-12-08 20:25:42.048551 Epoch 142  	Train Loss = 12.02745 Val Loss = 13.66814
Early stopping at epoch: 142
Best at epoch 112:
Train Loss = 12.08723
Train RMSE = 19.56426, MAE = 12.07951, MAPE = 11.66992
Val Loss = 13.60806
Val RMSE = 21.83380, MAE = 13.61772, MAPE = 13.23051
--------- Test ---------
All Steps RMSE = 26.21778, MAE = 14.65066, MAPE = 15.20019
Step 1 RMSE = 20.50967, MAE = 11.50391, MAPE = 12.52139
Step 2 RMSE = 22.69995, MAE = 12.80181, MAPE = 13.85438
Step 3 RMSE = 24.22820, MAE = 13.53399, MAPE = 14.77449
Step 4 RMSE = 25.21528, MAE = 13.99837, MAPE = 14.67492
Step 5 RMSE = 25.94615, MAE = 14.39958, MAPE = 14.94138
Step 6 RMSE = 26.50884, MAE = 14.75144, MAPE = 15.20908
Step 7 RMSE = 27.01606, MAE = 15.09097, MAPE = 15.46714
Step 8 RMSE = 27.47500, MAE = 15.40675, MAPE = 15.73801
Step 9 RMSE = 27.88670, MAE = 15.69296, MAPE = 15.96365
Step 10 RMSE = 28.24324, MAE = 15.95018, MAPE = 16.18244
Step 11 RMSE = 28.57794, MAE = 16.20564, MAPE = 16.40639
Step 12 RMSE = 28.93078, MAE = 16.47207, MAPE = 16.66893
Inference time: 11.05 s
