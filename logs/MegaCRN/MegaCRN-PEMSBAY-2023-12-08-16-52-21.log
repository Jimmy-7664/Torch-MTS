PEMSBAY
Trainset:	x-(36465, 12, 325, 1)	y-(36465, 12, 325, 2)
Valset:  	x-(5209, 12, 325, 1)  	y-(5209, 12, 325, 2)
Testset:	x-(10419, 12, 325, 1)	y-(10419, 12, 325, 2)

--------- MegaCRN ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "y_time_of_day": true,
    "runner": "megacrn",
    "loss": "megacrn",
    "loss_args": {
        "l1": 0.01,
        "l2": 0.01
    },
    "lr": 0.01,
    "eps": 0.001,
    "milestones": [
        50,
        100
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 30,
    "model_args": {
        "num_nodes": 325,
        "input_dim": 1,
        "output_dim": 1,
        "horizon": 12,
        "rnn_units": 64,
        "num_layers": 1,
        "cheb_k": 3,
        "ycov_dim": 1,
        "mem_num": 20,
        "mem_dim": 64,
        "tf_decay_steps": 2000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MegaCRN                                  [64, 12, 325, 1]          18,376
├─ADCRNN_Encoder: 1-1                    [64, 12, 325, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─AGCRNCell: 3-1               [64, 325, 64]             75,072
│    │    └─AGCRNCell: 3-2               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-3               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-4               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-5               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-6               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-7               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-8               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-9               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-10              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-11              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-12              [64, 325, 64]             (recursive)
├─ADCRNN_Decoder: 1-2                    [64, 325, 128]            --
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-13              [64, 325, 128]            299,904
├─Sequential: 1-3                        [64, 325, 1]              --
│    └─Linear: 2-3                       [64, 325, 1]              129
├─ADCRNN_Decoder: 1-4                    [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-14              [64, 325, 128]            (recursive)
├─Sequential: 1-5                        [64, 325, 1]              (recursive)
│    └─Linear: 2-5                       [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-6                    [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-15              [64, 325, 128]            (recursive)
├─Sequential: 1-7                        [64, 325, 1]              (recursive)
│    └─Linear: 2-7                       [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-8                    [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-16              [64, 325, 128]            (recursive)
├─Sequential: 1-9                        [64, 325, 1]              (recursive)
│    └─Linear: 2-9                       [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-10                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-17              [64, 325, 128]            (recursive)
├─Sequential: 1-11                       [64, 325, 1]              (recursive)
│    └─Linear: 2-11                      [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-12                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-18              [64, 325, 128]            (recursive)
├─Sequential: 1-13                       [64, 325, 1]              (recursive)
│    └─Linear: 2-13                      [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-14                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-19              [64, 325, 128]            (recursive)
├─Sequential: 1-15                       [64, 325, 1]              (recursive)
│    └─Linear: 2-15                      [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-16                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-20              [64, 325, 128]            (recursive)
├─Sequential: 1-17                       [64, 325, 1]              (recursive)
│    └─Linear: 2-17                      [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-18                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-21              [64, 325, 128]            (recursive)
├─Sequential: 1-19                       [64, 325, 1]              (recursive)
│    └─Linear: 2-19                      [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-20                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-22              [64, 325, 128]            (recursive)
├─Sequential: 1-21                       [64, 325, 1]              (recursive)
│    └─Linear: 2-21                      [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-22                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-23              [64, 325, 128]            (recursive)
├─Sequential: 1-23                       [64, 325, 1]              (recursive)
│    └─Linear: 2-23                      [64, 325, 1]              (recursive)
├─ADCRNN_Decoder: 1-24                   [64, 325, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-24              [64, 325, 128]            (recursive)
├─Sequential: 1-25                       [64, 325, 1]              (recursive)
│    └─Linear: 2-25                      [64, 325, 1]              (recursive)
==========================================================================================
Total params: 393,481
Trainable params: 393,481
Non-trainable params: 0
Total mult-adds (G): 93.45
==========================================================================================
Input size (MB): 2.00
Forward/backward pass size (MB): 1152.15
Params size (MB): 1.50
Estimated Total Size (MB): 1155.65
==========================================================================================

Loss: MegaCRNLoss

2023-12-08 16:55:23.969162 Epoch 1  	Train Loss = 0.95494 Val Loss = 2.27543
2023-12-08 16:58:28.794136 Epoch 2  	Train Loss = 0.86737 Val Loss = 2.15380
2023-12-08 17:01:37.926203 Epoch 3  	Train Loss = 0.85587 Val Loss = 2.28280
2023-12-08 17:04:56.599755 Epoch 4  	Train Loss = 0.84861 Val Loss = 2.05172
2023-12-08 17:08:18.611687 Epoch 5  	Train Loss = 0.83935 Val Loss = 2.13140
2023-12-08 17:11:37.590260 Epoch 6  	Train Loss = 0.83585 Val Loss = 1.98664
2023-12-08 17:14:55.561226 Epoch 7  	Train Loss = 0.83262 Val Loss = 1.88457
2023-12-08 17:18:12.702124 Epoch 8  	Train Loss = 0.83087 Val Loss = 2.00092
2023-12-08 17:21:29.900955 Epoch 9  	Train Loss = 0.82807 Val Loss = 1.84354
2023-12-08 17:24:44.466256 Epoch 10  	Train Loss = 0.82630 Val Loss = 2.24064
2023-12-08 17:28:00.500540 Epoch 11  	Train Loss = 0.82308 Val Loss = 1.86143
2023-12-08 17:31:16.557612 Epoch 12  	Train Loss = 0.81888 Val Loss = 1.80359
2023-12-08 17:34:32.042186 Epoch 13  	Train Loss = 0.81952 Val Loss = 1.85360
2023-12-08 17:37:45.066300 Epoch 14  	Train Loss = 0.81433 Val Loss = 1.77853
2023-12-08 17:41:00.091220 Epoch 15  	Train Loss = 0.81288 Val Loss = 1.82494
2023-12-08 17:44:16.599272 Epoch 16  	Train Loss = 0.81501 Val Loss = 1.81951
2023-12-08 17:47:32.856632 Epoch 17  	Train Loss = 0.82145 Val Loss = 1.76118
2023-12-08 17:50:47.505527 Epoch 18  	Train Loss = 0.82357 Val Loss = 1.74718
2023-12-08 17:54:03.170370 Epoch 19  	Train Loss = 0.82684 Val Loss = 1.90394
2023-12-08 17:57:28.406706 Epoch 20  	Train Loss = 0.83857 Val Loss = 1.86080
2023-12-08 18:00:46.331772 Epoch 21  	Train Loss = 0.85064 Val Loss = 1.97104
2023-12-08 18:04:02.987641 Epoch 22  	Train Loss = 0.86063 Val Loss = 1.80990
2023-12-08 18:07:16.640173 Epoch 23  	Train Loss = 0.88135 Val Loss = 1.80583
2023-12-08 18:10:31.744319 Epoch 24  	Train Loss = 0.89935 Val Loss = 1.73418
2023-12-08 18:13:47.575266 Epoch 25  	Train Loss = 0.93772 Val Loss = 1.72104
2023-12-08 18:17:03.013869 Epoch 26  	Train Loss = 0.96318 Val Loss = 1.70049
2023-12-08 18:20:15.988251 Epoch 27  	Train Loss = 1.00043 Val Loss = 1.79823
2023-12-08 18:23:27.590259 Epoch 28  	Train Loss = 1.04926 Val Loss = 1.71975
2023-12-08 18:26:42.487164 Epoch 29  	Train Loss = 1.10354 Val Loss = 1.72544
2023-12-08 18:29:57.635367 Epoch 30  	Train Loss = 1.14676 Val Loss = 1.84613
2023-12-08 18:33:11.877841 Epoch 31  	Train Loss = 1.19602 Val Loss = 1.65203
2023-12-08 18:36:26.017826 Epoch 32  	Train Loss = 1.23751 Val Loss = 1.74129
2023-12-08 18:39:39.873232 Epoch 33  	Train Loss = 1.28233 Val Loss = 1.66829
2023-12-08 18:42:54.608955 Epoch 34  	Train Loss = 1.31834 Val Loss = 1.65429
2023-12-08 18:46:09.223300 Epoch 35  	Train Loss = 1.34407 Val Loss = 1.69088
2023-12-08 18:49:23.694331 Epoch 36  	Train Loss = 1.35555 Val Loss = 1.63880
2023-12-08 18:52:38.387062 Epoch 37  	Train Loss = 1.37378 Val Loss = 1.65341
2023-12-08 18:55:53.097711 Epoch 38  	Train Loss = 1.36367 Val Loss = 1.60328
2023-12-08 18:59:08.226042 Epoch 39  	Train Loss = 1.37773 Val Loss = 1.62262
2023-12-08 19:02:23.413360 Epoch 40  	Train Loss = 1.37317 Val Loss = 1.58920
2023-12-08 19:05:37.514081 Epoch 41  	Train Loss = 1.38939 Val Loss = 1.59018
2023-12-08 19:08:51.483776 Epoch 42  	Train Loss = 1.37947 Val Loss = 1.59587
2023-12-08 19:12:00.924439 Epoch 43  	Train Loss = 1.37810 Val Loss = 1.61276
2023-12-08 19:15:10.842528 Epoch 44  	Train Loss = 1.37213 Val Loss = 1.59252
2023-12-08 19:18:24.704984 Epoch 45  	Train Loss = 1.36867 Val Loss = 1.60269
2023-12-08 19:21:47.731039 Epoch 46  	Train Loss = 1.36368 Val Loss = 1.60317
2023-12-08 19:25:04.032823 Epoch 47  	Train Loss = 1.35698 Val Loss = 1.59335
2023-12-08 19:28:18.557105 Epoch 48  	Train Loss = 1.35521 Val Loss = 1.58268
2023-12-08 19:31:32.955603 Epoch 49  	Train Loss = 1.35065 Val Loss = 1.58469
2023-12-08 19:34:46.862856 Epoch 50  	Train Loss = 1.34390 Val Loss = 1.58311
2023-12-08 19:38:01.359308 Epoch 51  	Train Loss = 1.28277 Val Loss = 1.55302
2023-12-08 19:41:16.071645 Epoch 52  	Train Loss = 1.27093 Val Loss = 1.55739
2023-12-08 19:44:30.192123 Epoch 53  	Train Loss = 1.26619 Val Loss = 1.56228
2023-12-08 19:47:53.632031 Epoch 54  	Train Loss = 1.26335 Val Loss = 1.56160
2023-12-08 19:51:11.742813 Epoch 55  	Train Loss = 1.26046 Val Loss = 1.56154
2023-12-08 19:54:27.536335 Epoch 56  	Train Loss = 1.25787 Val Loss = 1.56278
2023-12-08 19:57:41.776345 Epoch 57  	Train Loss = 1.25588 Val Loss = 1.56767
2023-12-08 20:00:55.811253 Epoch 58  	Train Loss = 1.25375 Val Loss = 1.56286
2023-12-08 20:04:09.931479 Epoch 59  	Train Loss = 1.25183 Val Loss = 1.56313
2023-12-08 20:07:23.929219 Epoch 60  	Train Loss = 1.24993 Val Loss = 1.56404
2023-12-08 20:10:37.730383 Epoch 61  	Train Loss = 1.24799 Val Loss = 1.57036
2023-12-08 20:13:51.699932 Epoch 62  	Train Loss = 1.24635 Val Loss = 1.56810
2023-12-08 20:17:06.504234 Epoch 63  	Train Loss = 1.24468 Val Loss = 1.56731
2023-12-08 20:20:20.397062 Epoch 64  	Train Loss = 1.24295 Val Loss = 1.56997
2023-12-08 20:23:34.293405 Epoch 65  	Train Loss = 1.24141 Val Loss = 1.57132
2023-12-08 20:26:47.893068 Epoch 66  	Train Loss = 1.24010 Val Loss = 1.57021
2023-12-08 20:29:54.439841 Epoch 67  	Train Loss = 1.23829 Val Loss = 1.57111
2023-12-08 20:32:59.489065 Epoch 68  	Train Loss = 1.23729 Val Loss = 1.56972
2023-12-08 20:36:03.823364 Epoch 69  	Train Loss = 1.23589 Val Loss = 1.57726
2023-12-08 20:39:05.971267 Epoch 70  	Train Loss = 1.23429 Val Loss = 1.57382
2023-12-08 20:42:07.272446 Epoch 71  	Train Loss = 1.23304 Val Loss = 1.57345
2023-12-08 20:45:09.185703 Epoch 72  	Train Loss = 1.23178 Val Loss = 1.57769
2023-12-08 20:48:19.788815 Epoch 73  	Train Loss = 1.23030 Val Loss = 1.57581
2023-12-08 20:51:34.714151 Epoch 74  	Train Loss = 1.22905 Val Loss = 1.57785
2023-12-08 20:54:49.926053 Epoch 75  	Train Loss = 1.22791 Val Loss = 1.57876
2023-12-08 20:58:12.750924 Epoch 76  	Train Loss = 1.22697 Val Loss = 1.57764
2023-12-08 21:01:37.709196 Epoch 77  	Train Loss = 1.22533 Val Loss = 1.58193
2023-12-08 21:04:56.211522 Epoch 78  	Train Loss = 1.22465 Val Loss = 1.58018
2023-12-08 21:08:13.102897 Epoch 79  	Train Loss = 1.22330 Val Loss = 1.57854
2023-12-08 21:11:28.985494 Epoch 80  	Train Loss = 1.22230 Val Loss = 1.57897
2023-12-08 21:14:44.479799 Epoch 81  	Train Loss = 1.22081 Val Loss = 1.57899
Early stopping at epoch: 81
Best at epoch 51:
Train Loss = 1.28277
Train RMSE = 2.72545, MAE = 1.27120, MAPE = 2.64440
Val Loss = 1.55302
Val RMSE = 3.59176, MAE = 1.54187, MAPE = 3.49170
--------- Test ---------
All Steps RMSE = 3.61887, MAE = 1.54703, MAPE = 3.47782
Step 1 RMSE = 1.53924, MAE = 0.85162, MAPE = 1.63904
Step 2 RMSE = 2.19860, MAE = 1.10834, MAPE = 2.22327
Step 3 RMSE = 2.71201, MAE = 1.28466, MAPE = 2.67205
Step 4 RMSE = 3.11790, MAE = 1.41746, MAPE = 3.04151
Step 5 RMSE = 3.43873, MAE = 1.52041, MAPE = 3.34609
Step 6 RMSE = 3.69354, MAE = 1.60424, MAPE = 3.60133
Step 7 RMSE = 3.89693, MAE = 1.67345, MAPE = 3.81712
Step 8 RMSE = 4.06015, MAE = 1.73163, MAPE = 4.00028
Step 9 RMSE = 4.19478, MAE = 1.78140, MAPE = 4.15700
Step 10 RMSE = 4.30701, MAE = 1.82502, MAPE = 4.29409
Step 11 RMSE = 4.40520, MAE = 1.86454, MAPE = 4.41630
Step 12 RMSE = 4.49277, MAE = 1.90156, MAPE = 4.52582
Inference time: 21.00 s
