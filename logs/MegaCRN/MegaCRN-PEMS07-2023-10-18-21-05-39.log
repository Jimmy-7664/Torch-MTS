PEMS07
Trainset:	x-(16921, 12, 883, 1)	y-(16921, 12, 883, 2)
Valset:  	x-(5640, 12, 883, 1)  	y-(5640, 12, 883, 2)
Testset:	x-(5640, 12, 883, 1)	y-(5640, 12, 883, 2)

--------- MegaCRN ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "y_time_of_day": true,
    "runner": "megacrn",
    "loss": "megacrn",
    "loss_args": {
        "l1": 0.01,
        "l2": 0.01
    },
    "lr": 0.01,
    "eps": 0.001,
    "milestones": [
        50,
        100
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 30,
    "model_args": {
        "num_nodes": 883,
        "input_dim": 1,
        "output_dim": 1,
        "horizon": 12,
        "rnn_units": 64,
        "num_layers": 1,
        "cheb_k": 3,
        "ycov_dim": 1,
        "mem_num": 20,
        "mem_dim": 64,
        "tf_decay_steps": 2000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MegaCRN                                  [64, 12, 883, 1]          40,696
├─ADCRNN_Encoder: 1-1                    [64, 12, 883, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─AGCRNCell: 3-1               [64, 883, 64]             75,072
│    │    └─AGCRNCell: 3-2               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-3               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-4               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-5               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-6               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-7               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-8               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-9               [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-10              [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-11              [64, 883, 64]             (recursive)
│    │    └─AGCRNCell: 3-12              [64, 883, 64]             (recursive)
├─ADCRNN_Decoder: 1-2                    [64, 883, 128]            --
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-13              [64, 883, 128]            299,904
├─Sequential: 1-3                        [64, 883, 1]              --
│    └─Linear: 2-3                       [64, 883, 1]              129
├─ADCRNN_Decoder: 1-4                    [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-14              [64, 883, 128]            (recursive)
├─Sequential: 1-5                        [64, 883, 1]              (recursive)
│    └─Linear: 2-5                       [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-6                    [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-15              [64, 883, 128]            (recursive)
├─Sequential: 1-7                        [64, 883, 1]              (recursive)
│    └─Linear: 2-7                       [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-8                    [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-16              [64, 883, 128]            (recursive)
├─Sequential: 1-9                        [64, 883, 1]              (recursive)
│    └─Linear: 2-9                       [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-10                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-17              [64, 883, 128]            (recursive)
├─Sequential: 1-11                       [64, 883, 1]              (recursive)
│    └─Linear: 2-11                      [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-12                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-18              [64, 883, 128]            (recursive)
├─Sequential: 1-13                       [64, 883, 1]              (recursive)
│    └─Linear: 2-13                      [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-14                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-19              [64, 883, 128]            (recursive)
├─Sequential: 1-15                       [64, 883, 1]              (recursive)
│    └─Linear: 2-15                      [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-16                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-20              [64, 883, 128]            (recursive)
├─Sequential: 1-17                       [64, 883, 1]              (recursive)
│    └─Linear: 2-17                      [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-18                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-21              [64, 883, 128]            (recursive)
├─Sequential: 1-19                       [64, 883, 1]              (recursive)
│    └─Linear: 2-19                      [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-20                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-22              [64, 883, 128]            (recursive)
├─Sequential: 1-21                       [64, 883, 1]              (recursive)
│    └─Linear: 2-21                      [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-22                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-23              [64, 883, 128]            (recursive)
├─Sequential: 1-23                       [64, 883, 1]              (recursive)
│    └─Linear: 2-23                      [64, 883, 1]              (recursive)
├─ADCRNN_Decoder: 1-24                   [64, 883, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-24              [64, 883, 128]            (recursive)
├─Sequential: 1-25                       [64, 883, 1]              (recursive)
│    └─Linear: 2-25                      [64, 883, 1]              (recursive)
==========================================================================================
Total params: 415,801
Trainable params: 415,801
Non-trainable params: 0
Total mult-adds (G): 253.90
==========================================================================================
Input size (MB): 5.43
Forward/backward pass size (MB): 260.86
Params size (MB): 1.50
Estimated Total Size (MB): 267.79
==========================================================================================

Loss: MegaCRNLoss

2023-10-18 21:09:09.340098 Epoch 1  	Train Loss = 24.80156 Val Loss = 31.76704
2023-10-18 21:12:33.564405 Epoch 2  	Train Loss = 19.57828 Val Loss = 33.27280
2023-10-18 21:15:58.299727 Epoch 3  	Train Loss = 19.05764 Val Loss = 26.97052
2023-10-18 21:19:23.326568 Epoch 4  	Train Loss = 19.00406 Val Loss = 26.88167
2023-10-18 21:22:48.155066 Epoch 5  	Train Loss = 18.81207 Val Loss = 28.18619
2023-10-18 21:26:13.789644 Epoch 6  	Train Loss = 18.96274 Val Loss = 27.83925
2023-10-18 21:29:38.659939 Epoch 7  	Train Loss = 18.61565 Val Loss = 43.09117
2023-10-18 21:33:04.202705 Epoch 8  	Train Loss = 18.38689 Val Loss = 33.63314
2023-10-18 21:36:29.784316 Epoch 9  	Train Loss = 18.20174 Val Loss = 27.79493
2023-10-18 21:39:54.671393 Epoch 10  	Train Loss = 18.07444 Val Loss = 27.59861
2023-10-18 21:43:19.448005 Epoch 11  	Train Loss = 18.02930 Val Loss = 26.08549
2023-10-18 21:46:44.213052 Epoch 12  	Train Loss = 17.89551 Val Loss = 26.58849
2023-10-18 21:50:09.144941 Epoch 13  	Train Loss = 17.78211 Val Loss = 27.80029
2023-10-18 21:53:33.980367 Epoch 14  	Train Loss = 18.12219 Val Loss = 30.25856
2023-10-18 21:56:58.490438 Epoch 15  	Train Loss = 17.76159 Val Loss = 24.50732
2023-10-18 22:00:23.212822 Epoch 16  	Train Loss = 17.84141 Val Loss = 32.60324
2023-10-18 22:03:47.580531 Epoch 17  	Train Loss = 17.89480 Val Loss = 25.78904
2023-10-18 22:07:12.935437 Epoch 18  	Train Loss = 17.93167 Val Loss = 30.10575
2023-10-18 22:10:37.912448 Epoch 19  	Train Loss = 17.78975 Val Loss = 24.90132
2023-10-18 22:14:02.208195 Epoch 20  	Train Loss = 17.47540 Val Loss = 24.60526
2023-10-18 22:17:26.792999 Epoch 21  	Train Loss = 17.40519 Val Loss = 24.76913
2023-10-18 22:20:51.563161 Epoch 22  	Train Loss = 17.43917 Val Loss = 24.79311
2023-10-18 22:24:16.355571 Epoch 23  	Train Loss = 17.51081 Val Loss = 24.32061
2023-10-18 22:27:40.866628 Epoch 24  	Train Loss = 17.28908 Val Loss = 26.95412
2023-10-18 22:31:04.402936 Epoch 25  	Train Loss = 17.60670 Val Loss = 23.35378
2023-10-18 22:34:27.959252 Epoch 26  	Train Loss = 17.19024 Val Loss = 23.43093
2023-10-18 22:37:53.706938 Epoch 27  	Train Loss = 17.20251 Val Loss = 25.33581
2023-10-18 22:41:18.628138 Epoch 28  	Train Loss = 17.12442 Val Loss = 22.51683
2023-10-18 22:44:43.319624 Epoch 29  	Train Loss = 17.15727 Val Loss = 27.08112
2023-10-18 22:48:07.618631 Epoch 30  	Train Loss = 17.00441 Val Loss = 22.37407
2023-10-18 22:51:31.867601 Epoch 31  	Train Loss = 16.89505 Val Loss = 25.87078
2023-10-18 22:54:56.179321 Epoch 32  	Train Loss = 17.29132 Val Loss = 35.33609
2023-10-18 22:58:20.468795 Epoch 33  	Train Loss = 17.02849 Val Loss = 23.27784
2023-10-18 23:01:44.943030 Epoch 34  	Train Loss = 16.65100 Val Loss = 22.04388
2023-10-18 23:05:09.155220 Epoch 35  	Train Loss = 16.65919 Val Loss = 22.03706
2023-10-18 23:08:34.331514 Epoch 36  	Train Loss = 16.48671 Val Loss = 22.94245
2023-10-18 23:11:59.574693 Epoch 37  	Train Loss = 16.73360 Val Loss = 22.30644
2023-10-18 23:15:24.479541 Epoch 38  	Train Loss = 16.64502 Val Loss = 26.17630
2023-10-18 23:18:48.973131 Epoch 39  	Train Loss = 16.63330 Val Loss = 29.79177
2023-10-18 23:22:13.338867 Epoch 40  	Train Loss = 16.83616 Val Loss = 21.82112
2023-10-18 23:25:38.233050 Epoch 41  	Train Loss = 16.57209 Val Loss = 23.11405
2023-10-18 23:29:03.168535 Epoch 42  	Train Loss = 16.85590 Val Loss = 29.90669
2023-10-18 23:32:27.705726 Epoch 43  	Train Loss = 16.83639 Val Loss = 22.19130
2023-10-18 23:35:52.229071 Epoch 44  	Train Loss = 16.52451 Val Loss = 22.60021
2023-10-18 23:39:16.784737 Epoch 45  	Train Loss = 16.72925 Val Loss = 22.25349
2023-10-18 23:42:41.925032 Epoch 46  	Train Loss = 16.63649 Val Loss = 21.53684
2023-10-18 23:46:06.704932 Epoch 47  	Train Loss = 16.66821 Val Loss = 22.61593
2023-10-18 23:49:31.501176 Epoch 48  	Train Loss = 16.81816 Val Loss = 25.64336
2023-10-18 23:52:56.449071 Epoch 49  	Train Loss = 16.88539 Val Loss = 22.45392
2023-10-18 23:56:21.424158 Epoch 50  	Train Loss = 16.96643 Val Loss = 22.00297
2023-10-18 23:59:45.817255 Epoch 51  	Train Loss = 16.44809 Val Loss = 20.55587
2023-10-19 00:03:10.843505 Epoch 52  	Train Loss = 16.45533 Val Loss = 20.35535
2023-10-19 00:06:35.456301 Epoch 53  	Train Loss = 16.50036 Val Loss = 20.36156
2023-10-19 00:10:01.407710 Epoch 54  	Train Loss = 16.55738 Val Loss = 20.41132
2023-10-19 00:13:26.769666 Epoch 55  	Train Loss = 16.58240 Val Loss = 20.33828
2023-10-19 00:16:51.377159 Epoch 56  	Train Loss = 16.70917 Val Loss = 20.24358
2023-10-19 00:20:16.132745 Epoch 57  	Train Loss = 16.74310 Val Loss = 20.29533
2023-10-19 00:23:40.229525 Epoch 58  	Train Loss = 16.91267 Val Loss = 20.26938
2023-10-19 00:27:04.541308 Epoch 59  	Train Loss = 16.94872 Val Loss = 20.26390
2023-10-19 00:30:29.394229 Epoch 60  	Train Loss = 17.11716 Val Loss = 20.09137
2023-10-19 00:33:53.830748 Epoch 61  	Train Loss = 17.20245 Val Loss = 20.07646
2023-10-19 00:37:18.605802 Epoch 62  	Train Loss = 17.31026 Val Loss = 20.07840
2023-10-19 00:40:43.530781 Epoch 63  	Train Loss = 17.43153 Val Loss = 20.15212
2023-10-19 00:44:08.608361 Epoch 64  	Train Loss = 17.50630 Val Loss = 20.03915
2023-10-19 00:47:32.907372 Epoch 65  	Train Loss = 17.56131 Val Loss = 19.94923
2023-10-19 00:50:57.549037 Epoch 66  	Train Loss = 17.72597 Val Loss = 19.98678
2023-10-19 00:54:21.882140 Epoch 67  	Train Loss = 17.82452 Val Loss = 20.04570
2023-10-19 00:57:46.513752 Epoch 68  	Train Loss = 17.83947 Val Loss = 19.98981
2023-10-19 01:01:10.722779 Epoch 69  	Train Loss = 18.01207 Val Loss = 19.99934
2023-10-19 01:04:35.137487 Epoch 70  	Train Loss = 18.06485 Val Loss = 19.96177
2023-10-19 01:07:59.634861 Epoch 71  	Train Loss = 18.23749 Val Loss = 19.82213
2023-10-19 01:11:24.015828 Epoch 72  	Train Loss = 18.26644 Val Loss = 19.79871
2023-10-19 01:14:48.620625 Epoch 73  	Train Loss = 18.29268 Val Loss = 19.98859
2023-10-19 01:18:13.164843 Epoch 74  	Train Loss = 18.42272 Val Loss = 19.79314
2023-10-19 01:21:37.701099 Epoch 75  	Train Loss = 18.40436 Val Loss = 19.84112
2023-10-19 01:25:02.290922 Epoch 76  	Train Loss = 18.52025 Val Loss = 19.99009
2023-10-19 01:28:27.134827 Epoch 77  	Train Loss = 18.50609 Val Loss = 19.82803
2023-10-19 01:31:51.978884 Epoch 78  	Train Loss = 18.58187 Val Loss = 19.64101
2023-10-19 01:35:16.809750 Epoch 79  	Train Loss = 18.56421 Val Loss = 19.65927
2023-10-19 01:38:41.588955 Epoch 80  	Train Loss = 18.54786 Val Loss = 19.69264
2023-10-19 01:42:07.165502 Epoch 81  	Train Loss = 18.60841 Val Loss = 19.76446
2023-10-19 01:45:32.133646 Epoch 82  	Train Loss = 18.58846 Val Loss = 19.73174
2023-10-19 01:48:57.512984 Epoch 83  	Train Loss = 18.61845 Val Loss = 19.68597
2023-10-19 01:52:22.147336 Epoch 84  	Train Loss = 18.64828 Val Loss = 19.93614
2023-10-19 01:55:47.091250 Epoch 85  	Train Loss = 18.65756 Val Loss = 19.66866
2023-10-19 01:59:11.701160 Epoch 86  	Train Loss = 18.66974 Val Loss = 19.80864
2023-10-19 02:02:36.266734 Epoch 87  	Train Loss = 18.66495 Val Loss = 19.60890
2023-10-19 02:06:01.271990 Epoch 88  	Train Loss = 18.68535 Val Loss = 19.63038
2023-10-19 02:09:25.867563 Epoch 89  	Train Loss = 18.67698 Val Loss = 19.59889
2023-10-19 02:12:50.375780 Epoch 90  	Train Loss = 18.66189 Val Loss = 19.67549
2023-10-19 02:16:14.865193 Epoch 91  	Train Loss = 18.65314 Val Loss = 19.67495
2023-10-19 02:19:39.554110 Epoch 92  	Train Loss = 18.63647 Val Loss = 19.62914
2023-10-19 02:23:04.077812 Epoch 93  	Train Loss = 18.64619 Val Loss = 19.61933
2023-10-19 02:26:28.770384 Epoch 94  	Train Loss = 18.61940 Val Loss = 19.61891
2023-10-19 02:29:53.568973 Epoch 95  	Train Loss = 18.62714 Val Loss = 19.82198
2023-10-19 02:33:18.645090 Epoch 96  	Train Loss = 18.61542 Val Loss = 19.81124
2023-10-19 02:36:43.578179 Epoch 97  	Train Loss = 18.60115 Val Loss = 19.47775
2023-10-19 02:40:08.744006 Epoch 98  	Train Loss = 18.58305 Val Loss = 19.51029
2023-10-19 02:43:33.676079 Epoch 99  	Train Loss = 18.57255 Val Loss = 19.56670
2023-10-19 02:46:58.049305 Epoch 100  	Train Loss = 18.53848 Val Loss = 19.51500
2023-10-19 02:50:21.962362 Epoch 101  	Train Loss = 18.38383 Val Loss = 19.40459
2023-10-19 02:53:46.594182 Epoch 102  	Train Loss = 18.35677 Val Loss = 19.43617
2023-10-19 02:57:11.507436 Epoch 103  	Train Loss = 18.35743 Val Loss = 19.39996
2023-10-19 03:00:36.834991 Epoch 104  	Train Loss = 18.35107 Val Loss = 19.39027
2023-10-19 03:04:02.008232 Epoch 105  	Train Loss = 18.33995 Val Loss = 19.39433
2023-10-19 03:07:26.401169 Epoch 106  	Train Loss = 18.32566 Val Loss = 19.39988
2023-10-19 03:10:51.035261 Epoch 107  	Train Loss = 18.33801 Val Loss = 19.40928
2023-10-19 03:14:16.255324 Epoch 108  	Train Loss = 18.34394 Val Loss = 19.39650
2023-10-19 03:17:41.355909 Epoch 109  	Train Loss = 18.33482 Val Loss = 19.39888
2023-10-19 03:21:06.186206 Epoch 110  	Train Loss = 18.32898 Val Loss = 19.39268
2023-10-19 03:24:31.099448 Epoch 111  	Train Loss = 18.32686 Val Loss = 19.40651
2023-10-19 03:27:55.865933 Epoch 112  	Train Loss = 18.32226 Val Loss = 19.39990
2023-10-19 03:31:19.464580 Epoch 113  	Train Loss = 18.31280 Val Loss = 19.37776
2023-10-19 03:34:44.385309 Epoch 114  	Train Loss = 18.31278 Val Loss = 19.38407
2023-10-19 03:38:08.923535 Epoch 115  	Train Loss = 18.31513 Val Loss = 19.39217
2023-10-19 03:41:33.847620 Epoch 116  	Train Loss = 18.30932 Val Loss = 19.37467
2023-10-19 03:44:58.415173 Epoch 117  	Train Loss = 18.31206 Val Loss = 19.40536
2023-10-19 03:48:23.143117 Epoch 118  	Train Loss = 18.30235 Val Loss = 19.38656
2023-10-19 03:51:48.247545 Epoch 119  	Train Loss = 18.29807 Val Loss = 19.38642
2023-10-19 03:55:13.159705 Epoch 120  	Train Loss = 18.30191 Val Loss = 19.39085
2023-10-19 03:58:37.836369 Epoch 121  	Train Loss = 18.29334 Val Loss = 19.39189
2023-10-19 04:02:02.775410 Epoch 122  	Train Loss = 18.29187 Val Loss = 19.37275
2023-10-19 04:05:27.304854 Epoch 123  	Train Loss = 18.28729 Val Loss = 19.40434
2023-10-19 04:08:51.908856 Epoch 124  	Train Loss = 18.29022 Val Loss = 19.38747
2023-10-19 04:12:16.094411 Epoch 125  	Train Loss = 18.27808 Val Loss = 19.36506
2023-10-19 04:15:40.882716 Epoch 126  	Train Loss = 18.28084 Val Loss = 19.40716
2023-10-19 04:19:05.411068 Epoch 127  	Train Loss = 18.28118 Val Loss = 19.38971
2023-10-19 04:22:30.023301 Epoch 128  	Train Loss = 18.27784 Val Loss = 19.37397
2023-10-19 04:25:54.085821 Epoch 129  	Train Loss = 18.27349 Val Loss = 19.38322
2023-10-19 04:29:18.400779 Epoch 130  	Train Loss = 18.27473 Val Loss = 19.37611
2023-10-19 04:32:43.325207 Epoch 131  	Train Loss = 18.25839 Val Loss = 19.41392
2023-10-19 04:36:08.597319 Epoch 132  	Train Loss = 18.26102 Val Loss = 19.39007
2023-10-19 04:39:34.155472 Epoch 133  	Train Loss = 18.26203 Val Loss = 19.37905
2023-10-19 04:42:59.424435 Epoch 134  	Train Loss = 18.25730 Val Loss = 19.38354
2023-10-19 04:46:24.685998 Epoch 135  	Train Loss = 18.25173 Val Loss = 19.37650
2023-10-19 04:49:49.622302 Epoch 136  	Train Loss = 18.25519 Val Loss = 19.37934
2023-10-19 04:53:14.208975 Epoch 137  	Train Loss = 18.25218 Val Loss = 19.37608
2023-10-19 04:56:38.981916 Epoch 138  	Train Loss = 18.25085 Val Loss = 19.39541
2023-10-19 05:00:03.544754 Epoch 139  	Train Loss = 18.24443 Val Loss = 19.40021
2023-10-19 05:03:28.043624 Epoch 140  	Train Loss = 18.24131 Val Loss = 19.37638
2023-10-19 05:06:52.102035 Epoch 141  	Train Loss = 18.23447 Val Loss = 19.39298
2023-10-19 05:10:16.564511 Epoch 142  	Train Loss = 18.24104 Val Loss = 19.36456
2023-10-19 05:13:41.860503 Epoch 143  	Train Loss = 18.23212 Val Loss = 19.38501
2023-10-19 05:17:06.890180 Epoch 144  	Train Loss = 18.23320 Val Loss = 19.36742
2023-10-19 05:20:32.047141 Epoch 145  	Train Loss = 18.23361 Val Loss = 19.37526
2023-10-19 05:23:57.098105 Epoch 146  	Train Loss = 18.23093 Val Loss = 19.37248
2023-10-19 05:27:21.885453 Epoch 147  	Train Loss = 18.22637 Val Loss = 19.38079
2023-10-19 05:30:46.317926 Epoch 148  	Train Loss = 18.22158 Val Loss = 19.38371
2023-10-19 05:34:10.665691 Epoch 149  	Train Loss = 18.21795 Val Loss = 19.36944
2023-10-19 05:37:35.369197 Epoch 150  	Train Loss = 18.21819 Val Loss = 19.37509
2023-10-19 05:40:59.731397 Epoch 151  	Train Loss = 18.21637 Val Loss = 19.37061
2023-10-19 05:44:24.444178 Epoch 152  	Train Loss = 18.21038 Val Loss = 19.37744
2023-10-19 05:47:49.110348 Epoch 153  	Train Loss = 18.21052 Val Loss = 19.37582
2023-10-19 05:51:13.981279 Epoch 154  	Train Loss = 18.20610 Val Loss = 19.37625
2023-10-19 05:54:39.744805 Epoch 155  	Train Loss = 18.20708 Val Loss = 19.36570
2023-10-19 05:58:05.296021 Epoch 156  	Train Loss = 18.20545 Val Loss = 19.37360
2023-10-19 06:01:30.063635 Epoch 157  	Train Loss = 18.20210 Val Loss = 19.37113
2023-10-19 06:04:55.116187 Epoch 158  	Train Loss = 18.19847 Val Loss = 19.38761
2023-10-19 06:08:20.202566 Epoch 159  	Train Loss = 18.20102 Val Loss = 19.36986
2023-10-19 06:11:44.793104 Epoch 160  	Train Loss = 18.19021 Val Loss = 19.38542
2023-10-19 06:15:09.186651 Epoch 161  	Train Loss = 18.19475 Val Loss = 19.38125
2023-10-19 06:18:33.407865 Epoch 162  	Train Loss = 18.18935 Val Loss = 19.35772
2023-10-19 06:21:58.414273 Epoch 163  	Train Loss = 18.18483 Val Loss = 19.37082
2023-10-19 06:25:23.224572 Epoch 164  	Train Loss = 18.18141 Val Loss = 19.38798
2023-10-19 06:28:48.143607 Epoch 165  	Train Loss = 18.18323 Val Loss = 19.36871
2023-10-19 06:32:12.819669 Epoch 166  	Train Loss = 18.18030 Val Loss = 19.37808
2023-10-19 06:35:37.262085 Epoch 167  	Train Loss = 18.18260 Val Loss = 19.39087
2023-10-19 06:39:02.172686 Epoch 168  	Train Loss = 18.17227 Val Loss = 19.35781
2023-10-19 06:42:27.062009 Epoch 169  	Train Loss = 18.16925 Val Loss = 19.37867
2023-10-19 06:45:52.192516 Epoch 170  	Train Loss = 18.17120 Val Loss = 19.37762
2023-10-19 06:49:17.142613 Epoch 171  	Train Loss = 18.16718 Val Loss = 19.35233
2023-10-19 06:52:41.983084 Epoch 172  	Train Loss = 18.16886 Val Loss = 19.35471
2023-10-19 06:56:07.051280 Epoch 173  	Train Loss = 18.16769 Val Loss = 19.36987
2023-10-19 06:59:32.141729 Epoch 174  	Train Loss = 18.16424 Val Loss = 19.35387
2023-10-19 07:02:57.494272 Epoch 175  	Train Loss = 18.15958 Val Loss = 19.35592
2023-10-19 07:06:22.388351 Epoch 176  	Train Loss = 18.16088 Val Loss = 19.35351
2023-10-19 07:09:47.364723 Epoch 177  	Train Loss = 18.15671 Val Loss = 19.37081
2023-10-19 07:13:12.681490 Epoch 178  	Train Loss = 18.15518 Val Loss = 19.36686
2023-10-19 07:16:37.094426 Epoch 179  	Train Loss = 18.14983 Val Loss = 19.37112
2023-10-19 07:20:01.769553 Epoch 180  	Train Loss = 18.15097 Val Loss = 19.35844
2023-10-19 07:23:26.195941 Epoch 181  	Train Loss = 18.14884 Val Loss = 19.36181
2023-10-19 07:26:50.660579 Epoch 182  	Train Loss = 18.14100 Val Loss = 19.34992
2023-10-19 07:30:15.707117 Epoch 183  	Train Loss = 18.14341 Val Loss = 19.37162
2023-10-19 07:33:41.078047 Epoch 184  	Train Loss = 18.13948 Val Loss = 19.36052
2023-10-19 07:37:06.142478 Epoch 185  	Train Loss = 18.14080 Val Loss = 19.35075
2023-10-19 07:40:30.915063 Epoch 186  	Train Loss = 18.13914 Val Loss = 19.36066
2023-10-19 07:43:56.279244 Epoch 187  	Train Loss = 18.13453 Val Loss = 19.34522
2023-10-19 07:47:21.085462 Epoch 188  	Train Loss = 18.13286 Val Loss = 19.35774
2023-10-19 07:50:46.233040 Epoch 189  	Train Loss = 18.13125 Val Loss = 19.35325
2023-10-19 07:54:10.856283 Epoch 190  	Train Loss = 18.12907 Val Loss = 19.35338
2023-10-19 07:57:35.923426 Epoch 191  	Train Loss = 18.12950 Val Loss = 19.35429
2023-10-19 08:01:01.208737 Epoch 192  	Train Loss = 18.12405 Val Loss = 19.35670
2023-10-19 08:04:26.067848 Epoch 193  	Train Loss = 18.12049 Val Loss = 19.36944
2023-10-19 08:07:51.087300 Epoch 194  	Train Loss = 18.12252 Val Loss = 19.35869
2023-10-19 08:11:16.129604 Epoch 195  	Train Loss = 18.11695 Val Loss = 19.34691
2023-10-19 08:14:41.133257 Epoch 196  	Train Loss = 18.11628 Val Loss = 19.35553
2023-10-19 08:18:06.621564 Epoch 197  	Train Loss = 18.11374 Val Loss = 19.35780
2023-10-19 08:21:31.409251 Epoch 198  	Train Loss = 18.11776 Val Loss = 19.35963
2023-10-19 08:24:56.267149 Epoch 199  	Train Loss = 18.10637 Val Loss = 19.34956
2023-10-19 08:28:21.397406 Epoch 200  	Train Loss = 18.10675 Val Loss = 19.35691
Early stopping at epoch: 200
Best at epoch 187:
Train Loss = 18.13453
Train RMSE = 30.50890, MAE = 18.11170, MAPE = 8.01975
Val Loss = 19.34522
Val RMSE = 32.65264, MAE = 19.31974, MAPE = 8.46168
--------- Test ---------
All Steps RMSE = 32.86476, MAE = 19.71360, MAPE = 8.29768
Step 1 RMSE = 26.52932, MAE = 16.49417, MAPE = 6.97706
Step 2 RMSE = 28.87893, MAE = 17.71449, MAPE = 7.47257
Step 3 RMSE = 30.28485, MAE = 18.42408, MAPE = 7.74561
Step 4 RMSE = 31.31481, MAE = 18.93383, MAPE = 7.94424
Step 5 RMSE = 32.18323, MAE = 19.37486, MAPE = 8.11591
Step 6 RMSE = 32.94618, MAE = 19.76985, MAPE = 8.29155
Step 7 RMSE = 33.65598, MAE = 20.14196, MAPE = 8.46133
Step 8 RMSE = 34.29107, MAE = 20.48164, MAPE = 8.61289
Step 9 RMSE = 34.86268, MAE = 20.80711, MAPE = 8.75785
Step 10 RMSE = 35.41828, MAE = 21.13111, MAPE = 8.90180
Step 11 RMSE = 35.96341, MAE = 21.46152, MAPE = 9.05607
Step 12 RMSE = 36.50138, MAE = 21.82614, MAPE = 9.23400
Inference time: 23.87 s
