METRLA
Trainset:	x-(23974, 12, 207, 1)	y-(23974, 12, 207, 2)
Valset:  	x-(3425, 12, 207, 1)  	y-(3425, 12, 207, 2)
Testset:	x-(6850, 12, 207, 1)	y-(6850, 12, 207, 2)

--------- MegaCRN ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "y_time_of_day": true,
    "runner": "megacrn",
    "loss": "megacrn",
    "loss_args": {
        "l1": 0.01,
        "l2": 0.01
    },
    "lr": 0.01,
    "eps": 0.001,
    "milestones": [
        50,
        100
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 30,
    "model_args": {
        "num_nodes": 207,
        "input_dim": 1,
        "output_dim": 1,
        "horizon": 12,
        "rnn_units": 64,
        "num_layers": 1,
        "cheb_k": 3,
        "ycov_dim": 1,
        "mem_num": 20,
        "mem_dim": 64,
        "tf_decay_steps": 2000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MegaCRN                                  [64, 12, 207, 1]          13,656
├─ADCRNN_Encoder: 1-1                    [64, 12, 207, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─AGCRNCell: 3-1               [64, 207, 64]             75,072
│    │    └─AGCRNCell: 3-2               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-3               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-4               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-5               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-6               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-7               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-8               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-9               [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-10              [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-11              [64, 207, 64]             (recursive)
│    │    └─AGCRNCell: 3-12              [64, 207, 64]             (recursive)
├─ADCRNN_Decoder: 1-2                    [64, 207, 128]            --
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-13              [64, 207, 128]            299,904
├─Sequential: 1-3                        [64, 207, 1]              --
│    └─Linear: 2-3                       [64, 207, 1]              129
├─ADCRNN_Decoder: 1-4                    [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-14              [64, 207, 128]            (recursive)
├─Sequential: 1-5                        [64, 207, 1]              (recursive)
│    └─Linear: 2-5                       [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-6                    [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-15              [64, 207, 128]            (recursive)
├─Sequential: 1-7                        [64, 207, 1]              (recursive)
│    └─Linear: 2-7                       [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-8                    [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-16              [64, 207, 128]            (recursive)
├─Sequential: 1-9                        [64, 207, 1]              (recursive)
│    └─Linear: 2-9                       [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-10                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-17              [64, 207, 128]            (recursive)
├─Sequential: 1-11                       [64, 207, 1]              (recursive)
│    └─Linear: 2-11                      [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-12                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-18              [64, 207, 128]            (recursive)
├─Sequential: 1-13                       [64, 207, 1]              (recursive)
│    └─Linear: 2-13                      [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-14                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-19              [64, 207, 128]            (recursive)
├─Sequential: 1-15                       [64, 207, 1]              (recursive)
│    └─Linear: 2-15                      [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-16                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-20              [64, 207, 128]            (recursive)
├─Sequential: 1-17                       [64, 207, 1]              (recursive)
│    └─Linear: 2-17                      [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-18                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-21              [64, 207, 128]            (recursive)
├─Sequential: 1-19                       [64, 207, 1]              (recursive)
│    └─Linear: 2-19                      [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-20                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-22              [64, 207, 128]            (recursive)
├─Sequential: 1-21                       [64, 207, 1]              (recursive)
│    └─Linear: 2-21                      [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-22                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-23              [64, 207, 128]            (recursive)
├─Sequential: 1-23                       [64, 207, 1]              (recursive)
│    └─Linear: 2-23                      [64, 207, 1]              (recursive)
├─ADCRNN_Decoder: 1-24                   [64, 207, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-24              [64, 207, 128]            (recursive)
├─Sequential: 1-25                       [64, 207, 1]              (recursive)
│    └─Linear: 2-25                      [64, 207, 1]              (recursive)
==========================================================================================
Total params: 388,761
Trainable params: 388,761
Non-trainable params: 0
Total mult-adds (G): 59.52
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 61.15
Params size (MB): 1.50
Estimated Total Size (MB): 63.93
==========================================================================================

Loss: MegaCRNLoss

2023-06-04 00:09:21.129715 Epoch 1  	Train Loss = 3.52640 Val Loss = 5.88957
2023-06-04 00:10:04.782559 Epoch 2  	Train Loss = 2.78608 Val Loss = 4.70572
2023-06-04 00:10:48.808725 Epoch 3  	Train Loss = 2.74844 Val Loss = 5.01261
2023-06-04 00:11:33.447993 Epoch 4  	Train Loss = 3.17643 Val Loss = 4.74860
2023-06-04 00:12:17.885467 Epoch 5  	Train Loss = 3.11739 Val Loss = 5.55361
2023-06-04 00:13:02.720503 Epoch 6  	Train Loss = 3.11680 Val Loss = 5.22975
2023-06-04 00:13:47.465125 Epoch 7  	Train Loss = 3.38534 Val Loss = 4.85901
2023-06-04 00:14:32.754849 Epoch 8  	Train Loss = 3.80978 Val Loss = 4.61811
2023-06-04 00:15:17.136818 Epoch 9  	Train Loss = 3.76135 Val Loss = 4.64693
2023-06-04 00:16:01.453700 Epoch 10  	Train Loss = 3.17758 Val Loss = 4.79205
2023-06-04 00:16:45.322842 Epoch 11  	Train Loss = 3.14317 Val Loss = 4.72411
2023-06-04 00:17:30.776887 Epoch 12  	Train Loss = 3.12512 Val Loss = 4.38848
2023-06-04 00:18:15.059649 Epoch 13  	Train Loss = 3.29509 Val Loss = 4.17103
2023-06-04 00:18:58.852674 Epoch 14  	Train Loss = 3.16099 Val Loss = 4.58574
2023-06-04 00:19:42.928851 Epoch 15  	Train Loss = 3.71195 Val Loss = 3.78376
2023-06-04 00:20:26.771226 Epoch 16  	Train Loss = 3.57127 Val Loss = 4.11848
2023-06-04 00:21:10.631603 Epoch 17  	Train Loss = 3.33088 Val Loss = 3.91229
2023-06-04 00:21:55.481909 Epoch 18  	Train Loss = 3.33949 Val Loss = 3.69370
2023-06-04 00:22:39.642518 Epoch 19  	Train Loss = 3.57991 Val Loss = 6.84634
2023-06-04 00:23:23.381633 Epoch 20  	Train Loss = 3.41111 Val Loss = 7.15366
2023-06-04 00:24:07.422692 Epoch 21  	Train Loss = 3.08124 Val Loss = 5.02490
2023-06-04 00:24:51.188517 Epoch 22  	Train Loss = 3.44838 Val Loss = 5.22379
2023-06-04 00:25:35.403981 Epoch 23  	Train Loss = 3.38917 Val Loss = 4.60872
2023-06-04 00:26:19.253412 Epoch 24  	Train Loss = 3.09178 Val Loss = 4.28450
2023-06-04 00:27:03.794277 Epoch 25  	Train Loss = 3.05696 Val Loss = 4.38314
2023-06-04 00:27:48.206288 Epoch 26  	Train Loss = 2.93883 Val Loss = 4.16753
2023-06-04 00:28:32.040656 Epoch 27  	Train Loss = 2.84207 Val Loss = 4.29299
2023-06-04 00:29:16.484815 Epoch 28  	Train Loss = 2.75130 Val Loss = 3.76857
2023-06-04 00:30:00.422784 Epoch 29  	Train Loss = 2.91818 Val Loss = 4.09056
2023-06-04 00:30:44.445076 Epoch 30  	Train Loss = 2.89120 Val Loss = 4.13903
2023-06-04 00:31:28.349513 Epoch 31  	Train Loss = 2.74951 Val Loss = 3.56420
2023-06-04 00:32:12.440989 Epoch 32  	Train Loss = 3.34311 Val Loss = 3.54295
2023-06-04 00:32:56.275951 Epoch 33  	Train Loss = 3.04970 Val Loss = 3.85932
2023-06-04 00:33:40.606637 Epoch 34  	Train Loss = 3.11475 Val Loss = 3.51693
2023-06-04 00:34:25.339115 Epoch 35  	Train Loss = 3.19741 Val Loss = 3.53167
2023-06-04 00:35:09.568617 Epoch 36  	Train Loss = 2.95584 Val Loss = 3.65552
2023-06-04 00:35:53.791414 Epoch 37  	Train Loss = 3.03324 Val Loss = 3.92367
2023-06-04 00:36:37.793113 Epoch 38  	Train Loss = 2.85726 Val Loss = 3.53777
2023-06-04 00:37:21.998711 Epoch 39  	Train Loss = 2.87202 Val Loss = 3.48585
2023-06-04 00:38:06.239276 Epoch 40  	Train Loss = 2.91951 Val Loss = 3.45097
2023-06-04 00:38:50.291637 Epoch 41  	Train Loss = 2.93612 Val Loss = 3.46296
2023-06-04 00:39:35.042476 Epoch 42  	Train Loss = 2.87681 Val Loss = 3.39279
2023-06-04 00:40:19.185157 Epoch 43  	Train Loss = 2.92539 Val Loss = 3.38056
2023-06-04 00:41:03.287297 Epoch 44  	Train Loss = 2.97140 Val Loss = 3.41559
2023-06-04 00:41:47.675958 Epoch 45  	Train Loss = 3.04309 Val Loss = 3.89786
2023-06-04 00:42:31.791179 Epoch 46  	Train Loss = 3.06449 Val Loss = 3.40842
2023-06-04 00:43:16.144073 Epoch 47  	Train Loss = 3.06114 Val Loss = 3.32959
2023-06-04 00:44:00.241745 Epoch 48  	Train Loss = 3.12077 Val Loss = 3.31823
2023-06-04 00:44:44.487292 Epoch 49  	Train Loss = 3.12591 Val Loss = 3.28968
2023-06-04 00:45:28.705265 Epoch 50  	Train Loss = 3.13179 Val Loss = 3.12316
2023-06-04 00:46:12.800038 Epoch 51  	Train Loss = 2.96725 Val Loss = 3.04992
2023-06-04 00:46:56.828397 Epoch 52  	Train Loss = 2.96938 Val Loss = 3.02822
2023-06-04 00:47:41.019141 Epoch 53  	Train Loss = 2.96880 Val Loss = 3.00959
2023-06-04 00:48:25.106597 Epoch 54  	Train Loss = 2.98146 Val Loss = 3.00761
2023-06-04 00:49:09.356811 Epoch 55  	Train Loss = 2.98883 Val Loss = 2.98996
2023-06-04 00:49:53.685831 Epoch 56  	Train Loss = 2.99102 Val Loss = 2.96720
2023-06-04 00:50:38.053219 Epoch 57  	Train Loss = 2.97832 Val Loss = 2.95147
2023-06-04 00:51:22.407007 Epoch 58  	Train Loss = 2.98090 Val Loss = 2.95170
2023-06-04 00:52:06.605808 Epoch 59  	Train Loss = 2.98314 Val Loss = 2.92926
2023-06-04 00:52:50.887845 Epoch 60  	Train Loss = 2.97562 Val Loss = 2.92791
2023-06-04 00:53:35.084057 Epoch 61  	Train Loss = 2.97370 Val Loss = 2.91752
2023-06-04 00:54:19.569221 Epoch 62  	Train Loss = 2.97931 Val Loss = 2.91303
2023-06-04 00:55:03.973202 Epoch 63  	Train Loss = 2.96715 Val Loss = 2.90243
2023-06-04 00:55:48.640572 Epoch 64  	Train Loss = 2.95466 Val Loss = 2.89683
2023-06-04 00:56:32.887909 Epoch 65  	Train Loss = 2.94644 Val Loss = 2.89318
2023-06-04 00:57:16.941947 Epoch 66  	Train Loss = 2.94294 Val Loss = 2.88723
2023-06-04 00:58:01.030442 Epoch 67  	Train Loss = 2.93022 Val Loss = 2.89580
2023-06-04 00:58:45.130311 Epoch 68  	Train Loss = 2.91780 Val Loss = 2.88490
2023-06-04 00:59:29.108992 Epoch 69  	Train Loss = 2.91720 Val Loss = 2.87772
2023-06-04 01:00:13.282900 Epoch 70  	Train Loss = 2.90286 Val Loss = 2.87234
2023-06-04 01:00:57.398540 Epoch 71  	Train Loss = 2.89485 Val Loss = 2.86087
2023-06-04 01:01:41.504808 Epoch 72  	Train Loss = 2.88481 Val Loss = 2.85861
2023-06-04 01:02:25.647709 Epoch 73  	Train Loss = 2.87540 Val Loss = 2.86721
2023-06-04 01:03:12.379252 Epoch 74  	Train Loss = 2.86783 Val Loss = 2.84843
2023-06-04 01:03:56.861509 Epoch 75  	Train Loss = 2.85721 Val Loss = 2.85872
2023-06-04 01:04:41.191198 Epoch 76  	Train Loss = 2.84890 Val Loss = 2.86496
2023-06-04 01:05:25.341806 Epoch 77  	Train Loss = 2.84096 Val Loss = 2.85820
2023-06-04 01:06:09.471996 Epoch 78  	Train Loss = 2.83428 Val Loss = 2.85876
2023-06-04 01:06:53.754408 Epoch 79  	Train Loss = 2.82647 Val Loss = 2.85258
2023-06-04 01:07:37.861285 Epoch 80  	Train Loss = 2.81884 Val Loss = 2.85695
2023-06-04 01:08:22.110371 Epoch 81  	Train Loss = 2.80782 Val Loss = 2.85343
2023-06-04 01:09:06.330939 Epoch 82  	Train Loss = 2.80078 Val Loss = 2.85167
2023-06-04 01:09:50.439816 Epoch 83  	Train Loss = 2.79455 Val Loss = 2.85556
2023-06-04 01:10:34.572948 Epoch 84  	Train Loss = 2.78648 Val Loss = 2.85357
2023-06-04 01:11:18.660719 Epoch 85  	Train Loss = 2.77941 Val Loss = 2.85461
2023-06-04 01:12:02.895752 Epoch 86  	Train Loss = 2.77591 Val Loss = 2.85207
2023-06-04 01:12:47.586133 Epoch 87  	Train Loss = 2.76760 Val Loss = 2.84412
2023-06-04 01:13:32.224961 Epoch 88  	Train Loss = 2.76050 Val Loss = 2.85132
2023-06-04 01:14:16.810478 Epoch 89  	Train Loss = 2.75234 Val Loss = 2.84950
2023-06-04 01:15:01.396177 Epoch 90  	Train Loss = 2.74878 Val Loss = 2.85755
2023-06-04 01:15:45.469238 Epoch 91  	Train Loss = 2.74279 Val Loss = 2.86475
2023-06-04 01:16:30.275506 Epoch 92  	Train Loss = 2.73393 Val Loss = 2.84224
2023-06-04 01:17:14.853478 Epoch 93  	Train Loss = 2.72805 Val Loss = 2.85160
2023-06-04 01:17:59.198666 Epoch 94  	Train Loss = 2.72020 Val Loss = 2.85794
2023-06-04 01:18:43.117844 Epoch 95  	Train Loss = 2.71825 Val Loss = 2.86279
2023-06-04 01:19:27.183880 Epoch 96  	Train Loss = 2.71800 Val Loss = 2.84965
2023-06-04 01:20:11.240117 Epoch 97  	Train Loss = 2.70714 Val Loss = 2.86176
2023-06-04 01:20:55.720482 Epoch 98  	Train Loss = 2.69940 Val Loss = 2.86276
2023-06-04 01:21:40.381008 Epoch 99  	Train Loss = 2.69170 Val Loss = 2.84884
2023-06-04 01:22:24.471383 Epoch 100  	Train Loss = 2.68956 Val Loss = 2.85685
2023-06-04 01:23:08.612497 Epoch 101  	Train Loss = 2.65513 Val Loss = 2.85404
2023-06-04 01:23:53.278961 Epoch 102  	Train Loss = 2.64915 Val Loss = 2.85835
2023-06-04 01:24:37.823944 Epoch 103  	Train Loss = 2.64705 Val Loss = 2.85951
2023-06-04 01:25:22.341191 Epoch 104  	Train Loss = 2.64570 Val Loss = 2.86101
2023-06-04 01:26:07.343978 Epoch 105  	Train Loss = 2.64401 Val Loss = 2.86071
2023-06-04 01:26:52.348334 Epoch 106  	Train Loss = 2.64270 Val Loss = 2.86050
2023-06-04 01:27:36.844076 Epoch 107  	Train Loss = 2.64174 Val Loss = 2.86098
2023-06-04 01:28:21.108484 Epoch 108  	Train Loss = 2.64027 Val Loss = 2.86171
2023-06-04 01:29:05.368617 Epoch 109  	Train Loss = 2.63953 Val Loss = 2.86480
2023-06-04 01:29:49.526277 Epoch 110  	Train Loss = 2.63811 Val Loss = 2.86251
2023-06-04 01:30:33.706658 Epoch 111  	Train Loss = 2.63706 Val Loss = 2.86478
2023-06-04 01:31:18.428971 Epoch 112  	Train Loss = 2.63585 Val Loss = 2.86397
2023-06-04 01:32:02.661683 Epoch 113  	Train Loss = 2.63568 Val Loss = 2.86604
2023-06-04 01:32:46.877342 Epoch 114  	Train Loss = 2.63450 Val Loss = 2.86721
2023-06-04 01:33:30.921468 Epoch 115  	Train Loss = 2.63323 Val Loss = 2.86651
2023-06-04 01:34:15.066977 Epoch 116  	Train Loss = 2.63212 Val Loss = 2.86697
2023-06-04 01:34:59.245005 Epoch 117  	Train Loss = 2.63156 Val Loss = 2.86827
2023-06-04 01:35:43.293977 Epoch 118  	Train Loss = 2.63059 Val Loss = 2.87143
2023-06-04 01:36:27.386367 Epoch 119  	Train Loss = 2.62966 Val Loss = 2.86865
2023-06-04 01:37:11.993154 Epoch 120  	Train Loss = 2.62891 Val Loss = 2.86614
2023-06-04 01:37:56.694043 Epoch 121  	Train Loss = 2.62835 Val Loss = 2.86560
2023-06-04 01:38:41.525705 Epoch 122  	Train Loss = 2.62735 Val Loss = 2.87169
Early stopping at epoch: 122
Best at epoch 92:
Train Loss = 2.73393
Train RMSE = 5.17421, MAE = 2.62326, MAPE = 6.85617
Val Loss = 2.84224
Val RMSE = 6.10297, MAE = 2.89420, MAPE = 8.09721
--------- Test ---------
All Steps RMSE = 6.63205, MAE = 3.22803, MAPE = 9.03199
Step 1 RMSE = 4.08188, MAE = 2.34150, MAPE = 5.71998
Step 2 RMSE = 4.87935, MAE = 2.61989, MAPE = 6.69232
Step 3 RMSE = 5.42303, MAE = 2.81612, MAPE = 7.42040
Step 4 RMSE = 5.90734, MAE = 2.98681, MAPE = 8.09061
Step 5 RMSE = 6.32407, MAE = 3.13474, MAPE = 8.66108
Step 6 RMSE = 6.67728, MAE = 3.26708, MAPE = 9.15932
Step 7 RMSE = 6.97895, MAE = 3.38150, MAPE = 9.60119
Step 8 RMSE = 7.23107, MAE = 3.47982, MAPE = 9.98379
Step 9 RMSE = 7.44173, MAE = 3.56505, MAPE = 10.31681
Step 10 RMSE = 7.62654, MAE = 3.64314, MAPE = 10.62853
Step 11 RMSE = 7.79288, MAE = 3.71552, MAPE = 10.92058
Step 12 RMSE = 7.94925, MAE = 3.78530, MAPE = 11.18953
Inference time: 4.72 s
