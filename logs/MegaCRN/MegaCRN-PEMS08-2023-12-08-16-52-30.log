PEMS08
Trainset:	x-(10700, 12, 170, 1)	y-(10700, 12, 170, 2)
Valset:  	x-(3567, 12, 170, 1)  	y-(3567, 12, 170, 2)
Testset:	x-(3566, 12, 170, 1)	y-(3566, 12, 170, 2)

--------- MegaCRN ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "y_time_of_day": true,
    "runner": "megacrn",
    "loss": "megacrn",
    "loss_args": {
        "l1": 0.01,
        "l2": 0.01
    },
    "lr": 0.01,
    "eps": 0.001,
    "milestones": [
        50,
        100
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 30,
    "model_args": {
        "num_nodes": 170,
        "input_dim": 1,
        "output_dim": 1,
        "horizon": 12,
        "rnn_units": 64,
        "num_layers": 1,
        "cheb_k": 3,
        "ycov_dim": 1,
        "mem_num": 20,
        "mem_dim": 64,
        "tf_decay_steps": 2000,
        "use_teacher_forcing": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MegaCRN                                  [64, 12, 170, 1]          12,176
├─ADCRNN_Encoder: 1-1                    [64, 12, 170, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─AGCRNCell: 3-1               [64, 170, 64]             75,072
│    │    └─AGCRNCell: 3-2               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-3               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-4               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-5               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-6               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-7               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-8               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-9               [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-10              [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-11              [64, 170, 64]             (recursive)
│    │    └─AGCRNCell: 3-12              [64, 170, 64]             (recursive)
├─ADCRNN_Decoder: 1-2                    [64, 170, 128]            --
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-13              [64, 170, 128]            299,904
├─Sequential: 1-3                        [64, 170, 1]              --
│    └─Linear: 2-3                       [64, 170, 1]              129
├─ADCRNN_Decoder: 1-4                    [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-14              [64, 170, 128]            (recursive)
├─Sequential: 1-5                        [64, 170, 1]              (recursive)
│    └─Linear: 2-5                       [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-6                    [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-15              [64, 170, 128]            (recursive)
├─Sequential: 1-7                        [64, 170, 1]              (recursive)
│    └─Linear: 2-7                       [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-8                    [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-16              [64, 170, 128]            (recursive)
├─Sequential: 1-9                        [64, 170, 1]              (recursive)
│    └─Linear: 2-9                       [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-10                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-17              [64, 170, 128]            (recursive)
├─Sequential: 1-11                       [64, 170, 1]              (recursive)
│    └─Linear: 2-11                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-12                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-18              [64, 170, 128]            (recursive)
├─Sequential: 1-13                       [64, 170, 1]              (recursive)
│    └─Linear: 2-13                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-14                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-19              [64, 170, 128]            (recursive)
├─Sequential: 1-15                       [64, 170, 1]              (recursive)
│    └─Linear: 2-15                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-16                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-20              [64, 170, 128]            (recursive)
├─Sequential: 1-17                       [64, 170, 1]              (recursive)
│    └─Linear: 2-17                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-18                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-21              [64, 170, 128]            (recursive)
├─Sequential: 1-19                       [64, 170, 1]              (recursive)
│    └─Linear: 2-19                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-20                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-22              [64, 170, 128]            (recursive)
├─Sequential: 1-21                       [64, 170, 1]              (recursive)
│    └─Linear: 2-21                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-22                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-23              [64, 170, 128]            (recursive)
├─Sequential: 1-23                       [64, 170, 1]              (recursive)
│    └─Linear: 2-23                      [64, 170, 1]              (recursive)
├─ADCRNN_Decoder: 1-24                   [64, 170, 128]            (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─AGCRNCell: 3-24              [64, 170, 128]            (recursive)
├─Sequential: 1-25                       [64, 170, 1]              (recursive)
│    └─Linear: 2-25                      [64, 170, 1]              (recursive)
==========================================================================================
Total params: 387,281
Trainable params: 387,281
Non-trainable params: 0
Total mult-adds (G): 48.88
==========================================================================================
Input size (MB): 1.04
Forward/backward pass size (MB): 602.66
Params size (MB): 1.50
Estimated Total Size (MB): 605.21
==========================================================================================

Loss: MegaCRNLoss

2023-12-08 16:53:10.192946 Epoch 1  	Train Loss = 22.26634 Val Loss = 29.31280
2023-12-08 16:53:46.969401 Epoch 2  	Train Loss = 14.98495 Val Loss = 23.81224
2023-12-08 16:54:24.259145 Epoch 3  	Train Loss = 14.56028 Val Loss = 25.32476
2023-12-08 16:55:00.961378 Epoch 4  	Train Loss = 14.47356 Val Loss = 20.68404
2023-12-08 16:55:37.222062 Epoch 5  	Train Loss = 14.40572 Val Loss = 21.93206
2023-12-08 16:56:13.307242 Epoch 6  	Train Loss = 14.04255 Val Loss = 22.67437
2023-12-08 16:56:49.953389 Epoch 7  	Train Loss = 14.13620 Val Loss = 22.74239
2023-12-08 16:57:26.082397 Epoch 8  	Train Loss = 13.80496 Val Loss = 22.00335
2023-12-08 16:58:03.575444 Epoch 9  	Train Loss = 13.83056 Val Loss = 19.19981
2023-12-08 16:58:39.903923 Epoch 10  	Train Loss = 13.64847 Val Loss = 20.83220
2023-12-08 16:59:16.230029 Epoch 11  	Train Loss = 13.61166 Val Loss = 20.95982
2023-12-08 16:59:52.103810 Epoch 12  	Train Loss = 13.20347 Val Loss = 19.29054
2023-12-08 17:00:28.412822 Epoch 13  	Train Loss = 13.18282 Val Loss = 20.55314
2023-12-08 17:01:04.462963 Epoch 14  	Train Loss = 13.10750 Val Loss = 18.91007
2023-12-08 17:01:40.435392 Epoch 15  	Train Loss = 13.28057 Val Loss = 20.14736
2023-12-08 17:02:16.692993 Epoch 16  	Train Loss = 13.05958 Val Loss = 18.67497
2023-12-08 17:02:52.534914 Epoch 17  	Train Loss = 13.49914 Val Loss = 21.83970
2023-12-08 17:03:28.349086 Epoch 18  	Train Loss = 13.18477 Val Loss = 20.18455
2023-12-08 17:04:05.187644 Epoch 19  	Train Loss = 12.99373 Val Loss = 18.74008
2023-12-08 17:04:41.297673 Epoch 20  	Train Loss = 12.76505 Val Loss = 20.55883
2023-12-08 17:05:17.994284 Epoch 21  	Train Loss = 12.93438 Val Loss = 17.09967
2023-12-08 17:05:54.409544 Epoch 22  	Train Loss = 12.78637 Val Loss = 19.46205
2023-12-08 17:06:31.118879 Epoch 23  	Train Loss = 12.72714 Val Loss = 21.85140
2023-12-08 17:07:07.932463 Epoch 24  	Train Loss = 12.88541 Val Loss = 19.04483
2023-12-08 17:07:44.986338 Epoch 25  	Train Loss = 12.56475 Val Loss = 17.77148
2023-12-08 17:08:21.301484 Epoch 26  	Train Loss = 12.61477 Val Loss = 18.23982
2023-12-08 17:08:58.183548 Epoch 27  	Train Loss = 12.67447 Val Loss = 21.09792
2023-12-08 17:09:35.075130 Epoch 28  	Train Loss = 12.80642 Val Loss = 16.65258
2023-12-08 17:10:10.761300 Epoch 29  	Train Loss = 12.37705 Val Loss = 17.28104
2023-12-08 17:10:47.501340 Epoch 30  	Train Loss = 12.33648 Val Loss = 18.12421
2023-12-08 17:11:23.155099 Epoch 31  	Train Loss = 12.38499 Val Loss = 18.92395
2023-12-08 17:11:58.959782 Epoch 32  	Train Loss = 12.41903 Val Loss = 20.98594
2023-12-08 17:12:35.935966 Epoch 33  	Train Loss = 12.71607 Val Loss = 16.87655
2023-12-08 17:13:11.989324 Epoch 34  	Train Loss = 12.29893 Val Loss = 16.68732
2023-12-08 17:13:48.453084 Epoch 35  	Train Loss = 12.51199 Val Loss = 16.74293
2023-12-08 17:14:25.047120 Epoch 36  	Train Loss = 12.24486 Val Loss = 16.18822
2023-12-08 17:15:01.306985 Epoch 37  	Train Loss = 12.19368 Val Loss = 16.19812
2023-12-08 17:15:37.788648 Epoch 38  	Train Loss = 12.38670 Val Loss = 17.35518
2023-12-08 17:16:14.291074 Epoch 39  	Train Loss = 12.16965 Val Loss = 17.57405
2023-12-08 17:16:50.286325 Epoch 40  	Train Loss = 12.27048 Val Loss = 16.70455
2023-12-08 17:17:26.574361 Epoch 41  	Train Loss = 12.11158 Val Loss = 17.42865
2023-12-08 17:18:02.646571 Epoch 42  	Train Loss = 12.19088 Val Loss = 18.50155
2023-12-08 17:18:38.871963 Epoch 43  	Train Loss = 12.11487 Val Loss = 16.14743
2023-12-08 17:19:15.636007 Epoch 44  	Train Loss = 11.97303 Val Loss = 16.76261
2023-12-08 17:19:51.591887 Epoch 45  	Train Loss = 12.08888 Val Loss = 16.49762
2023-12-08 17:20:28.231666 Epoch 46  	Train Loss = 12.05198 Val Loss = 16.84308
2023-12-08 17:21:04.655984 Epoch 47  	Train Loss = 12.05205 Val Loss = 15.98369
2023-12-08 17:21:41.545818 Epoch 48  	Train Loss = 12.02907 Val Loss = 16.14979
2023-12-08 17:22:18.354154 Epoch 49  	Train Loss = 11.88012 Val Loss = 15.60669
2023-12-08 17:22:55.209962 Epoch 50  	Train Loss = 11.98935 Val Loss = 17.16700
2023-12-08 17:23:31.467791 Epoch 51  	Train Loss = 11.61158 Val Loss = 15.52703
2023-12-08 17:24:08.909451 Epoch 52  	Train Loss = 11.55956 Val Loss = 15.42644
2023-12-08 17:24:45.592223 Epoch 53  	Train Loss = 11.54218 Val Loss = 15.50011
2023-12-08 17:25:22.554001 Epoch 54  	Train Loss = 11.53430 Val Loss = 15.43903
2023-12-08 17:25:57.218577 Epoch 55  	Train Loss = 11.51139 Val Loss = 15.37619
2023-12-08 17:26:32.909291 Epoch 56  	Train Loss = 11.50840 Val Loss = 15.64815
2023-12-08 17:27:08.996997 Epoch 57  	Train Loss = 11.50300 Val Loss = 15.62270
2023-12-08 17:27:45.472258 Epoch 58  	Train Loss = 11.48759 Val Loss = 15.51466
2023-12-08 17:28:22.054346 Epoch 59  	Train Loss = 11.50656 Val Loss = 15.44668
2023-12-08 17:28:58.324571 Epoch 60  	Train Loss = 11.49167 Val Loss = 15.39154
2023-12-08 17:29:34.063534 Epoch 61  	Train Loss = 11.48101 Val Loss = 15.45397
2023-12-08 17:30:09.401973 Epoch 62  	Train Loss = 11.48914 Val Loss = 15.58571
2023-12-08 17:30:46.678602 Epoch 63  	Train Loss = 11.48692 Val Loss = 15.40836
2023-12-08 17:31:22.544107 Epoch 64  	Train Loss = 11.50298 Val Loss = 15.63362
2023-12-08 17:31:58.721700 Epoch 65  	Train Loss = 11.50872 Val Loss = 15.48720
2023-12-08 17:32:35.008560 Epoch 66  	Train Loss = 11.49472 Val Loss = 15.57158
2023-12-08 17:33:10.998807 Epoch 67  	Train Loss = 11.50902 Val Loss = 15.41078
2023-12-08 17:33:47.352411 Epoch 68  	Train Loss = 11.52459 Val Loss = 15.49809
2023-12-08 17:34:23.288129 Epoch 69  	Train Loss = 11.53728 Val Loss = 15.39005
2023-12-08 17:34:59.549535 Epoch 70  	Train Loss = 11.51696 Val Loss = 15.47235
2023-12-08 17:35:35.822636 Epoch 71  	Train Loss = 11.52869 Val Loss = 15.39564
2023-12-08 17:36:12.576852 Epoch 72  	Train Loss = 11.52817 Val Loss = 15.71285
2023-12-08 17:36:47.868731 Epoch 73  	Train Loss = 11.57823 Val Loss = 15.35970
2023-12-08 17:37:24.381615 Epoch 74  	Train Loss = 11.55745 Val Loss = 15.33340
2023-12-08 17:37:58.691392 Epoch 75  	Train Loss = 11.57056 Val Loss = 15.51612
2023-12-08 17:38:35.048501 Epoch 76  	Train Loss = 11.60342 Val Loss = 15.41068
2023-12-08 17:39:11.008173 Epoch 77  	Train Loss = 11.59979 Val Loss = 15.39188
2023-12-08 17:39:47.553142 Epoch 78  	Train Loss = 11.63135 Val Loss = 15.40086
2023-12-08 17:40:22.118059 Epoch 79  	Train Loss = 11.68306 Val Loss = 15.45339
2023-12-08 17:40:58.087637 Epoch 80  	Train Loss = 11.69679 Val Loss = 15.38348
2023-12-08 17:41:35.866369 Epoch 81  	Train Loss = 11.68387 Val Loss = 15.43050
2023-12-08 17:42:12.672821 Epoch 82  	Train Loss = 11.74539 Val Loss = 15.38278
2023-12-08 17:42:49.153590 Epoch 83  	Train Loss = 11.72657 Val Loss = 15.37089
2023-12-08 17:43:25.733756 Epoch 84  	Train Loss = 11.77412 Val Loss = 15.40067
2023-12-08 17:44:01.949793 Epoch 85  	Train Loss = 11.82839 Val Loss = 15.38688
2023-12-08 17:44:38.998016 Epoch 86  	Train Loss = 11.78483 Val Loss = 15.35410
2023-12-08 17:45:14.000997 Epoch 87  	Train Loss = 11.87026 Val Loss = 15.51317
2023-12-08 17:45:50.795015 Epoch 88  	Train Loss = 11.87088 Val Loss = 15.34787
2023-12-08 17:46:26.660601 Epoch 89  	Train Loss = 11.86091 Val Loss = 15.35935
2023-12-08 17:47:02.854008 Epoch 90  	Train Loss = 11.97108 Val Loss = 15.49259
2023-12-08 17:47:39.688237 Epoch 91  	Train Loss = 12.04441 Val Loss = 15.57673
2023-12-08 17:48:16.538799 Epoch 92  	Train Loss = 12.00597 Val Loss = 15.38276
2023-12-08 17:48:53.407052 Epoch 93  	Train Loss = 12.04056 Val Loss = 15.33536
2023-12-08 17:49:30.302953 Epoch 94  	Train Loss = 12.10643 Val Loss = 15.42301
2023-12-08 17:50:07.009801 Epoch 95  	Train Loss = 12.17328 Val Loss = 15.63062
2023-12-08 17:50:43.645253 Epoch 96  	Train Loss = 12.15854 Val Loss = 15.46193
2023-12-08 17:51:19.945313 Epoch 97  	Train Loss = 12.23837 Val Loss = 15.43628
2023-12-08 17:51:54.812865 Epoch 98  	Train Loss = 12.22601 Val Loss = 15.34441
2023-12-08 17:52:31.006139 Epoch 99  	Train Loss = 12.27643 Val Loss = 15.40530
2023-12-08 17:53:06.807684 Epoch 100  	Train Loss = 12.32373 Val Loss = 15.43913
2023-12-08 17:53:42.481479 Epoch 101  	Train Loss = 12.23754 Val Loss = 15.30816
2023-12-08 17:54:18.264084 Epoch 102  	Train Loss = 12.20159 Val Loss = 15.37753
2023-12-08 17:54:54.137447 Epoch 103  	Train Loss = 12.30516 Val Loss = 15.35266
2023-12-08 17:55:30.079851 Epoch 104  	Train Loss = 12.31657 Val Loss = 15.31470
2023-12-08 17:56:07.197575 Epoch 105  	Train Loss = 12.30670 Val Loss = 15.32886
2023-12-08 17:56:42.967379 Epoch 106  	Train Loss = 12.37488 Val Loss = 15.34964
2023-12-08 17:57:19.596017 Epoch 107  	Train Loss = 12.37525 Val Loss = 15.37327
2023-12-08 17:57:55.842755 Epoch 108  	Train Loss = 12.42320 Val Loss = 15.39162
2023-12-08 17:58:32.591923 Epoch 109  	Train Loss = 12.49852 Val Loss = 15.35964
2023-12-08 17:59:08.647441 Epoch 110  	Train Loss = 12.46298 Val Loss = 15.34289
2023-12-08 17:59:44.872468 Epoch 111  	Train Loss = 12.51694 Val Loss = 15.38457
2023-12-08 18:00:21.648699 Epoch 112  	Train Loss = 12.57722 Val Loss = 15.32568
2023-12-08 18:00:58.161333 Epoch 113  	Train Loss = 12.59412 Val Loss = 15.39346
2023-12-08 18:01:34.911809 Epoch 114  	Train Loss = 12.56471 Val Loss = 15.37694
2023-12-08 18:02:11.264064 Epoch 115  	Train Loss = 12.56861 Val Loss = 15.35978
2023-12-08 18:02:47.444959 Epoch 116  	Train Loss = 12.64146 Val Loss = 15.35433
2023-12-08 18:03:24.262791 Epoch 117  	Train Loss = 12.64095 Val Loss = 15.34800
2023-12-08 18:04:00.398858 Epoch 118  	Train Loss = 12.64418 Val Loss = 15.36695
2023-12-08 18:04:36.617431 Epoch 119  	Train Loss = 12.69262 Val Loss = 15.36891
2023-12-08 18:05:12.863294 Epoch 120  	Train Loss = 12.67635 Val Loss = 15.39997
2023-12-08 18:05:48.532796 Epoch 121  	Train Loss = 12.69874 Val Loss = 15.39232
2023-12-08 18:06:24.386918 Epoch 122  	Train Loss = 12.70818 Val Loss = 15.38100
2023-12-08 18:06:59.943882 Epoch 123  	Train Loss = 12.74502 Val Loss = 15.40543
2023-12-08 18:07:35.980813 Epoch 124  	Train Loss = 12.70580 Val Loss = 15.45271
2023-12-08 18:08:12.704794 Epoch 125  	Train Loss = 12.76550 Val Loss = 15.39462
2023-12-08 18:08:48.589767 Epoch 126  	Train Loss = 12.71436 Val Loss = 15.38750
2023-12-08 18:09:24.803604 Epoch 127  	Train Loss = 12.71238 Val Loss = 15.39716
2023-12-08 18:10:01.072383 Epoch 128  	Train Loss = 12.74626 Val Loss = 15.44707
2023-12-08 18:10:37.418535 Epoch 129  	Train Loss = 12.74966 Val Loss = 15.39079
2023-12-08 18:11:14.180602 Epoch 130  	Train Loss = 12.74938 Val Loss = 15.40416
2023-12-08 18:11:49.631540 Epoch 131  	Train Loss = 12.74886 Val Loss = 15.41990
Early stopping at epoch: 131
Best at epoch 101:
Train Loss = 12.23754
Train RMSE = 22.50532, MAE = 13.07764, MAPE = 8.54160
Val Loss = 15.30816
Val RMSE = 25.76015, MAE = 15.30014, MAPE = 13.35420
--------- Test ---------
All Steps RMSE = 23.72738, MAE = 14.74527, MAPE = 9.47855
Step 1 RMSE = 19.32750, MAE = 12.33275, MAPE = 8.09902
Step 2 RMSE = 20.79913, MAE = 13.14837, MAPE = 8.50726
Step 3 RMSE = 21.72051, MAE = 13.66412, MAPE = 8.79193
Step 4 RMSE = 22.48592, MAE = 14.04001, MAPE = 9.02973
Step 5 RMSE = 23.13970, MAE = 14.40098, MAPE = 9.25258
Step 6 RMSE = 23.71553, MAE = 14.73684, MAPE = 9.45992
Step 7 RMSE = 24.25839, MAE = 15.05605, MAPE = 9.64951
Step 8 RMSE = 24.75311, MAE = 15.35683, MAPE = 9.83184
Step 9 RMSE = 25.19893, MAE = 15.62761, MAPE = 10.00316
Step 10 RMSE = 25.63421, MAE = 15.90084, MAPE = 10.17791
Step 11 RMSE = 26.05399, MAE = 16.18558, MAPE = 10.36606
Step 12 RMSE = 26.50180, MAE = 16.49321, MAPE = 10.57384
Inference time: 3.81 s
