PEMS03
Trainset:	x-(15711, 12, 358, 2)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 2)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 2)	y-(5237, 12, 358, 1)

--------- STNorm ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "lr": 0.01,
    "weight_decay": 0.0001,
    "clip_grad": 5,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "model_args": {
        "num_nodes": 358,
        "tnorm_bool": true,
        "snorm_bool": true,
        "in_dim": 2,
        "out_dim": 12,
        "channels": 32,
        "kernel_size": 2,
        "blocks": 4,
        "layers": 2
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STNorm                                   [64, 12, 358, 1]          --
├─Conv2d: 1-1                            [64, 32, 358, 13]         96
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-1                        [64, 32, 358, 13]         22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-2                        [64, 32, 358, 13]         64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-3                       [64, 32, 358, 12]         6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-4                       [64, 32, 358, 12]         6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-5                       [64, 32, 358, 12]         1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-6                       [64, 32, 358, 12]         1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-7                        [64, 32, 358, 12]         22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-8                        [64, 32, 358, 12]         64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-9                       [64, 32, 358, 10]         6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-10                      [64, 32, 358, 10]         6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-11                      [64, 32, 358, 10]         1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-12                      [64, 32, 358, 10]         1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-13                       [64, 32, 358, 10]         22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-14                       [64, 32, 358, 10]         64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-15                      [64, 32, 358, 9]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-16                      [64, 32, 358, 9]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-17                      [64, 32, 358, 9]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-18                      [64, 32, 358, 9]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-19                       [64, 32, 358, 9]          22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-20                       [64, 32, 358, 9]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-21                      [64, 32, 358, 7]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-22                      [64, 32, 358, 7]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-23                      [64, 32, 358, 7]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-24                      [64, 32, 358, 7]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-25                       [64, 32, 358, 7]          22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-26                       [64, 32, 358, 7]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-27                      [64, 32, 358, 6]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-28                      [64, 32, 358, 6]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-29                      [64, 32, 358, 6]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-30                      [64, 32, 358, 6]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-31                       [64, 32, 358, 6]          22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-32                       [64, 32, 358, 6]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-33                      [64, 32, 358, 4]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-34                      [64, 32, 358, 4]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-35                      [64, 32, 358, 4]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-36                      [64, 32, 358, 4]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-37                       [64, 32, 358, 4]          22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-38                       [64, 32, 358, 4]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-39                      [64, 32, 358, 3]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-40                      [64, 32, 358, 3]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-41                      [64, 32, 358, 3]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-42                      [64, 32, 358, 3]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-43                       [64, 32, 358, 3]          22,912
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-44                       [64, 32, 358, 3]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-45                      [64, 32, 358, 1]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-46                      [64, 32, 358, 1]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-47                      [64, 32, 358, 1]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-48                      [64, 32, 358, 1]          1,056
├─Conv2d: 1-50                           [64, 32, 358, 1]          1,056
├─Conv2d: 1-51                           [64, 12, 358, 1]          396
==========================================================================================
Total params: 301,068
Trainable params: 301,068
Non-trainable params: 0
Total mult-adds (G): 17.29
==========================================================================================
Input size (MB): 2.20
Forward/backward pass size (MB): 2055.11
Params size (MB): 1.20
Estimated Total Size (MB): 2058.52
==========================================================================================

Loss: HuberLoss

2023-06-01 18:17:41.353025 Epoch 1  	Train Loss = 22.96586 Val Loss = 16.44278
2023-06-01 18:18:00.943312 Epoch 2  	Train Loss = 17.05213 Val Loss = 17.59302
2023-06-01 18:18:20.556134 Epoch 3  	Train Loss = 16.39499 Val Loss = 17.03088
2023-06-01 18:18:40.208887 Epoch 4  	Train Loss = 15.53428 Val Loss = 15.78961
2023-06-01 18:18:59.803109 Epoch 5  	Train Loss = 15.69950 Val Loss = 15.78038
2023-06-01 18:19:19.303389 Epoch 6  	Train Loss = 15.14496 Val Loss = 15.00221
2023-06-01 18:19:38.954007 Epoch 7  	Train Loss = 14.98406 Val Loss = 16.20200
2023-06-01 18:19:58.919460 Epoch 8  	Train Loss = 14.85775 Val Loss = 15.24860
2023-06-01 18:20:18.564946 Epoch 9  	Train Loss = 14.90071 Val Loss = 15.21934
2023-06-01 18:20:38.241712 Epoch 10  	Train Loss = 14.61753 Val Loss = 15.53750
2023-06-01 18:20:57.893898 Epoch 11  	Train Loss = 13.49848 Val Loss = 13.98687
2023-06-01 18:21:17.594077 Epoch 12  	Train Loss = 13.24771 Val Loss = 13.94968
2023-06-01 18:21:37.366692 Epoch 13  	Train Loss = 13.11345 Val Loss = 13.90279
2023-06-01 18:21:56.946113 Epoch 14  	Train Loss = 13.04741 Val Loss = 13.75960
2023-06-01 18:22:16.779528 Epoch 15  	Train Loss = 12.97215 Val Loss = 13.66950
2023-06-01 18:22:36.451549 Epoch 16  	Train Loss = 12.89465 Val Loss = 13.63281
2023-06-01 18:22:56.158107 Epoch 17  	Train Loss = 12.82908 Val Loss = 13.57069
2023-06-01 18:23:15.973967 Epoch 18  	Train Loss = 12.76569 Val Loss = 13.60845
2023-06-01 18:23:35.465855 Epoch 19  	Train Loss = 12.72153 Val Loss = 13.51017
2023-06-01 18:23:55.122067 Epoch 20  	Train Loss = 12.69075 Val Loss = 13.60546
2023-06-01 18:24:14.864109 Epoch 21  	Train Loss = 12.65666 Val Loss = 13.59337
2023-06-01 18:24:34.686275 Epoch 22  	Train Loss = 12.63041 Val Loss = 13.48356
2023-06-01 18:24:54.475555 Epoch 23  	Train Loss = 12.61080 Val Loss = 13.61206
2023-06-01 18:25:14.333185 Epoch 24  	Train Loss = 12.58178 Val Loss = 13.45437
2023-06-01 18:25:34.075206 Epoch 25  	Train Loss = 12.58327 Val Loss = 13.47328
2023-06-01 18:25:53.742840 Epoch 26  	Train Loss = 12.53151 Val Loss = 13.49601
2023-06-01 18:26:13.418418 Epoch 27  	Train Loss = 12.51192 Val Loss = 13.45126
2023-06-01 18:26:33.009665 Epoch 28  	Train Loss = 12.48188 Val Loss = 13.48942
2023-06-01 18:26:52.599959 Epoch 29  	Train Loss = 12.46023 Val Loss = 13.48408
2023-06-01 18:27:11.983788 Epoch 30  	Train Loss = 12.43261 Val Loss = 13.45230
2023-06-01 18:27:31.440247 Epoch 31  	Train Loss = 12.28007 Val Loss = 13.35886
2023-06-01 18:27:51.208098 Epoch 32  	Train Loss = 12.25989 Val Loss = 13.38314
2023-06-01 18:28:10.946138 Epoch 33  	Train Loss = 12.24917 Val Loss = 13.36736
2023-06-01 18:28:30.643408 Epoch 34  	Train Loss = 12.24086 Val Loss = 13.36459
2023-06-01 18:28:50.471849 Epoch 35  	Train Loss = 12.23464 Val Loss = 13.39237
2023-06-01 18:29:10.167161 Epoch 36  	Train Loss = 12.22833 Val Loss = 13.36134
2023-06-01 18:29:29.876269 Epoch 37  	Train Loss = 12.22340 Val Loss = 13.36149
2023-06-01 18:29:49.588439 Epoch 38  	Train Loss = 12.21766 Val Loss = 13.38696
2023-06-01 18:30:09.270682 Epoch 39  	Train Loss = 12.20965 Val Loss = 13.35707
2023-06-01 18:30:28.992419 Epoch 40  	Train Loss = 12.20867 Val Loss = 13.36970
2023-06-01 18:30:48.743064 Epoch 41  	Train Loss = 12.20426 Val Loss = 13.38530
2023-06-01 18:31:08.431001 Epoch 42  	Train Loss = 12.19803 Val Loss = 13.38746
2023-06-01 18:31:28.133612 Epoch 43  	Train Loss = 12.19215 Val Loss = 13.38882
2023-06-01 18:31:47.808330 Epoch 44  	Train Loss = 12.18656 Val Loss = 13.37060
2023-06-01 18:32:07.265401 Epoch 45  	Train Loss = 12.18605 Val Loss = 13.36296
2023-06-01 18:32:26.725077 Epoch 46  	Train Loss = 12.18348 Val Loss = 13.35851
2023-06-01 18:32:46.294317 Epoch 47  	Train Loss = 12.17707 Val Loss = 13.36608
2023-06-01 18:33:05.770962 Epoch 48  	Train Loss = 12.17147 Val Loss = 13.37833
2023-06-01 18:33:25.314251 Epoch 49  	Train Loss = 12.16469 Val Loss = 13.35836
Early stopping at epoch: 49
Best at epoch 39:
Train Loss = 12.20965
Train RMSE = 20.66753, MAE = 12.63544, MAPE = 11.73485
Val Loss = 13.35707
Val RMSE = 22.34226, MAE = 13.87229, MAPE = 12.65821
--------- Test ---------
All Steps RMSE = 27.00487, MAE = 15.09946, MAPE = 14.06369
Step 1 RMSE = 21.82678, MAE = 12.63176, MAPE = 12.41206
Step 2 RMSE = 23.60406, MAE = 13.41847, MAPE = 12.91108
Step 3 RMSE = 25.26802, MAE = 14.15522, MAPE = 13.36312
Step 4 RMSE = 26.39497, MAE = 14.64721, MAPE = 13.72544
Step 5 RMSE = 26.98489, MAE = 14.94012, MAPE = 13.85864
Step 6 RMSE = 27.34561, MAE = 15.17154, MAPE = 13.99756
Step 7 RMSE = 27.74457, MAE = 15.45532, MAPE = 14.30956
Step 8 RMSE = 28.19270, MAE = 15.74654, MAPE = 14.59087
Step 9 RMSE = 28.56313, MAE = 15.97465, MAPE = 14.68595
Step 10 RMSE = 28.78515, MAE = 16.14349, MAPE = 14.71554
Step 11 RMSE = 28.96433, MAE = 16.31006, MAPE = 14.92196
Step 12 RMSE = 29.30005, MAE = 16.59918, MAPE = 15.27257
Inference time: 1.32 s
