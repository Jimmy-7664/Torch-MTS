PEMS04
Trainset:	x-(10181, 12, 307, 2)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 2)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 2)	y-(3394, 12, 307, 1)

--------- STNorm ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "lr": 0.01,
    "weight_decay": 0.0001,
    "clip_grad": 5,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "model_args": {
        "num_nodes": 307,
        "tnorm_bool": true,
        "snorm_bool": true,
        "in_dim": 2,
        "out_dim": 12,
        "channels": 32,
        "kernel_size": 2,
        "blocks": 4,
        "layers": 2
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STNorm                                   [64, 12, 307, 1]          --
├─Conv2d: 1-1                            [64, 32, 307, 13]         96
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-1                        [64, 32, 307, 13]         19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-2                        [64, 32, 307, 13]         64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-3                       [64, 32, 307, 12]         6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-4                       [64, 32, 307, 12]         6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-5                       [64, 32, 307, 12]         1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-6                       [64, 32, 307, 12]         1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-7                        [64, 32, 307, 12]         19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-8                        [64, 32, 307, 12]         64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-9                       [64, 32, 307, 10]         6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-10                      [64, 32, 307, 10]         6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-11                      [64, 32, 307, 10]         1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-12                      [64, 32, 307, 10]         1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-13                       [64, 32, 307, 10]         19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-14                       [64, 32, 307, 10]         64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-15                      [64, 32, 307, 9]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-16                      [64, 32, 307, 9]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-17                      [64, 32, 307, 9]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-18                      [64, 32, 307, 9]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-19                       [64, 32, 307, 9]          19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-20                       [64, 32, 307, 9]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-21                      [64, 32, 307, 7]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-22                      [64, 32, 307, 7]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-23                      [64, 32, 307, 7]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-24                      [64, 32, 307, 7]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-25                       [64, 32, 307, 7]          19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-26                       [64, 32, 307, 7]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-27                      [64, 32, 307, 6]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-28                      [64, 32, 307, 6]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-29                      [64, 32, 307, 6]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-30                      [64, 32, 307, 6]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-31                       [64, 32, 307, 6]          19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-32                       [64, 32, 307, 6]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-33                      [64, 32, 307, 4]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-34                      [64, 32, 307, 4]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-35                      [64, 32, 307, 4]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-36                      [64, 32, 307, 4]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-37                       [64, 32, 307, 4]          19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-38                       [64, 32, 307, 4]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-39                      [64, 32, 307, 3]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-40                      [64, 32, 307, 3]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-41                      [64, 32, 307, 3]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-42                      [64, 32, 307, 3]          1,056
├─ModuleList: 1-44                       --                        (recursive)
│    └─TNorm: 2-43                       [64, 32, 307, 3]          19,648
├─ModuleList: 1-45                       --                        (recursive)
│    └─SNorm: 2-44                       [64, 32, 307, 3]          64
├─ModuleList: 1-46                       --                        (recursive)
│    └─Conv2d: 2-45                      [64, 32, 307, 1]          6,176
├─ModuleList: 1-47                       --                        (recursive)
│    └─Conv2d: 2-46                      [64, 32, 307, 1]          6,176
├─ModuleList: 1-48                       --                        (recursive)
│    └─Conv2d: 2-47                      [64, 32, 307, 1]          1,056
├─ModuleList: 1-49                       --                        (recursive)
│    └─Conv2d: 2-48                      [64, 32, 307, 1]          1,056
├─Conv2d: 1-50                           [64, 32, 307, 1]          1,056
├─Conv2d: 1-51                           [64, 12, 307, 1]          396
==========================================================================================
Total params: 274,956
Trainable params: 274,956
Non-trainable params: 0
Total mult-adds (G): 14.83
==========================================================================================
Input size (MB): 1.89
Forward/backward pass size (MB): 1762.35
Params size (MB): 1.10
Estimated Total Size (MB): 1765.33
==========================================================================================

Loss: HuberLoss

2023-06-02 09:47:27.209764 Epoch 1  	Train Loss = 30.38718 Val Loss = 29.96921
2023-06-02 09:47:38.700725 Epoch 2  	Train Loss = 22.74916 Val Loss = 22.62612
2023-06-02 09:47:50.210018 Epoch 3  	Train Loss = 21.18197 Val Loss = 26.42966
2023-06-02 09:48:01.751408 Epoch 4  	Train Loss = 21.20045 Val Loss = 21.75048
2023-06-02 09:48:13.296456 Epoch 5  	Train Loss = 20.64525 Val Loss = 20.73197
2023-06-02 09:48:24.775320 Epoch 6  	Train Loss = 20.09731 Val Loss = 21.65881
2023-06-02 09:48:36.320146 Epoch 7  	Train Loss = 19.68282 Val Loss = 19.71330
2023-06-02 09:48:47.841209 Epoch 8  	Train Loss = 20.45094 Val Loss = 20.73218
2023-06-02 09:48:59.434972 Epoch 9  	Train Loss = 19.96937 Val Loss = 22.33735
2023-06-02 09:49:11.060458 Epoch 10  	Train Loss = 19.68158 Val Loss = 20.74579
2023-06-02 09:49:22.740459 Epoch 11  	Train Loss = 18.23790 Val Loss = 18.66193
2023-06-02 09:49:34.242179 Epoch 12  	Train Loss = 17.88254 Val Loss = 18.71076
2023-06-02 09:49:45.782992 Epoch 13  	Train Loss = 17.74139 Val Loss = 18.51318
2023-06-02 09:49:57.466543 Epoch 14  	Train Loss = 17.59245 Val Loss = 18.54302
2023-06-02 09:50:09.093860 Epoch 15  	Train Loss = 17.52771 Val Loss = 18.48622
2023-06-02 09:50:20.644140 Epoch 16  	Train Loss = 17.43935 Val Loss = 18.47471
2023-06-02 09:50:31.849227 Epoch 17  	Train Loss = 17.39384 Val Loss = 18.34738
2023-06-02 09:50:43.110973 Epoch 18  	Train Loss = 17.30524 Val Loss = 18.47336
2023-06-02 09:50:54.613173 Epoch 19  	Train Loss = 17.21865 Val Loss = 18.53711
2023-06-02 09:51:06.090921 Epoch 20  	Train Loss = 17.17870 Val Loss = 18.27507
2023-06-02 09:51:17.596617 Epoch 21  	Train Loss = 17.17710 Val Loss = 18.43408
2023-06-02 09:51:29.113265 Epoch 22  	Train Loss = 17.09748 Val Loss = 18.31027
2023-06-02 09:51:40.690665 Epoch 23  	Train Loss = 17.08955 Val Loss = 18.41197
2023-06-02 09:51:52.224205 Epoch 24  	Train Loss = 17.04030 Val Loss = 18.20511
2023-06-02 09:52:03.854192 Epoch 25  	Train Loss = 16.99604 Val Loss = 18.33846
2023-06-02 09:52:15.487479 Epoch 26  	Train Loss = 16.94974 Val Loss = 18.28175
2023-06-02 09:52:27.051138 Epoch 27  	Train Loss = 16.90802 Val Loss = 18.25161
2023-06-02 09:52:38.643989 Epoch 28  	Train Loss = 16.87837 Val Loss = 18.26248
2023-06-02 09:52:50.201396 Epoch 29  	Train Loss = 16.87579 Val Loss = 18.20918
2023-06-02 09:53:01.753168 Epoch 30  	Train Loss = 16.82480 Val Loss = 18.19749
2023-06-02 09:53:13.351302 Epoch 31  	Train Loss = 16.62713 Val Loss = 18.05751
2023-06-02 09:53:25.080747 Epoch 32  	Train Loss = 16.56455 Val Loss = 18.06515
2023-06-02 09:53:36.625049 Epoch 33  	Train Loss = 16.57763 Val Loss = 18.03652
2023-06-02 09:53:48.175157 Epoch 34  	Train Loss = 16.55467 Val Loss = 18.05322
2023-06-02 09:53:59.881345 Epoch 35  	Train Loss = 16.60384 Val Loss = 18.05717
2023-06-02 09:54:11.588683 Epoch 36  	Train Loss = 16.58729 Val Loss = 18.05128
2023-06-02 09:54:23.162481 Epoch 37  	Train Loss = 16.55624 Val Loss = 18.04394
2023-06-02 09:54:34.969853 Epoch 38  	Train Loss = 16.56742 Val Loss = 18.07584
2023-06-02 09:54:46.600910 Epoch 39  	Train Loss = 16.51087 Val Loss = 18.05718
2023-06-02 09:54:58.252121 Epoch 40  	Train Loss = 16.48534 Val Loss = 18.05333
2023-06-02 09:55:09.948592 Epoch 41  	Train Loss = 16.48918 Val Loss = 18.06883
2023-06-02 09:55:21.572332 Epoch 42  	Train Loss = 16.50460 Val Loss = 18.02675
2023-06-02 09:55:33.179122 Epoch 43  	Train Loss = 16.49848 Val Loss = 18.03963
2023-06-02 09:55:44.771974 Epoch 44  	Train Loss = 16.48282 Val Loss = 18.04882
2023-06-02 09:55:56.283293 Epoch 45  	Train Loss = 16.50053 Val Loss = 18.03543
2023-06-02 09:56:07.733116 Epoch 46  	Train Loss = 16.44920 Val Loss = 18.06026
2023-06-02 09:56:19.314942 Epoch 47  	Train Loss = 16.49465 Val Loss = 18.06890
2023-06-02 09:56:30.938605 Epoch 48  	Train Loss = 16.44297 Val Loss = 18.04651
2023-06-02 09:56:42.537001 Epoch 49  	Train Loss = 16.45049 Val Loss = 18.07022
2023-06-02 09:56:54.154054 Epoch 50  	Train Loss = 16.46741 Val Loss = 18.07217
2023-06-02 09:57:05.777696 Epoch 51  	Train Loss = 16.44573 Val Loss = 18.05033
2023-06-02 09:57:17.403978 Epoch 52  	Train Loss = 16.44007 Val Loss = 18.05657
Early stopping at epoch: 52
Best at epoch 42:
Train Loss = 16.50460
Train RMSE = 28.04613, MAE = 17.02511, MAPE = 12.20075
Val Loss = 18.02675
Val RMSE = 31.10405, MAE = 18.86115, MAPE = 12.27588
--------- Test ---------
All Steps RMSE = 30.82898, MAE = 18.90893, MAPE = 12.45876
Step 1 RMSE = 27.51410, MAE = 17.13774, MAPE = 11.46577
Step 2 RMSE = 28.52814, MAE = 17.66431, MAPE = 11.81167
Step 3 RMSE = 29.37469, MAE = 18.13674, MAPE = 12.08069
Step 4 RMSE = 30.04094, MAE = 18.49030, MAPE = 12.25511
Step 5 RMSE = 30.54339, MAE = 18.74454, MAPE = 12.32960
Step 6 RMSE = 30.99841, MAE = 18.98046, MAPE = 12.45709
Step 7 RMSE = 31.34327, MAE = 19.16832, MAPE = 12.56810
Step 8 RMSE = 31.62971, MAE = 19.33038, MAPE = 12.68471
Step 9 RMSE = 31.91802, MAE = 19.50877, MAPE = 12.76785
Step 10 RMSE = 32.17511, MAE = 19.68227, MAPE = 12.88347
Step 11 RMSE = 32.43928, MAE = 19.85060, MAPE = 12.98535
Step 12 RMSE = 32.95320, MAE = 20.21253, MAPE = 13.21548
Inference time: 0.75 s
