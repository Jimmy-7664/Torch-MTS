PEMSBAY
Trainset:	x-(36465, 12, 325, 2)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 2)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 2)	y-(10419, 12, 325, 1)

--------- AGCRN ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 325,
        "out_steps": 12,
        "input_dim": 2,
        "output_dim": 1,
        "embed_dim": 10,
        "rnn_units": 64,
        "num_layers": 2,
        "cheb_k": 2,
        "default_graph": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
AGCRN                                    [64, 12, 325, 1]          3,250
├─AVWDCRNN: 1-1                          [64, 12, 325, 64]         --
│    └─ModuleList: 2-1                   --                        --
│    │    └─AGCRNCell: 3-1               [64, 325, 64]             255,360
│    │    └─AGCRNCell: 3-2               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-3               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-4               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-5               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-6               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-7               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-8               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-9               [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-10              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-11              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-12              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-13              [64, 325, 64]             493,440
│    │    └─AGCRNCell: 3-14              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-15              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-16              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-17              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-18              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-19              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-20              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-21              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-22              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-23              [64, 325, 64]             (recursive)
│    │    └─AGCRNCell: 3-24              [64, 325, 64]             (recursive)
├─Conv2d: 1-2                            [64, 12, 325, 1]          780
==========================================================================================
Total params: 752,830
Trainable params: 752,830
Non-trainable params: 0
Total mult-adds (G): 186.92
==========================================================================================
Input size (MB): 2.00
Forward/backward pass size (MB): 65.89
Params size (MB): 3.00
Estimated Total Size (MB): 70.89
==========================================================================================

Loss: MaskedMAELoss

2023-05-06 16:21:46.851757 Epoch 1  	Train Loss = 2.21005 Val Loss = 2.13115
2023-05-06 16:22:44.405820 Epoch 2  	Train Loss = 1.84761 Val Loss = 2.02550
2023-05-06 16:23:41.729398 Epoch 3  	Train Loss = 1.75040 Val Loss = 1.94289
2023-05-06 16:24:39.232988 Epoch 4  	Train Loss = 1.68129 Val Loss = 1.84412
2023-05-06 16:25:36.909747 Epoch 5  	Train Loss = 1.63114 Val Loss = 1.81676
2023-05-06 16:26:34.658701 Epoch 6  	Train Loss = 1.59891 Val Loss = 1.77914
2023-05-06 16:27:32.224714 Epoch 7  	Train Loss = 1.57384 Val Loss = 1.76878
2023-05-06 16:28:30.452951 Epoch 8  	Train Loss = 1.55398 Val Loss = 1.73827
2023-05-06 16:29:27.821686 Epoch 9  	Train Loss = 1.53755 Val Loss = 1.72276
2023-05-06 16:30:25.308489 Epoch 10  	Train Loss = 1.52739 Val Loss = 1.72021
2023-05-06 16:31:23.178672 Epoch 11  	Train Loss = 1.51146 Val Loss = 1.70684
2023-05-06 16:32:21.064743 Epoch 12  	Train Loss = 1.50087 Val Loss = 1.71448
2023-05-06 16:33:18.584202 Epoch 13  	Train Loss = 1.49158 Val Loss = 1.70073
2023-05-06 16:34:16.311272 Epoch 14  	Train Loss = 1.48182 Val Loss = 1.68530
2023-05-06 16:35:13.912399 Epoch 15  	Train Loss = 1.47120 Val Loss = 1.68852
2023-05-06 16:36:11.177789 Epoch 16  	Train Loss = 1.46400 Val Loss = 1.68961
2023-05-06 16:37:08.547889 Epoch 17  	Train Loss = 1.45668 Val Loss = 1.68520
2023-05-06 16:38:05.973185 Epoch 18  	Train Loss = 1.45020 Val Loss = 1.68243
2023-05-06 16:39:03.392310 Epoch 19  	Train Loss = 1.44086 Val Loss = 1.68597
2023-05-06 16:40:00.742120 Epoch 20  	Train Loss = 1.43671 Val Loss = 1.68118
2023-05-06 16:40:57.769095 Epoch 21  	Train Loss = 1.43033 Val Loss = 1.66685
2023-05-06 16:41:54.986467 Epoch 22  	Train Loss = 1.42296 Val Loss = 1.67268
2023-05-06 16:42:51.968909 Epoch 23  	Train Loss = 1.41697 Val Loss = 1.66967
2023-05-06 16:43:48.904858 Epoch 24  	Train Loss = 1.41214 Val Loss = 1.67091
2023-05-06 16:44:46.046428 Epoch 25  	Train Loss = 1.40706 Val Loss = 1.67090
2023-05-06 16:45:43.165011 Epoch 26  	Train Loss = 1.40199 Val Loss = 1.67347
2023-05-06 16:46:40.251084 Epoch 27  	Train Loss = 1.39891 Val Loss = 1.66754
2023-05-06 16:47:37.726013 Epoch 28  	Train Loss = 1.39309 Val Loss = 1.66397
2023-05-06 16:48:35.036029 Epoch 29  	Train Loss = 1.38724 Val Loss = 1.66905
2023-05-06 16:49:32.230816 Epoch 30  	Train Loss = 1.38249 Val Loss = 1.66169
2023-05-06 16:50:29.621006 Epoch 31  	Train Loss = 1.37935 Val Loss = 1.65945
2023-05-06 16:51:26.837213 Epoch 32  	Train Loss = 1.37488 Val Loss = 1.66189
2023-05-06 16:52:24.039850 Epoch 33  	Train Loss = 1.37120 Val Loss = 1.66115
2023-05-06 16:53:21.239389 Epoch 34  	Train Loss = 1.36631 Val Loss = 1.65943
2023-05-06 16:54:18.225343 Epoch 35  	Train Loss = 1.36312 Val Loss = 1.65617
2023-05-06 16:55:15.812369 Epoch 36  	Train Loss = 1.35925 Val Loss = 1.65870
2023-05-06 16:56:12.889297 Epoch 37  	Train Loss = 1.35854 Val Loss = 1.65716
2023-05-06 16:57:09.945544 Epoch 38  	Train Loss = 1.35252 Val Loss = 1.66212
2023-05-06 16:58:07.292782 Epoch 39  	Train Loss = 1.34936 Val Loss = 1.65872
2023-05-06 16:59:04.408856 Epoch 40  	Train Loss = 1.37596 Val Loss = 1.69176
2023-05-06 17:00:01.675868 Epoch 41  	Train Loss = 1.36786 Val Loss = 1.66819
2023-05-06 17:00:58.888185 Epoch 42  	Train Loss = 1.35265 Val Loss = 1.65693
2023-05-06 17:01:58.369006 Epoch 43  	Train Loss = 1.34833 Val Loss = 1.66800
2023-05-06 17:02:55.948308 Epoch 44  	Train Loss = 1.35522 Val Loss = 1.65939
2023-05-06 17:03:53.916711 Epoch 45  	Train Loss = 1.34117 Val Loss = 1.66505
2023-05-06 17:04:51.215546 Epoch 46  	Train Loss = 1.33952 Val Loss = 1.65978
2023-05-06 17:05:48.851782 Epoch 47  	Train Loss = 1.33707 Val Loss = 1.65662
2023-05-06 17:06:46.461878 Epoch 48  	Train Loss = 1.33068 Val Loss = 1.66110
2023-05-06 17:07:43.875170 Epoch 49  	Train Loss = 1.32950 Val Loss = 1.66005
2023-05-06 17:08:40.910242 Epoch 50  	Train Loss = 1.32531 Val Loss = 1.65598
2023-05-06 17:09:39.504866 Epoch 51  	Train Loss = 1.28673 Val Loss = 1.63881
2023-05-06 17:10:36.700755 Epoch 52  	Train Loss = 1.27821 Val Loss = 1.63996
2023-05-06 17:11:33.855499 Epoch 53  	Train Loss = 1.27547 Val Loss = 1.64111
2023-05-06 17:12:31.612019 Epoch 54  	Train Loss = 1.27363 Val Loss = 1.64300
2023-05-06 17:13:28.914141 Epoch 55  	Train Loss = 1.27191 Val Loss = 1.64244
2023-05-06 17:14:26.016338 Epoch 56  	Train Loss = 1.27058 Val Loss = 1.64389
2023-05-06 17:15:23.084266 Epoch 57  	Train Loss = 1.26925 Val Loss = 1.64421
2023-05-06 17:16:20.174510 Epoch 58  	Train Loss = 1.26815 Val Loss = 1.64432
2023-05-06 17:17:17.319439 Epoch 59  	Train Loss = 1.26705 Val Loss = 1.64547
2023-05-06 17:18:14.528765 Epoch 60  	Train Loss = 1.26602 Val Loss = 1.64581
2023-05-06 17:19:12.207051 Epoch 61  	Train Loss = 1.26492 Val Loss = 1.64616
2023-05-06 17:20:09.426343 Epoch 62  	Train Loss = 1.26399 Val Loss = 1.64703
2023-05-06 17:21:06.662833 Epoch 63  	Train Loss = 1.26311 Val Loss = 1.64603
2023-05-06 17:22:04.154410 Epoch 64  	Train Loss = 1.26219 Val Loss = 1.64859
2023-05-06 17:23:01.195706 Epoch 65  	Train Loss = 1.26129 Val Loss = 1.64918
2023-05-06 17:23:58.475473 Epoch 66  	Train Loss = 1.26048 Val Loss = 1.64789
2023-05-06 17:24:55.488291 Epoch 67  	Train Loss = 1.25963 Val Loss = 1.64717
2023-05-06 17:25:52.643508 Epoch 68  	Train Loss = 1.25896 Val Loss = 1.64993
2023-05-06 17:26:49.802805 Epoch 69  	Train Loss = 1.25818 Val Loss = 1.64989
2023-05-06 17:27:47.226204 Epoch 70  	Train Loss = 1.25753 Val Loss = 1.65117
2023-05-06 17:28:44.342571 Epoch 71  	Train Loss = 1.25680 Val Loss = 1.65118
Early stopping at epoch: 71
Best at epoch 51:
Train Loss = 1.28673
Train RMSE = 2.71282, MAE = 1.25378, MAPE = 2.63863
Val Loss = 1.63881
Val RMSE = 3.80015, MAE = 1.63883, MAPE = 3.75646
--------- Test ---------
All Steps RMSE = 3.83361, MAE = 1.67354, MAPE = 3.79282
Step 1 RMSE = 1.78263, MAE = 0.97128, MAPE = 1.99677
Step 2 RMSE = 2.42201, MAE = 1.21675, MAPE = 2.57303
Step 3 RMSE = 2.96336, MAE = 1.40651, MAPE = 3.05819
Step 4 RMSE = 3.39321, MAE = 1.55025, MAPE = 3.44486
Step 5 RMSE = 3.71817, MAE = 1.65861, MAPE = 3.73940
Step 6 RMSE = 3.95744, MAE = 1.74264, MAPE = 3.96848
Step 7 RMSE = 4.13383, MAE = 1.80899, MAPE = 4.15363
Step 8 RMSE = 4.27183, MAE = 1.86290, MAPE = 4.30433
Step 9 RMSE = 4.38406, MAE = 1.90600, MAPE = 4.42166
Step 10 RMSE = 4.48424, MAE = 1.94494, MAPE = 4.52061
Step 11 RMSE = 4.57924, MAE = 1.98417, MAPE = 4.61408
Step 12 RMSE = 4.67954, MAE = 2.02949, MAPE = 4.71887
Inference time: 5.99 s
