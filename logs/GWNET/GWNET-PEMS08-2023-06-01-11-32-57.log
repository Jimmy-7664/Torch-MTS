PEMS08
Trainset:	x-(10700, 12, 170, 2)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 2)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 2)	y-(3566, 12, 170, 1)

--------- GWNET ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        40
    ],
    "early_stop": 15,
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 300,
    "use_cl": false,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 170,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/PEMS08/adj_PEMS08_distance.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    [64, 12, 170, 1]          11,848
├─Conv2d: 1-1                            [64, 32, 170, 13]         96
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-1                       [64, 32, 170, 12]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-2                       [64, 32, 170, 12]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-3                       [64, 256, 170, 12]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-4                          [64, 32, 170, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 170, 12]         --
│    │    └─linear: 3-7                  [64, 32, 170, 12]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-5                  [64, 32, 170, 12]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-6                       [64, 32, 170, 10]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 32, 170, 10]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-8                       [64, 256, 170, 10]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-9                          [64, 32, 170, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 170, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 170, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 170, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 170, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 170, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 170, 10]         --
│    │    └─linear: 3-14                 [64, 32, 170, 10]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-10                 [64, 32, 170, 10]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-11                      [64, 32, 170, 9]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-12                      [64, 32, 170, 9]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 256, 170, 9]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-14                         [64, 32, 170, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 170, 9]          --
│    │    └─linear: 3-21                 [64, 32, 170, 9]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-15                 [64, 32, 170, 9]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-16                      [64, 32, 170, 7]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-17                      [64, 32, 170, 7]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-18                      [64, 256, 170, 7]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-19                         [64, 32, 170, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 170, 7]          --
│    │    └─linear: 3-28                 [64, 32, 170, 7]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-20                 [64, 32, 170, 7]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-21                      [64, 32, 170, 6]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-22                      [64, 32, 170, 6]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-23                      [64, 256, 170, 6]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-24                         [64, 32, 170, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 170, 6]          --
│    │    └─linear: 3-35                 [64, 32, 170, 6]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-25                 [64, 32, 170, 6]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-26                      [64, 32, 170, 4]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-27                      [64, 32, 170, 4]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-28                      [64, 256, 170, 4]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-29                         [64, 32, 170, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 170, 4]          --
│    │    └─linear: 3-42                 [64, 32, 170, 4]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-30                 [64, 32, 170, 4]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-31                      [64, 32, 170, 3]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-32                      [64, 32, 170, 3]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-33                      [64, 256, 170, 3]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-34                         [64, 32, 170, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 170, 3]          --
│    │    └─linear: 3-49                 [64, 32, 170, 3]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-35                 [64, 32, 170, 3]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-36                      [64, 32, 170, 1]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-37                      [64, 32, 170, 1]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-38                      [64, 256, 170, 1]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-39                         [64, 32, 170, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 170, 1]          --
│    │    └─linear: 3-56                 [64, 32, 170, 1]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-40                 [64, 32, 170, 1]          64
├─Conv2d: 1-42                           [64, 512, 170, 1]         131,584
├─Conv2d: 1-43                           [64, 12, 170, 1]          6,156
==========================================================================================
Total params: 308,660
Trainable params: 308,660
Non-trainable params: 0
Total mult-adds (G): 12.72
==========================================================================================
Input size (MB): 1.04
Forward/backward pass size (MB): 1819.83
Params size (MB): 1.19
Estimated Total Size (MB): 1822.06
==========================================================================================

Loss: HuberLoss

2023-06-01 11:33:10.227996 Epoch 1  	Train Loss = 29.03917 Val Loss = 23.55401
2023-06-01 11:33:19.789934 Epoch 2  	Train Loss = 22.29739 Val Loss = 24.34885
2023-06-01 11:33:29.358639 Epoch 3  	Train Loss = 20.40392 Val Loss = 20.12369
2023-06-01 11:33:39.050795 Epoch 4  	Train Loss = 19.38497 Val Loss = 18.80413
2023-06-01 11:33:49.143129 Epoch 5  	Train Loss = 18.70594 Val Loss = 20.65823
2023-06-01 11:33:59.259522 Epoch 6  	Train Loss = 18.31091 Val Loss = 20.70167
2023-06-01 11:34:09.253359 Epoch 7  	Train Loss = 18.08200 Val Loss = 18.01747
2023-06-01 11:34:18.764428 Epoch 8  	Train Loss = 17.77811 Val Loss = 17.17509
2023-06-01 11:34:28.418269 Epoch 9  	Train Loss = 17.56088 Val Loss = 17.22611
2023-06-01 11:34:38.116576 Epoch 10  	Train Loss = 17.46487 Val Loss = 18.10663
2023-06-01 11:34:47.540952 Epoch 11  	Train Loss = 17.25129 Val Loss = 18.17607
2023-06-01 11:34:56.934473 Epoch 12  	Train Loss = 17.08906 Val Loss = 16.91775
2023-06-01 11:35:06.365205 Epoch 13  	Train Loss = 16.97811 Val Loss = 17.24791
2023-06-01 11:35:15.858473 Epoch 14  	Train Loss = 16.92828 Val Loss = 17.29805
2023-06-01 11:35:25.376897 Epoch 15  	Train Loss = 16.68793 Val Loss = 16.54070
2023-06-01 11:35:34.878647 Epoch 16  	Train Loss = 16.63910 Val Loss = 16.98880
2023-06-01 11:35:44.328103 Epoch 17  	Train Loss = 16.51445 Val Loss = 16.46773
2023-06-01 11:35:53.760481 Epoch 18  	Train Loss = 16.48406 Val Loss = 17.88922
2023-06-01 11:36:03.201165 Epoch 19  	Train Loss = 16.28397 Val Loss = 16.09185
2023-06-01 11:36:12.628951 Epoch 20  	Train Loss = 16.19221 Val Loss = 16.62842
2023-06-01 11:36:22.039566 Epoch 21  	Train Loss = 16.17837 Val Loss = 16.57899
2023-06-01 11:36:31.471230 Epoch 22  	Train Loss = 16.15958 Val Loss = 16.13256
2023-06-01 11:36:40.943196 Epoch 23  	Train Loss = 16.05217 Val Loss = 16.38132
2023-06-01 11:36:50.476257 Epoch 24  	Train Loss = 15.98827 Val Loss = 16.06626
2023-06-01 11:36:59.999806 Epoch 25  	Train Loss = 16.09004 Val Loss = 15.99074
2023-06-01 11:37:09.519011 Epoch 26  	Train Loss = 15.79583 Val Loss = 15.61376
2023-06-01 11:37:18.968319 Epoch 27  	Train Loss = 15.84328 Val Loss = 16.08683
2023-06-01 11:37:28.407701 Epoch 28  	Train Loss = 15.76787 Val Loss = 15.44632
2023-06-01 11:37:37.823265 Epoch 29  	Train Loss = 15.66203 Val Loss = 15.60238
2023-06-01 11:37:47.263681 Epoch 30  	Train Loss = 15.78106 Val Loss = 15.83604
2023-06-01 11:37:56.800281 Epoch 31  	Train Loss = 15.60095 Val Loss = 15.40763
2023-06-01 11:38:06.327694 Epoch 32  	Train Loss = 15.50080 Val Loss = 15.99557
2023-06-01 11:38:15.868823 Epoch 33  	Train Loss = 15.37923 Val Loss = 16.27169
2023-06-01 11:38:25.431471 Epoch 34  	Train Loss = 15.48050 Val Loss = 15.84637
2023-06-01 11:38:35.032813 Epoch 35  	Train Loss = 15.40151 Val Loss = 15.27724
2023-06-01 11:38:44.850745 Epoch 36  	Train Loss = 15.28399 Val Loss = 15.44638
2023-06-01 11:38:54.454532 Epoch 37  	Train Loss = 15.28135 Val Loss = 15.35442
2023-06-01 11:39:04.054195 Epoch 38  	Train Loss = 15.22693 Val Loss = 15.32101
2023-06-01 11:39:13.571172 Epoch 39  	Train Loss = 15.25351 Val Loss = 15.51555
2023-06-01 11:39:23.045308 Epoch 40  	Train Loss = 15.12932 Val Loss = 15.37480
2023-06-01 11:39:32.826266 Epoch 41  	Train Loss = 14.75877 Val Loss = 14.80487
2023-06-01 11:39:42.798256 Epoch 42  	Train Loss = 14.73044 Val Loss = 14.78053
2023-06-01 11:39:52.779489 Epoch 43  	Train Loss = 14.69481 Val Loss = 14.76975
2023-06-01 11:40:02.263616 Epoch 44  	Train Loss = 14.68847 Val Loss = 14.72722
2023-06-01 11:40:11.683655 Epoch 45  	Train Loss = 14.65269 Val Loss = 14.75404
2023-06-01 11:40:21.117695 Epoch 46  	Train Loss = 14.67461 Val Loss = 14.78296
2023-06-01 11:40:30.556135 Epoch 47  	Train Loss = 14.66078 Val Loss = 14.73121
2023-06-01 11:40:40.024051 Epoch 48  	Train Loss = 14.65079 Val Loss = 14.76129
2023-06-01 11:40:49.444446 Epoch 49  	Train Loss = 14.65148 Val Loss = 14.75756
2023-06-01 11:40:58.875736 Epoch 50  	Train Loss = 14.63471 Val Loss = 14.70175
2023-06-01 11:41:08.291420 Epoch 51  	Train Loss = 14.61998 Val Loss = 14.73830
2023-06-01 11:41:17.703700 Epoch 52  	Train Loss = 14.62662 Val Loss = 14.69562
2023-06-01 11:41:27.126258 Epoch 53  	Train Loss = 14.61978 Val Loss = 14.74071
2023-06-01 11:41:36.544535 Epoch 54  	Train Loss = 14.62760 Val Loss = 14.72166
2023-06-01 11:41:46.086303 Epoch 55  	Train Loss = 14.59764 Val Loss = 14.70931
2023-06-01 11:41:55.676673 Epoch 56  	Train Loss = 14.59833 Val Loss = 14.77441
2023-06-01 11:42:05.297953 Epoch 57  	Train Loss = 14.60310 Val Loss = 14.81846
2023-06-01 11:42:14.882547 Epoch 58  	Train Loss = 14.58151 Val Loss = 14.73221
2023-06-01 11:42:24.465161 Epoch 59  	Train Loss = 14.58049 Val Loss = 14.72220
2023-06-01 11:42:34.150099 Epoch 60  	Train Loss = 14.57697 Val Loss = 14.71819
2023-06-01 11:42:44.064557 Epoch 61  	Train Loss = 14.56312 Val Loss = 14.79302
2023-06-01 11:42:53.600438 Epoch 62  	Train Loss = 14.57428 Val Loss = 14.69152
2023-06-01 11:43:03.152185 Epoch 63  	Train Loss = 14.55566 Val Loss = 14.70530
2023-06-01 11:43:12.698927 Epoch 64  	Train Loss = 14.55651 Val Loss = 14.68117
2023-06-01 11:43:22.244506 Epoch 65  	Train Loss = 14.53065 Val Loss = 14.68153
2023-06-01 11:43:31.789837 Epoch 66  	Train Loss = 14.53209 Val Loss = 14.68211
2023-06-01 11:43:41.337752 Epoch 67  	Train Loss = 14.52599 Val Loss = 14.75892
2023-06-01 11:43:50.894864 Epoch 68  	Train Loss = 14.52256 Val Loss = 14.67970
2023-06-01 11:44:00.747797 Epoch 69  	Train Loss = 14.50780 Val Loss = 14.68908
2023-06-01 11:44:10.567169 Epoch 70  	Train Loss = 14.53004 Val Loss = 14.66069
2023-06-01 11:44:20.493377 Epoch 71  	Train Loss = 14.49015 Val Loss = 14.64099
2023-06-01 11:44:30.100421 Epoch 72  	Train Loss = 14.47507 Val Loss = 14.65850
2023-06-01 11:44:39.658455 Epoch 73  	Train Loss = 14.51490 Val Loss = 14.64274
2023-06-01 11:44:49.224140 Epoch 74  	Train Loss = 14.47878 Val Loss = 14.65237
2023-06-01 11:44:58.752814 Epoch 75  	Train Loss = 14.45367 Val Loss = 14.64706
2023-06-01 11:45:08.304153 Epoch 76  	Train Loss = 14.47085 Val Loss = 14.63681
2023-06-01 11:45:17.865913 Epoch 77  	Train Loss = 14.45712 Val Loss = 14.62316
2023-06-01 11:45:27.414713 Epoch 78  	Train Loss = 14.45036 Val Loss = 14.65108
2023-06-01 11:45:36.957001 Epoch 79  	Train Loss = 14.47219 Val Loss = 14.73094
2023-06-01 11:45:46.506633 Epoch 80  	Train Loss = 14.44177 Val Loss = 14.65916
2023-06-01 11:45:56.067084 Epoch 81  	Train Loss = 14.43554 Val Loss = 14.61506
2023-06-01 11:46:05.672360 Epoch 82  	Train Loss = 14.43299 Val Loss = 14.63435
2023-06-01 11:46:15.464281 Epoch 83  	Train Loss = 14.41616 Val Loss = 14.65249
2023-06-01 11:46:25.286486 Epoch 84  	Train Loss = 14.42727 Val Loss = 14.62459
2023-06-01 11:46:34.807093 Epoch 85  	Train Loss = 14.42274 Val Loss = 14.63323
2023-06-01 11:46:44.315123 Epoch 86  	Train Loss = 14.43731 Val Loss = 14.64739
2023-06-01 11:46:53.845672 Epoch 87  	Train Loss = 14.40308 Val Loss = 14.61628
2023-06-01 11:47:03.365061 Epoch 88  	Train Loss = 14.38773 Val Loss = 14.61488
2023-06-01 11:47:12.907411 Epoch 89  	Train Loss = 14.39380 Val Loss = 14.62035
2023-06-01 11:47:22.505453 Epoch 90  	Train Loss = 14.39051 Val Loss = 14.62756
2023-06-01 11:47:32.592854 Epoch 91  	Train Loss = 14.38797 Val Loss = 14.59725
2023-06-01 11:47:42.497183 Epoch 92  	Train Loss = 14.37985 Val Loss = 14.66593
2023-06-01 11:47:52.259802 Epoch 93  	Train Loss = 14.35939 Val Loss = 14.66908
2023-06-01 11:48:01.841699 Epoch 94  	Train Loss = 14.37428 Val Loss = 14.61742
2023-06-01 11:48:11.398521 Epoch 95  	Train Loss = 14.36680 Val Loss = 14.58051
2023-06-01 11:48:21.002357 Epoch 96  	Train Loss = 14.36301 Val Loss = 14.61876
2023-06-01 11:48:30.544776 Epoch 97  	Train Loss = 14.35458 Val Loss = 14.56700
2023-06-01 11:48:40.141674 Epoch 98  	Train Loss = 14.34688 Val Loss = 14.59103
2023-06-01 11:48:49.596061 Epoch 99  	Train Loss = 14.35055 Val Loss = 14.57462
2023-06-01 11:48:59.019670 Epoch 100  	Train Loss = 14.35108 Val Loss = 14.62232
2023-06-01 11:49:08.446798 Epoch 101  	Train Loss = 14.34447 Val Loss = 14.57342
2023-06-01 11:49:17.862467 Epoch 102  	Train Loss = 14.33510 Val Loss = 14.65205
2023-06-01 11:49:27.267230 Epoch 103  	Train Loss = 14.31018 Val Loss = 14.58986
2023-06-01 11:49:36.672259 Epoch 104  	Train Loss = 14.30852 Val Loss = 14.54697
2023-06-01 11:49:46.100119 Epoch 105  	Train Loss = 14.31659 Val Loss = 14.59848
2023-06-01 11:49:55.509749 Epoch 106  	Train Loss = 14.32881 Val Loss = 14.54143
2023-06-01 11:50:04.920982 Epoch 107  	Train Loss = 14.31284 Val Loss = 14.58894
2023-06-01 11:50:14.358893 Epoch 108  	Train Loss = 14.29942 Val Loss = 14.69502
2023-06-01 11:50:23.802534 Epoch 109  	Train Loss = 14.29683 Val Loss = 14.63061
2023-06-01 11:50:33.236197 Epoch 110  	Train Loss = 14.28111 Val Loss = 14.56118
2023-06-01 11:50:42.649507 Epoch 111  	Train Loss = 14.28255 Val Loss = 14.57679
2023-06-01 11:50:52.148613 Epoch 112  	Train Loss = 14.28555 Val Loss = 14.61382
2023-06-01 11:51:01.609822 Epoch 113  	Train Loss = 14.28172 Val Loss = 14.61361
2023-06-01 11:51:11.046688 Epoch 114  	Train Loss = 14.28718 Val Loss = 14.55557
2023-06-01 11:51:20.487183 Epoch 115  	Train Loss = 14.26033 Val Loss = 14.56429
2023-06-01 11:51:29.920491 Epoch 116  	Train Loss = 14.26835 Val Loss = 14.51696
2023-06-01 11:51:39.383704 Epoch 117  	Train Loss = 14.25712 Val Loss = 14.61484
2023-06-01 11:51:48.804597 Epoch 118  	Train Loss = 14.26284 Val Loss = 14.56717
2023-06-01 11:51:58.224229 Epoch 119  	Train Loss = 14.25014 Val Loss = 14.54672
2023-06-01 11:52:07.652632 Epoch 120  	Train Loss = 14.24413 Val Loss = 14.60468
2023-06-01 11:52:17.179019 Epoch 121  	Train Loss = 14.25165 Val Loss = 14.57300
2023-06-01 11:52:26.932475 Epoch 122  	Train Loss = 14.24027 Val Loss = 14.61812
2023-06-01 11:52:36.877083 Epoch 123  	Train Loss = 14.24461 Val Loss = 14.55514
2023-06-01 11:52:46.429284 Epoch 124  	Train Loss = 14.22360 Val Loss = 14.55150
2023-06-01 11:52:55.934294 Epoch 125  	Train Loss = 14.22722 Val Loss = 14.59346
2023-06-01 11:53:05.399353 Epoch 126  	Train Loss = 14.24426 Val Loss = 14.56498
2023-06-01 11:53:14.876187 Epoch 127  	Train Loss = 14.21745 Val Loss = 14.60049
2023-06-01 11:53:24.386875 Epoch 128  	Train Loss = 14.20867 Val Loss = 14.54537
2023-06-01 11:53:33.866573 Epoch 129  	Train Loss = 14.22422 Val Loss = 14.62084
2023-06-01 11:53:43.565558 Epoch 130  	Train Loss = 14.20592 Val Loss = 14.55022
2023-06-01 11:53:53.558631 Epoch 131  	Train Loss = 14.18310 Val Loss = 14.51221
2023-06-01 11:54:03.051319 Epoch 132  	Train Loss = 14.18355 Val Loss = 14.57394
2023-06-01 11:54:12.766286 Epoch 133  	Train Loss = 14.19100 Val Loss = 14.50827
2023-06-01 11:54:22.214904 Epoch 134  	Train Loss = 14.18759 Val Loss = 14.49689
2023-06-01 11:54:31.713454 Epoch 135  	Train Loss = 14.20524 Val Loss = 14.54163
2023-06-01 11:54:41.242066 Epoch 136  	Train Loss = 14.16213 Val Loss = 14.59515
2023-06-01 11:54:50.772955 Epoch 137  	Train Loss = 14.18989 Val Loss = 14.60707
2023-06-01 11:55:00.195776 Epoch 138  	Train Loss = 14.18603 Val Loss = 14.53040
2023-06-01 11:55:09.620183 Epoch 139  	Train Loss = 14.16824 Val Loss = 14.49887
2023-06-01 11:55:19.033615 Epoch 140  	Train Loss = 14.15695 Val Loss = 14.50903
2023-06-01 11:55:28.431776 Epoch 141  	Train Loss = 14.16831 Val Loss = 14.55340
2023-06-01 11:55:37.839032 Epoch 142  	Train Loss = 14.17052 Val Loss = 14.51762
2023-06-01 11:55:47.319156 Epoch 143  	Train Loss = 14.17366 Val Loss = 14.55278
2023-06-01 11:55:56.975616 Epoch 144  	Train Loss = 14.15741 Val Loss = 14.56410
2023-06-01 11:56:06.818495 Epoch 145  	Train Loss = 14.16143 Val Loss = 14.52473
2023-06-01 11:56:16.343387 Epoch 146  	Train Loss = 14.16781 Val Loss = 14.52284
2023-06-01 11:56:25.735802 Epoch 147  	Train Loss = 14.14609 Val Loss = 14.59585
2023-06-01 11:56:35.163418 Epoch 148  	Train Loss = 14.13575 Val Loss = 14.49383
2023-06-01 11:56:44.621464 Epoch 149  	Train Loss = 14.11578 Val Loss = 14.45813
2023-06-01 11:56:54.086863 Epoch 150  	Train Loss = 14.12327 Val Loss = 14.55398
2023-06-01 11:57:03.506378 Epoch 151  	Train Loss = 14.12041 Val Loss = 14.46477
2023-06-01 11:57:12.916256 Epoch 152  	Train Loss = 14.11233 Val Loss = 14.48188
2023-06-01 11:57:22.327991 Epoch 153  	Train Loss = 14.12076 Val Loss = 14.46940
2023-06-01 11:57:31.720823 Epoch 154  	Train Loss = 14.11260 Val Loss = 14.48206
2023-06-01 11:57:41.123247 Epoch 155  	Train Loss = 14.12282 Val Loss = 14.44140
2023-06-01 11:57:50.577808 Epoch 156  	Train Loss = 14.11565 Val Loss = 14.44608
2023-06-01 11:58:00.114239 Epoch 157  	Train Loss = 14.08872 Val Loss = 14.49349
2023-06-01 11:58:09.948414 Epoch 158  	Train Loss = 14.09011 Val Loss = 14.50576
2023-06-01 11:58:19.675080 Epoch 159  	Train Loss = 14.10189 Val Loss = 14.46267
2023-06-01 11:58:29.115471 Epoch 160  	Train Loss = 14.08500 Val Loss = 14.57297
2023-06-01 11:58:38.519555 Epoch 161  	Train Loss = 14.11022 Val Loss = 14.47874
2023-06-01 11:58:48.036270 Epoch 162  	Train Loss = 14.09245 Val Loss = 14.48884
2023-06-01 11:58:57.691315 Epoch 163  	Train Loss = 14.07561 Val Loss = 14.52362
2023-06-01 11:59:07.597365 Epoch 164  	Train Loss = 14.08469 Val Loss = 14.49998
2023-06-01 11:59:17.142540 Epoch 165  	Train Loss = 14.09231 Val Loss = 14.53835
2023-06-01 11:59:26.556815 Epoch 166  	Train Loss = 14.06067 Val Loss = 14.46590
2023-06-01 11:59:35.962981 Epoch 167  	Train Loss = 14.07220 Val Loss = 14.49135
2023-06-01 11:59:45.418667 Epoch 168  	Train Loss = 14.05962 Val Loss = 14.47697
2023-06-01 11:59:54.964855 Epoch 169  	Train Loss = 14.07939 Val Loss = 14.49556
2023-06-01 12:00:04.566599 Epoch 170  	Train Loss = 14.05699 Val Loss = 14.50153
Early stopping at epoch: 170
Best at epoch 155:
Train Loss = 14.12282
Train RMSE = 23.55824, MAE = 14.15657, MAPE = 9.31002
Val Loss = 14.44140
Val RMSE = 24.58554, MAE = 14.95225, MAPE = 10.51171
--------- Test ---------
All Steps RMSE = 23.51232, MAE = 14.66947, MAPE = 9.59139
Step 1 RMSE = 19.67918, MAE = 12.61949, MAPE = 8.23024
Step 2 RMSE = 20.90742, MAE = 13.25344, MAPE = 8.58076
Step 3 RMSE = 21.79638, MAE = 13.71531, MAPE = 8.82079
Step 4 RMSE = 22.51005, MAE = 14.08273, MAPE = 9.37700
Step 5 RMSE = 23.06961, MAE = 14.39561, MAPE = 9.59303
Step 6 RMSE = 23.54481, MAE = 14.66006, MAPE = 9.58834
Step 7 RMSE = 24.01819, MAE = 14.97013, MAPE = 9.94637
Step 8 RMSE = 24.45877, MAE = 15.24690, MAPE = 10.00763
Step 9 RMSE = 24.82871, MAE = 15.45967, MAPE = 10.03706
Step 10 RMSE = 25.10891, MAE = 15.65651, MAPE = 10.32259
Step 11 RMSE = 25.43368, MAE = 15.82479, MAPE = 10.21372
Step 12 RMSE = 25.92905, MAE = 16.14908, MAPE = 10.37923
Inference time: 0.89 s
