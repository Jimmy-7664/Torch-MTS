METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 1)

Random seed = 233
--------- GWNET ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        40
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/METRLA/adj_mx.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    [64, 12, 207, 1]          12,588
├─Conv2d: 1-1                            [64, 32, 207, 13]         96
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-1                       [64, 32, 207, 12]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-2                       [64, 32, 207, 12]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-3                       [64, 256, 207, 12]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-4                          [64, 32, 207, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 207, 12]         --
│    │    └─linear: 3-7                  [64, 32, 207, 12]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-5                  [64, 32, 207, 12]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-6                       [64, 32, 207, 10]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 32, 207, 10]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-8                       [64, 256, 207, 10]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-9                          [64, 32, 207, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 207, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 207, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 207, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 207, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 207, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 207, 10]         --
│    │    └─linear: 3-14                 [64, 32, 207, 10]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-10                 [64, 32, 207, 10]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-11                      [64, 32, 207, 9]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-12                      [64, 32, 207, 9]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 256, 207, 9]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-14                         [64, 32, 207, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 207, 9]          --
│    │    └─linear: 3-21                 [64, 32, 207, 9]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-15                 [64, 32, 207, 9]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-16                      [64, 32, 207, 7]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-17                      [64, 32, 207, 7]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-18                      [64, 256, 207, 7]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-19                         [64, 32, 207, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 207, 7]          --
│    │    └─linear: 3-28                 [64, 32, 207, 7]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-20                 [64, 32, 207, 7]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-21                      [64, 32, 207, 6]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-22                      [64, 32, 207, 6]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-23                      [64, 256, 207, 6]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-24                         [64, 32, 207, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 207, 6]          --
│    │    └─linear: 3-35                 [64, 32, 207, 6]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-25                 [64, 32, 207, 6]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-26                      [64, 32, 207, 4]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-27                      [64, 32, 207, 4]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-28                      [64, 256, 207, 4]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-29                         [64, 32, 207, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 207, 4]          --
│    │    └─linear: 3-42                 [64, 32, 207, 4]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-30                 [64, 32, 207, 4]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-31                      [64, 32, 207, 3]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-32                      [64, 32, 207, 3]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-33                      [64, 256, 207, 3]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-34                         [64, 32, 207, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 207, 3]          --
│    │    └─linear: 3-49                 [64, 32, 207, 3]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-35                 [64, 32, 207, 3]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-36                      [64, 32, 207, 1]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-37                      [64, 32, 207, 1]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-38                      [64, 256, 207, 1]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-39                         [64, 32, 207, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 207, 1]          --
│    │    └─linear: 3-56                 [64, 32, 207, 1]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-40                 [64, 32, 207, 1]          64
├─Conv2d: 1-42                           [64, 512, 207, 1]         131,584
├─Conv2d: 1-43                           [64, 12, 207, 1]          6,156
==========================================================================================
Total params: 309,400
Trainable params: 309,400
Non-trainable params: 0
Total mult-adds (G): 15.49
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 2215.91
Params size (MB): 1.19
Estimated Total Size (MB): 2218.37
==========================================================================================

Loss: MaskedMAELoss

2024-04-21 18:28:21.856601 Epoch 1  	Train Loss = 4.01635 Val Loss = 3.36801
2024-04-21 18:28:47.603458 Epoch 2  	Train Loss = 3.55496 Val Loss = 3.28064
2024-04-21 18:29:13.279017 Epoch 3  	Train Loss = 3.44226 Val Loss = 3.13365
2024-04-21 18:29:38.921755 Epoch 4  	Train Loss = 3.36900 Val Loss = 3.11752
2024-04-21 18:30:04.636316 Epoch 5  	Train Loss = 3.30456 Val Loss = 3.08208
2024-04-21 18:30:30.442025 Epoch 6  	Train Loss = 3.25438 Val Loss = 3.03351
2024-04-21 18:30:56.081630 Epoch 7  	Train Loss = 3.21053 Val Loss = 2.98242
2024-04-21 18:31:21.766742 Epoch 8  	Train Loss = 3.17617 Val Loss = 2.96823
2024-04-21 18:31:47.488176 Epoch 9  	Train Loss = 3.14143 Val Loss = 2.93235
2024-04-21 18:32:13.176738 Epoch 10  	Train Loss = 3.11354 Val Loss = 2.92868
2024-04-21 18:32:38.839111 Epoch 11  	Train Loss = 3.09826 Val Loss = 2.95876
2024-04-21 18:33:04.604987 Epoch 12  	Train Loss = 3.07338 Val Loss = 2.90492
2024-04-21 18:33:30.468245 Epoch 13  	Train Loss = 3.05560 Val Loss = 2.91202
2024-04-21 18:33:56.162478 Epoch 14  	Train Loss = 3.03766 Val Loss = 2.87761
2024-04-21 18:34:21.870729 Epoch 15  	Train Loss = 3.02562 Val Loss = 2.85438
2024-04-21 18:34:47.922939 Epoch 16  	Train Loss = 3.00639 Val Loss = 2.87846
2024-04-21 18:35:13.582512 Epoch 17  	Train Loss = 2.99918 Val Loss = 2.84915
2024-04-21 18:35:39.304680 Epoch 18  	Train Loss = 2.99099 Val Loss = 2.87403
2024-04-21 18:36:05.091696 Epoch 19  	Train Loss = 2.97648 Val Loss = 2.82103
2024-04-21 18:36:30.793262 Epoch 20  	Train Loss = 2.96950 Val Loss = 2.82500
2024-04-21 18:36:56.712304 Epoch 21  	Train Loss = 2.95920 Val Loss = 2.81684
2024-04-21 18:37:22.563729 Epoch 22  	Train Loss = 2.94918 Val Loss = 2.84034
2024-04-21 18:37:48.359826 Epoch 23  	Train Loss = 2.94791 Val Loss = 2.82931
2024-04-21 18:38:14.088340 Epoch 24  	Train Loss = 2.93045 Val Loss = 2.80289
2024-04-21 18:38:39.932024 Epoch 25  	Train Loss = 2.92716 Val Loss = 2.84377
2024-04-21 18:39:05.606950 Epoch 26  	Train Loss = 2.92047 Val Loss = 2.80218
2024-04-21 18:39:31.380936 Epoch 27  	Train Loss = 2.91661 Val Loss = 2.80919
2024-04-21 18:39:57.189853 Epoch 28  	Train Loss = 2.90548 Val Loss = 2.79061
2024-04-21 18:40:22.992743 Epoch 29  	Train Loss = 2.90690 Val Loss = 2.78954
2024-04-21 18:40:48.800351 Epoch 30  	Train Loss = 2.89832 Val Loss = 2.77290
2024-04-21 18:41:14.615839 Epoch 31  	Train Loss = 2.89199 Val Loss = 2.77620
2024-04-21 18:41:40.346281 Epoch 32  	Train Loss = 2.88532 Val Loss = 2.77152
2024-04-21 18:42:06.047124 Epoch 33  	Train Loss = 2.89239 Val Loss = 2.80118
2024-04-21 18:42:31.878690 Epoch 34  	Train Loss = 2.87932 Val Loss = 2.78925
2024-04-21 18:42:57.776058 Epoch 35  	Train Loss = 2.87490 Val Loss = 2.78419
2024-04-21 18:43:23.707655 Epoch 36  	Train Loss = 2.87183 Val Loss = 2.78616
2024-04-21 18:43:49.657531 Epoch 37  	Train Loss = 2.86653 Val Loss = 2.80423
2024-04-21 18:44:15.494852 Epoch 38  	Train Loss = 2.86693 Val Loss = 2.84320
2024-04-21 18:44:41.444150 Epoch 39  	Train Loss = 2.86000 Val Loss = 2.82195
2024-04-21 18:45:07.398946 Epoch 40  	Train Loss = 2.85694 Val Loss = 2.78144
2024-04-21 18:45:33.308204 Epoch 41  	Train Loss = 2.80361 Val Loss = 2.74581
2024-04-21 18:45:59.230312 Epoch 42  	Train Loss = 2.79613 Val Loss = 2.73801
2024-04-21 18:46:25.023193 Epoch 43  	Train Loss = 2.79403 Val Loss = 2.74345
2024-04-21 18:46:50.952806 Epoch 44  	Train Loss = 2.79415 Val Loss = 2.74104
2024-04-21 18:47:16.821793 Epoch 45  	Train Loss = 2.79152 Val Loss = 2.74697
2024-04-21 18:47:42.823718 Epoch 46  	Train Loss = 2.78871 Val Loss = 2.74829
2024-04-21 18:48:08.614253 Epoch 47  	Train Loss = 2.79093 Val Loss = 2.75530
2024-04-21 18:48:34.340709 Epoch 48  	Train Loss = 2.78747 Val Loss = 2.74229
2024-04-21 18:49:00.052195 Epoch 49  	Train Loss = 2.78702 Val Loss = 2.74395
2024-04-21 18:49:25.689720 Epoch 50  	Train Loss = 2.78548 Val Loss = 2.74345
2024-04-21 18:49:51.356132 Epoch 51  	Train Loss = 2.78461 Val Loss = 2.75188
2024-04-21 18:50:17.162518 Epoch 52  	Train Loss = 2.78411 Val Loss = 2.75052
Early stopping at epoch: 52
Best at epoch 42:
Train Loss = 2.79613
Train MAE = 2.74045, RMSE = 5.50249, MAPE = 7.22976
Val Loss = 2.73801
Val MAE = 2.77044, RMSE = 5.76334, MAPE = 7.73993
Model checkpoint saved to: ../saved_models/GWNET/GWNET-METRLA-2024-04-21-18-27-53.pt
--------- Test ---------
All Steps (1-12) MAE = 3.02144, RMSE = 6.10053, MAPE = 8.19817
Step 1 MAE = 2.22008, RMSE = 3.83157, MAPE = 5.30138
Step 2 MAE = 2.49622, RMSE = 4.61397, MAPE = 6.21353
Step 3 MAE = 2.68401, RMSE = 5.13239, MAPE = 6.89807
Step 4 MAE = 2.83196, RMSE = 5.54180, MAPE = 7.46220
Step 5 MAE = 2.95438, RMSE = 5.87144, MAPE = 7.93439
Step 6 MAE = 3.06026, RMSE = 6.15223, MAPE = 8.34179
Step 7 MAE = 3.15468, RMSE = 6.39588, MAPE = 8.69741
Step 8 MAE = 3.23745, RMSE = 6.60123, MAPE = 9.00785
Step 9 MAE = 3.30992, RMSE = 6.78484, MAPE = 9.27813
Step 10 MAE = 3.37506, RMSE = 6.94518, MAPE = 9.52134
Step 11 MAE = 3.43591, RMSE = 7.08921, MAPE = 9.74679
Step 12 MAE = 3.49745, RMSE = 7.22792, MAPE = 9.97522
Inference time: 2.22 s
