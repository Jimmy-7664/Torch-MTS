PEMS07
Trainset:	x-(16921, 12, 883, 2)	y-(16921, 12, 883, 1)
Valset:  	x-(5640, 12, 883, 2)  	y-(5640, 12, 883, 1)
Testset:	x-(5640, 12, 883, 2)	y-(5640, 12, 883, 1)

--------- GWNET ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        40
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 883,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/PEMS07/adj_PEMS07_distance.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    [64, 12, 883, 1]          26,108
├─Conv2d: 1-1                            [64, 32, 883, 13]         96
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-1                       [64, 32, 883, 12]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-2                       [64, 32, 883, 12]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-3                       [64, 256, 883, 12]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-4                          [64, 32, 883, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 883, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 883, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 883, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 883, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 883, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 883, 12]         --
│    │    └─linear: 3-7                  [64, 32, 883, 12]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-5                  [64, 32, 883, 12]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-6                       [64, 32, 883, 10]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 32, 883, 10]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-8                       [64, 256, 883, 10]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-9                          [64, 32, 883, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 883, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 883, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 883, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 883, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 883, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 883, 10]         --
│    │    └─linear: 3-14                 [64, 32, 883, 10]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-10                 [64, 32, 883, 10]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-11                      [64, 32, 883, 9]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-12                      [64, 32, 883, 9]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 256, 883, 9]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-14                         [64, 32, 883, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 883, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 883, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 883, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 883, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 883, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 883, 9]          --
│    │    └─linear: 3-21                 [64, 32, 883, 9]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-15                 [64, 32, 883, 9]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-16                      [64, 32, 883, 7]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-17                      [64, 32, 883, 7]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-18                      [64, 256, 883, 7]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-19                         [64, 32, 883, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 883, 7]          --
│    │    └─linear: 3-28                 [64, 32, 883, 7]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-20                 [64, 32, 883, 7]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-21                      [64, 32, 883, 6]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-22                      [64, 32, 883, 6]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-23                      [64, 256, 883, 6]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-24                         [64, 32, 883, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 883, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 883, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 883, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 883, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 883, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 883, 6]          --
│    │    └─linear: 3-35                 [64, 32, 883, 6]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-25                 [64, 32, 883, 6]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-26                      [64, 32, 883, 4]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-27                      [64, 32, 883, 4]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-28                      [64, 256, 883, 4]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-29                         [64, 32, 883, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 883, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 883, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 883, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 883, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 883, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 883, 4]          --
│    │    └─linear: 3-42                 [64, 32, 883, 4]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-30                 [64, 32, 883, 4]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-31                      [64, 32, 883, 3]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-32                      [64, 32, 883, 3]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-33                      [64, 256, 883, 3]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-34                         [64, 32, 883, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 883, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 883, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 883, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 883, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 883, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 883, 3]          --
│    │    └─linear: 3-49                 [64, 32, 883, 3]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-35                 [64, 32, 883, 3]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-36                      [64, 32, 883, 1]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-37                      [64, 32, 883, 1]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-38                      [64, 256, 883, 1]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-39                         [64, 32, 883, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 883, 1]          --
│    │    └─linear: 3-56                 [64, 32, 883, 1]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-40                 [64, 32, 883, 1]          64
├─Conv2d: 1-42                           [64, 512, 883, 1]         131,584
├─Conv2d: 1-43                           [64, 12, 883, 1]          6,156
==========================================================================================
Total params: 322,920
Trainable params: 322,920
Non-trainable params: 0
Total mult-adds (G): 66.06
==========================================================================================
Input size (MB): 5.43
Forward/backward pass size (MB): 9452.42
Params size (MB): 1.19
Estimated Total Size (MB): 9459.04
==========================================================================================

Loss: HuberLoss

2023-06-01 11:34:38.522282 Epoch 1  	Train Loss = 37.32368 Val Loss = 34.55663
2023-06-01 11:36:16.017904 Epoch 2  	Train Loss = 29.34563 Val Loss = 27.36942
2023-06-01 11:37:53.653426 Epoch 3  	Train Loss = 26.97344 Val Loss = 26.42111
2023-06-01 11:39:31.300340 Epoch 4  	Train Loss = 26.12807 Val Loss = 26.61163
2023-06-01 11:41:08.803935 Epoch 5  	Train Loss = 25.35304 Val Loss = 25.27984
2023-06-01 11:42:46.230109 Epoch 6  	Train Loss = 24.79006 Val Loss = 23.58005
2023-06-01 11:44:23.577487 Epoch 7  	Train Loss = 24.37081 Val Loss = 23.04874
2023-06-01 11:46:00.944163 Epoch 8  	Train Loss = 24.10671 Val Loss = 22.98370
2023-06-01 11:47:38.320959 Epoch 9  	Train Loss = 23.62952 Val Loss = 23.16856
2023-06-01 11:49:15.773000 Epoch 10  	Train Loss = 23.36760 Val Loss = 23.06275
2023-06-01 11:50:53.304535 Epoch 11  	Train Loss = 23.16770 Val Loss = 22.70618
2023-06-01 11:52:30.984227 Epoch 12  	Train Loss = 23.02742 Val Loss = 23.15803
2023-06-01 11:54:08.637708 Epoch 13  	Train Loss = 22.78664 Val Loss = 23.00665
2023-06-01 11:55:46.190195 Epoch 14  	Train Loss = 22.64357 Val Loss = 22.53993
2023-06-01 11:57:23.629817 Epoch 15  	Train Loss = 22.42723 Val Loss = 22.12807
2023-06-01 11:59:01.027663 Epoch 16  	Train Loss = 22.31809 Val Loss = 23.33019
2023-06-01 12:00:38.449709 Epoch 17  	Train Loss = 22.30318 Val Loss = 22.25727
2023-06-01 12:02:15.815226 Epoch 18  	Train Loss = 22.21575 Val Loss = 22.57843
2023-06-01 12:03:53.089865 Epoch 19  	Train Loss = 21.95215 Val Loss = 21.94415
2023-06-01 12:05:30.331805 Epoch 20  	Train Loss = 22.00780 Val Loss = 21.85504
2023-06-01 12:07:07.521956 Epoch 21  	Train Loss = 21.79589 Val Loss = 21.59099
2023-06-01 12:08:44.728017 Epoch 22  	Train Loss = 21.77310 Val Loss = 22.28912
2023-06-01 12:10:22.005890 Epoch 23  	Train Loss = 21.62092 Val Loss = 21.51424
2023-06-01 12:11:59.202146 Epoch 24  	Train Loss = 21.60561 Val Loss = 22.34627
2023-06-01 12:13:36.472377 Epoch 25  	Train Loss = 21.47272 Val Loss = 21.33432
2023-06-01 12:15:13.738582 Epoch 26  	Train Loss = 21.40985 Val Loss = 21.94694
2023-06-01 12:16:50.960033 Epoch 27  	Train Loss = 21.37890 Val Loss = 22.41027
2023-06-01 12:18:28.215458 Epoch 28  	Train Loss = 21.34366 Val Loss = 21.45134
2023-06-01 12:20:05.478575 Epoch 29  	Train Loss = 21.23148 Val Loss = 21.73778
2023-06-01 12:21:42.673278 Epoch 30  	Train Loss = 21.16133 Val Loss = 21.17414
2023-06-01 12:23:19.877925 Epoch 31  	Train Loss = 21.07782 Val Loss = 21.09954
2023-06-01 12:24:57.094366 Epoch 32  	Train Loss = 21.01046 Val Loss = 20.97736
2023-06-01 12:26:34.439506 Epoch 33  	Train Loss = 20.96219 Val Loss = 20.98582
2023-06-01 12:28:11.579453 Epoch 34  	Train Loss = 20.96857 Val Loss = 20.91286
2023-06-01 12:29:48.694851 Epoch 35  	Train Loss = 21.02989 Val Loss = 20.91197
2023-06-01 12:31:25.860714 Epoch 36  	Train Loss = 20.80140 Val Loss = 20.83938
2023-06-01 12:33:03.016087 Epoch 37  	Train Loss = 20.71832 Val Loss = 20.77368
2023-06-01 12:34:40.108891 Epoch 38  	Train Loss = 20.73624 Val Loss = 21.18782
2023-06-01 12:36:17.212498 Epoch 39  	Train Loss = 20.76319 Val Loss = 21.82677
2023-06-01 12:37:54.352918 Epoch 40  	Train Loss = 20.66924 Val Loss = 20.56928
2023-06-01 12:39:31.505329 Epoch 41  	Train Loss = 20.16472 Val Loss = 20.27714
2023-06-01 12:41:08.614297 Epoch 42  	Train Loss = 20.12207 Val Loss = 20.23547
2023-06-01 12:42:45.670198 Epoch 43  	Train Loss = 20.11411 Val Loss = 20.22163
2023-06-01 12:44:22.773297 Epoch 44  	Train Loss = 20.10518 Val Loss = 20.21737
2023-06-01 12:45:59.882244 Epoch 45  	Train Loss = 20.09540 Val Loss = 20.19297
2023-06-01 12:47:37.026905 Epoch 46  	Train Loss = 20.08299 Val Loss = 20.25019
2023-06-01 12:49:14.103853 Epoch 47  	Train Loss = 20.07994 Val Loss = 20.19714
2023-06-01 12:50:51.194874 Epoch 48  	Train Loss = 20.05569 Val Loss = 20.24306
2023-06-01 12:52:28.367993 Epoch 49  	Train Loss = 20.06096 Val Loss = 20.24477
2023-06-01 12:54:05.492980 Epoch 50  	Train Loss = 20.05697 Val Loss = 20.18669
2023-06-01 12:55:42.622227 Epoch 51  	Train Loss = 20.04733 Val Loss = 20.24144
2023-06-01 12:57:19.772245 Epoch 52  	Train Loss = 20.03048 Val Loss = 20.16261
2023-06-01 12:58:56.920224 Epoch 53  	Train Loss = 20.03447 Val Loss = 20.16713
2023-06-01 13:00:34.103146 Epoch 54  	Train Loss = 20.01045 Val Loss = 20.15760
2023-06-01 13:02:11.311605 Epoch 55  	Train Loss = 20.01565 Val Loss = 20.18637
2023-06-01 13:03:48.434628 Epoch 56  	Train Loss = 19.99341 Val Loss = 20.20000
2023-06-01 13:05:25.578727 Epoch 57  	Train Loss = 19.99806 Val Loss = 20.20109
2023-06-01 13:07:02.679919 Epoch 58  	Train Loss = 19.98167 Val Loss = 20.14257
2023-06-01 13:08:39.791472 Epoch 59  	Train Loss = 19.98616 Val Loss = 20.11911
2023-06-01 13:10:16.930921 Epoch 60  	Train Loss = 19.96893 Val Loss = 20.12231
2023-06-01 13:11:54.079368 Epoch 61  	Train Loss = 19.97221 Val Loss = 20.12133
2023-06-01 13:13:31.247052 Epoch 62  	Train Loss = 19.95320 Val Loss = 20.14446
2023-06-01 13:15:08.391876 Epoch 63  	Train Loss = 19.95541 Val Loss = 20.06580
2023-06-01 13:16:45.506964 Epoch 64  	Train Loss = 19.95064 Val Loss = 20.14587
2023-06-01 13:18:22.557422 Epoch 65  	Train Loss = 19.95271 Val Loss = 20.13426
2023-06-01 13:19:59.666835 Epoch 66  	Train Loss = 19.93153 Val Loss = 20.10512
2023-06-01 13:21:36.745851 Epoch 67  	Train Loss = 19.91427 Val Loss = 20.04926
2023-06-01 13:23:13.905656 Epoch 68  	Train Loss = 19.91477 Val Loss = 20.07683
2023-06-01 13:24:51.025611 Epoch 69  	Train Loss = 19.90323 Val Loss = 20.12383
2023-06-01 13:26:28.156671 Epoch 70  	Train Loss = 19.91632 Val Loss = 20.10768
2023-06-01 13:28:05.279553 Epoch 71  	Train Loss = 19.89175 Val Loss = 20.11188
2023-06-01 13:29:42.442619 Epoch 72  	Train Loss = 19.89053 Val Loss = 20.09675
2023-06-01 13:31:19.690654 Epoch 73  	Train Loss = 19.90109 Val Loss = 20.09576
2023-06-01 13:32:56.773870 Epoch 74  	Train Loss = 19.88148 Val Loss = 20.04000
2023-06-01 13:34:33.860950 Epoch 75  	Train Loss = 19.87655 Val Loss = 20.02415
2023-06-01 13:36:10.989855 Epoch 76  	Train Loss = 19.85655 Val Loss = 20.08480
2023-06-01 13:37:48.099144 Epoch 77  	Train Loss = 19.84717 Val Loss = 20.03168
2023-06-01 13:39:25.215731 Epoch 78  	Train Loss = 19.85332 Val Loss = 20.04001
2023-06-01 13:41:02.370629 Epoch 79  	Train Loss = 19.84590 Val Loss = 20.03751
2023-06-01 13:42:39.496353 Epoch 80  	Train Loss = 19.83905 Val Loss = 20.04510
2023-06-01 13:44:16.630508 Epoch 81  	Train Loss = 19.84035 Val Loss = 20.06547
2023-06-01 13:45:53.767527 Epoch 82  	Train Loss = 19.82011 Val Loss = 19.98943
2023-06-01 13:47:30.884442 Epoch 83  	Train Loss = 19.82401 Val Loss = 19.99205
2023-06-01 13:49:08.030491 Epoch 84  	Train Loss = 19.81512 Val Loss = 20.01129
2023-06-01 13:50:45.175694 Epoch 85  	Train Loss = 19.80658 Val Loss = 20.06366
2023-06-01 13:52:22.341107 Epoch 86  	Train Loss = 19.82162 Val Loss = 19.98992
2023-06-01 13:53:59.504240 Epoch 87  	Train Loss = 19.79462 Val Loss = 20.05909
2023-06-01 13:55:36.685379 Epoch 88  	Train Loss = 19.79670 Val Loss = 20.03545
2023-06-01 13:57:13.855759 Epoch 89  	Train Loss = 19.79612 Val Loss = 20.00218
2023-06-01 13:58:50.978516 Epoch 90  	Train Loss = 19.78574 Val Loss = 19.97717
2023-06-01 14:00:28.081164 Epoch 91  	Train Loss = 19.78283 Val Loss = 20.09747
2023-06-01 14:02:05.201991 Epoch 92  	Train Loss = 19.78516 Val Loss = 20.03106
2023-06-01 14:03:42.510460 Epoch 93  	Train Loss = 19.78052 Val Loss = 20.03748
2023-06-01 14:05:19.838074 Epoch 94  	Train Loss = 19.75546 Val Loss = 19.94621
2023-06-01 14:06:57.208422 Epoch 95  	Train Loss = 19.75808 Val Loss = 19.99220
2023-06-01 14:08:34.556426 Epoch 96  	Train Loss = 19.74989 Val Loss = 20.02913
2023-06-01 14:10:11.855503 Epoch 97  	Train Loss = 19.74666 Val Loss = 20.02501
2023-06-01 14:11:49.056745 Epoch 98  	Train Loss = 19.75064 Val Loss = 19.96480
2023-06-01 14:13:26.177805 Epoch 99  	Train Loss = 19.73982 Val Loss = 19.93325
2023-06-01 14:15:03.280101 Epoch 100  	Train Loss = 19.72585 Val Loss = 19.94616
2023-06-01 14:16:40.353365 Epoch 101  	Train Loss = 19.73803 Val Loss = 19.95496
2023-06-01 14:18:17.516287 Epoch 102  	Train Loss = 19.73098 Val Loss = 19.92165
2023-06-01 14:19:54.797254 Epoch 103  	Train Loss = 19.72527 Val Loss = 19.90394
2023-06-01 14:21:32.100058 Epoch 104  	Train Loss = 19.71122 Val Loss = 19.91412
2023-06-01 14:23:09.393025 Epoch 105  	Train Loss = 19.70736 Val Loss = 19.93392
2023-06-01 14:24:46.660841 Epoch 106  	Train Loss = 19.68909 Val Loss = 19.97683
2023-06-01 14:26:23.904531 Epoch 107  	Train Loss = 19.69614 Val Loss = 19.94835
2023-06-01 14:28:01.021235 Epoch 108  	Train Loss = 19.70736 Val Loss = 19.95925
2023-06-01 14:29:38.142350 Epoch 109  	Train Loss = 19.68770 Val Loss = 19.93754
2023-06-01 14:31:15.212811 Epoch 110  	Train Loss = 19.68201 Val Loss = 19.94866
2023-06-01 14:32:52.277578 Epoch 111  	Train Loss = 19.68066 Val Loss = 20.01206
2023-06-01 14:34:29.437789 Epoch 112  	Train Loss = 19.67190 Val Loss = 19.89107
2023-06-01 14:36:06.655011 Epoch 113  	Train Loss = 19.68298 Val Loss = 19.92858
2023-06-01 14:37:43.887415 Epoch 114  	Train Loss = 19.66888 Val Loss = 19.91384
2023-06-01 14:39:21.141217 Epoch 115  	Train Loss = 19.65894 Val Loss = 19.89509
2023-06-01 14:40:58.318052 Epoch 116  	Train Loss = 19.66531 Val Loss = 19.90571
2023-06-01 14:42:35.477463 Epoch 117  	Train Loss = 19.66232 Val Loss = 19.90651
2023-06-01 14:44:12.664322 Epoch 118  	Train Loss = 19.64841 Val Loss = 19.98442
2023-06-01 14:45:49.782520 Epoch 119  	Train Loss = 19.65128 Val Loss = 19.89067
2023-06-01 14:47:26.888961 Epoch 120  	Train Loss = 19.65370 Val Loss = 19.90522
2023-06-01 14:49:03.981641 Epoch 121  	Train Loss = 19.63726 Val Loss = 19.92327
2023-06-01 14:50:41.176763 Epoch 122  	Train Loss = 19.62953 Val Loss = 19.91962
2023-06-01 14:52:18.334192 Epoch 123  	Train Loss = 19.62417 Val Loss = 19.87720
2023-06-01 14:53:55.504542 Epoch 124  	Train Loss = 19.62490 Val Loss = 19.90087
2023-06-01 14:55:32.700950 Epoch 125  	Train Loss = 19.62194 Val Loss = 19.86513
2023-06-01 14:57:09.876890 Epoch 126  	Train Loss = 19.62095 Val Loss = 19.91352
2023-06-01 14:58:47.009719 Epoch 127  	Train Loss = 19.60779 Val Loss = 19.93047
2023-06-01 15:00:24.127858 Epoch 128  	Train Loss = 19.60417 Val Loss = 19.86193
2023-06-01 15:02:01.273812 Epoch 129  	Train Loss = 19.59564 Val Loss = 19.89960
2023-06-01 15:03:38.416302 Epoch 130  	Train Loss = 19.59949 Val Loss = 19.82767
2023-06-01 15:05:15.589035 Epoch 131  	Train Loss = 19.59299 Val Loss = 19.82570
2023-06-01 15:06:52.729296 Epoch 132  	Train Loss = 19.58453 Val Loss = 19.85354
2023-06-01 15:08:29.862577 Epoch 133  	Train Loss = 19.59450 Val Loss = 19.86654
2023-06-01 15:10:06.999990 Epoch 134  	Train Loss = 19.57073 Val Loss = 19.82491
2023-06-01 15:11:44.082458 Epoch 135  	Train Loss = 19.58734 Val Loss = 19.85352
2023-06-01 15:13:21.207898 Epoch 136  	Train Loss = 19.57417 Val Loss = 19.83438
2023-06-01 15:14:58.388176 Epoch 137  	Train Loss = 19.57634 Val Loss = 19.85522
2023-06-01 15:16:35.600914 Epoch 138  	Train Loss = 19.56285 Val Loss = 19.79829
2023-06-01 15:18:12.807173 Epoch 139  	Train Loss = 19.55170 Val Loss = 19.90187
2023-06-01 15:19:49.952489 Epoch 140  	Train Loss = 19.55446 Val Loss = 19.90039
2023-06-01 15:21:27.131093 Epoch 141  	Train Loss = 19.55774 Val Loss = 19.85141
2023-06-01 15:23:04.294497 Epoch 142  	Train Loss = 19.54726 Val Loss = 19.84003
2023-06-01 15:24:41.446724 Epoch 143  	Train Loss = 19.54078 Val Loss = 19.80492
2023-06-01 15:26:18.590687 Epoch 144  	Train Loss = 19.54468 Val Loss = 19.80572
2023-06-01 15:27:55.729470 Epoch 145  	Train Loss = 19.54609 Val Loss = 19.76574
2023-06-01 15:29:32.948518 Epoch 146  	Train Loss = 19.53092 Val Loss = 19.85585
2023-06-01 15:31:10.125015 Epoch 147  	Train Loss = 19.53624 Val Loss = 19.81837
2023-06-01 15:32:47.332497 Epoch 148  	Train Loss = 19.53461 Val Loss = 19.83348
2023-06-01 15:34:24.516134 Epoch 149  	Train Loss = 19.51485 Val Loss = 19.81623
2023-06-01 15:36:01.666508 Epoch 150  	Train Loss = 19.50688 Val Loss = 19.84598
2023-06-01 15:37:38.826196 Epoch 151  	Train Loss = 19.51044 Val Loss = 19.86952
2023-06-01 15:39:15.983901 Epoch 152  	Train Loss = 19.50934 Val Loss = 19.75992
2023-06-01 15:40:53.120914 Epoch 153  	Train Loss = 19.50016 Val Loss = 19.79308
2023-06-01 15:42:30.278021 Epoch 154  	Train Loss = 19.50401 Val Loss = 19.81279
2023-06-01 15:44:07.469876 Epoch 155  	Train Loss = 19.50519 Val Loss = 19.77563
2023-06-01 15:45:44.745822 Epoch 156  	Train Loss = 19.49145 Val Loss = 19.78096
2023-06-01 15:47:22.016452 Epoch 157  	Train Loss = 19.50432 Val Loss = 19.88288
2023-06-01 15:48:59.378623 Epoch 158  	Train Loss = 19.48818 Val Loss = 19.78766
2023-06-01 15:50:36.798236 Epoch 159  	Train Loss = 19.48814 Val Loss = 19.81338
2023-06-01 15:52:14.209975 Epoch 160  	Train Loss = 19.48596 Val Loss = 19.82567
2023-06-01 15:53:51.598428 Epoch 161  	Train Loss = 19.48499 Val Loss = 19.73643
2023-06-01 15:55:28.973997 Epoch 162  	Train Loss = 19.48180 Val Loss = 19.79609
2023-06-01 15:57:06.376080 Epoch 163  	Train Loss = 19.46457 Val Loss = 19.83177
2023-06-01 15:58:43.824641 Epoch 164  	Train Loss = 19.46511 Val Loss = 19.87817
2023-06-01 16:00:21.217988 Epoch 165  	Train Loss = 19.46756 Val Loss = 19.83952
2023-06-01 16:01:58.580066 Epoch 166  	Train Loss = 19.45855 Val Loss = 19.76835
2023-06-01 16:03:36.008118 Epoch 167  	Train Loss = 19.47347 Val Loss = 19.80416
2023-06-01 16:05:13.475633 Epoch 168  	Train Loss = 19.44159 Val Loss = 19.73722
2023-06-01 16:06:51.002004 Epoch 169  	Train Loss = 19.46176 Val Loss = 19.76644
2023-06-01 16:08:28.449719 Epoch 170  	Train Loss = 19.44127 Val Loss = 19.73254
2023-06-01 16:10:05.894258 Epoch 171  	Train Loss = 19.45768 Val Loss = 19.75368
2023-06-01 16:11:43.366762 Epoch 172  	Train Loss = 19.45234 Val Loss = 19.81989
2023-06-01 16:13:20.856886 Epoch 173  	Train Loss = 19.44464 Val Loss = 19.74282
2023-06-01 16:14:58.336321 Epoch 174  	Train Loss = 19.43376 Val Loss = 19.68499
2023-06-01 16:16:35.728815 Epoch 175  	Train Loss = 19.42397 Val Loss = 19.72735
2023-06-01 16:18:13.122854 Epoch 176  	Train Loss = 19.44174 Val Loss = 19.73900
2023-06-01 16:19:50.513350 Epoch 177  	Train Loss = 19.43813 Val Loss = 19.74609
2023-06-01 16:21:27.817693 Epoch 178  	Train Loss = 19.41663 Val Loss = 19.73341
2023-06-01 16:23:05.033677 Epoch 179  	Train Loss = 19.41415 Val Loss = 19.75692
2023-06-01 16:24:42.235744 Epoch 180  	Train Loss = 19.40517 Val Loss = 19.70313
2023-06-01 16:26:19.414390 Epoch 181  	Train Loss = 19.42039 Val Loss = 19.78872
2023-06-01 16:27:56.580741 Epoch 182  	Train Loss = 19.42103 Val Loss = 19.89649
2023-06-01 16:29:33.764456 Epoch 183  	Train Loss = 19.41191 Val Loss = 19.78225
2023-06-01 16:31:10.909132 Epoch 184  	Train Loss = 19.40618 Val Loss = 19.73960
Early stopping at epoch: 184
Best at epoch 174:
Train Loss = 19.43376
Train RMSE = 32.43332, MAE = 19.56878, MAPE = 8.60871
Val Loss = 19.68499
Val RMSE = 33.36489, MAE = 20.23219, MAPE = 8.85350
--------- Test ---------
All Steps RMSE = 33.58353, MAE = 20.50936, MAPE = 8.63194
Step 1 RMSE = 27.31050, MAE = 17.00464, MAPE = 7.11537
Step 2 RMSE = 29.54322, MAE = 18.14777, MAPE = 7.59199
Step 3 RMSE = 30.94740, MAE = 18.93122, MAPE = 7.90623
Step 4 RMSE = 31.97407, MAE = 19.52783, MAPE = 8.16997
Step 5 RMSE = 32.84650, MAE = 20.04408, MAPE = 8.37914
Step 6 RMSE = 33.59852, MAE = 20.51481, MAPE = 8.59329
Step 7 RMSE = 34.31126, MAE = 20.97600, MAPE = 8.81977
Step 8 RMSE = 35.00585, MAE = 21.40805, MAPE = 9.03932
Step 9 RMSE = 35.60979, MAE = 21.80215, MAPE = 9.20906
Step 10 RMSE = 36.17537, MAE = 22.16937, MAPE = 9.38060
Step 11 RMSE = 36.75615, MAE = 22.56362, MAPE = 9.58561
Step 12 RMSE = 37.37848, MAE = 23.02015, MAPE = 9.79191
Inference time: 9.91 s
