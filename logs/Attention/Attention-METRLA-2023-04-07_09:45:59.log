METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 2)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 2)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 2)

--------- Attention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "lr": 0.001,
    "milestones": [
        10,
        40
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "load_npz": true,
    "pass_device": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                --                        --
├─Linear: 1-1                            [64, 207, 12, 64]         128
├─Linear: 1-2                            [64, 207, 12, 64]         128
├─Sequential: 1-3                        [64, 207, 12, 64]         --
│    └─SelfAttentionLayer: 2-1           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-2               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-3              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-4               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-5          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-6               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-7              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-8               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-9          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-10              [64, 207, 12, 64]         128
│    │    └─Sequential: 3-11             [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-12              [64, 207, 12, 64]         128
├─Linear: 1-4                            [64, 207, 64, 12]         156
├─Linear: 1-5                            [64, 207, 12, 1]          65
==========================================================================================
Total params: 150,429
Trainable params: 150,429
Non-trainable params: 0
Total mult-adds (M): 9.63
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 2931.52
Params size (MB): 0.60
Estimated Total Size (MB): 2933.39
==========================================================================================

Loss: MaskedMAELoss

2023-04-07 09:46:44.842552 Epoch 1  	Train Loss = 4.64866 Val Loss = 3.93895
2023-04-07 09:47:18.229550 Epoch 2  	Train Loss = 3.81175 Val Loss = 3.40180
2023-04-07 09:47:52.100487 Epoch 3  	Train Loss = 3.66506 Val Loss = 3.38507
2023-04-07 09:48:26.492145 Epoch 4  	Train Loss = 3.58594 Val Loss = 3.37907
2023-04-07 09:49:01.301617 Epoch 5  	Train Loss = 3.58299 Val Loss = 3.35353
2023-04-07 09:49:36.353108 Epoch 6  	Train Loss = 3.55174 Val Loss = 3.33703
2023-04-07 09:50:11.517731 Epoch 7  	Train Loss = 3.54866 Val Loss = 3.35162
2023-04-07 09:50:46.715935 Epoch 8  	Train Loss = 3.54765 Val Loss = 3.36730
2023-04-07 09:51:21.958400 Epoch 9  	Train Loss = 3.53295 Val Loss = 3.33054
2023-04-07 09:51:57.241448 Epoch 10  	Train Loss = 3.52793 Val Loss = 3.34524
2023-04-07 09:52:32.471397 Epoch 11  	Train Loss = 3.48801 Val Loss = 3.29281
2023-04-07 09:53:07.709254 Epoch 12  	Train Loss = 3.48108 Val Loss = 3.28813
2023-04-07 09:53:42.914944 Epoch 13  	Train Loss = 3.47864 Val Loss = 3.28938
2023-04-07 09:54:18.130129 Epoch 14  	Train Loss = 3.47834 Val Loss = 3.28770
2023-04-07 09:54:53.342747 Epoch 15  	Train Loss = 3.47724 Val Loss = 3.28441
2023-04-07 09:55:28.567414 Epoch 16  	Train Loss = 3.47650 Val Loss = 3.28538
2023-04-07 09:56:03.782973 Epoch 17  	Train Loss = 3.47524 Val Loss = 3.28759
2023-04-07 09:56:38.965908 Epoch 18  	Train Loss = 3.47313 Val Loss = 3.28360
2023-04-07 09:57:14.034246 Epoch 19  	Train Loss = 3.47132 Val Loss = 3.29370
2023-04-07 09:57:49.053282 Epoch 20  	Train Loss = 3.47076 Val Loss = 3.28746
2023-04-07 09:58:24.042712 Epoch 21  	Train Loss = 3.46876 Val Loss = 3.28235
2023-04-07 09:58:59.196832 Epoch 22  	Train Loss = 3.46776 Val Loss = 3.28203
2023-04-07 09:59:34.363917 Epoch 23  	Train Loss = 3.48375 Val Loss = 3.34574
2023-04-07 10:00:09.464477 Epoch 24  	Train Loss = 3.50771 Val Loss = 3.31142
2023-04-07 10:00:44.498221 Epoch 25  	Train Loss = 3.48798 Val Loss = 3.29610
2023-04-07 10:01:19.515805 Epoch 26  	Train Loss = 3.47958 Val Loss = 3.29099
2023-04-07 10:01:54.548667 Epoch 27  	Train Loss = 3.47415 Val Loss = 3.28221
2023-04-07 10:02:29.606054 Epoch 28  	Train Loss = 3.46889 Val Loss = 3.27691
2023-04-07 10:03:04.667774 Epoch 29  	Train Loss = 3.46364 Val Loss = 3.27786
2023-04-07 10:03:39.755172 Epoch 30  	Train Loss = 3.46062 Val Loss = 3.26959
2023-04-07 10:04:14.868824 Epoch 31  	Train Loss = 3.45688 Val Loss = 3.26587
2023-04-07 10:04:49.976054 Epoch 32  	Train Loss = 3.45250 Val Loss = 3.26666
2023-04-07 10:05:25.101627 Epoch 33  	Train Loss = 3.45011 Val Loss = 3.26723
2023-04-07 10:06:00.252443 Epoch 34  	Train Loss = 3.44727 Val Loss = 3.25853
2023-04-07 10:06:35.364269 Epoch 35  	Train Loss = 3.44622 Val Loss = 3.27098
2023-04-07 10:07:10.473764 Epoch 36  	Train Loss = 3.44912 Val Loss = 3.25876
2023-04-07 10:07:45.579049 Epoch 37  	Train Loss = 3.44123 Val Loss = 3.25770
2023-04-07 10:08:20.678065 Epoch 38  	Train Loss = 3.43898 Val Loss = 3.26447
2023-04-07 10:08:55.782634 Epoch 39  	Train Loss = 3.43792 Val Loss = 3.26049
2023-04-07 10:09:30.869605 Epoch 40  	Train Loss = 3.43549 Val Loss = 3.25148
2023-04-07 10:10:05.972876 Epoch 41  	Train Loss = 3.42582 Val Loss = 3.24696
2023-04-07 10:10:41.072703 Epoch 42  	Train Loss = 3.42542 Val Loss = 3.24795
2023-04-07 10:11:16.197855 Epoch 43  	Train Loss = 3.42533 Val Loss = 3.24742
2023-04-07 10:11:51.278077 Epoch 44  	Train Loss = 3.42439 Val Loss = 3.24607
2023-04-07 10:12:26.344188 Epoch 45  	Train Loss = 3.42476 Val Loss = 3.24755
2023-04-07 10:13:01.369297 Epoch 46  	Train Loss = 3.42515 Val Loss = 3.24659
2023-04-07 10:13:36.426035 Epoch 47  	Train Loss = 3.42407 Val Loss = 3.24672
2023-04-07 10:14:11.488853 Epoch 48  	Train Loss = 3.42368 Val Loss = 3.24629
2023-04-07 10:14:46.514160 Epoch 49  	Train Loss = 3.42347 Val Loss = 3.24511
2023-04-07 10:15:21.503160 Epoch 50  	Train Loss = 3.42290 Val Loss = 3.24585
2023-04-07 10:15:56.460124 Epoch 51  	Train Loss = 3.42361 Val Loss = 3.24694
2023-04-07 10:16:31.420183 Epoch 52  	Train Loss = 3.42274 Val Loss = 3.24546
2023-04-07 10:17:06.376862 Epoch 53  	Train Loss = 3.42276 Val Loss = 3.24459
2023-04-07 10:17:41.388760 Epoch 54  	Train Loss = 3.42403 Val Loss = 3.24604
2023-04-07 10:18:16.395956 Epoch 55  	Train Loss = 3.42185 Val Loss = 3.24609
2023-04-07 10:18:51.393122 Epoch 56  	Train Loss = 3.42229 Val Loss = 3.24510
2023-04-07 10:19:26.407270 Epoch 57  	Train Loss = 3.42171 Val Loss = 3.24465
2023-04-07 10:20:01.417312 Epoch 58  	Train Loss = 3.42116 Val Loss = 3.24468
2023-04-07 10:20:36.395568 Epoch 59  	Train Loss = 3.42137 Val Loss = 3.24519
2023-04-07 10:21:11.387064 Epoch 60  	Train Loss = 3.42097 Val Loss = 3.24472
2023-04-07 10:21:46.389425 Epoch 61  	Train Loss = 3.42204 Val Loss = 3.24579
2023-04-07 10:22:21.419631 Epoch 62  	Train Loss = 3.42058 Val Loss = 3.24520
2023-04-07 10:22:56.457444 Epoch 63  	Train Loss = 3.42107 Val Loss = 3.24549
Early stopping at epoch: 63
Best at epoch 53:
Train Loss = 3.42276
Train RMSE = 7.00249, MAE = 3.41986, MAPE = 9.50814
Val Loss = 3.24459
Val RMSE = 6.90312, MAE = 3.28384, MAPE = 9.50237
--------- Test ---------
All Steps RMSE = 7.33073, MAE = 3.60539, MAPE = 10.38343
Step 1 RMSE = 4.34411, MAE = 2.42439, MAPE = 6.04390
Step 2 RMSE = 5.25357, MAE = 2.74335, MAPE = 7.10676
Step 3 RMSE = 5.94077, MAE = 3.00500, MAPE = 8.03452
Step 4 RMSE = 6.42989, MAE = 3.22618, MAPE = 8.97282
Step 5 RMSE = 6.88721, MAE = 3.41343, MAPE = 9.58907
Step 6 RMSE = 7.27264, MAE = 3.60052, MAPE = 10.24996
Step 7 RMSE = 7.63286, MAE = 3.77047, MAPE = 11.05110
Step 8 RMSE = 7.95303, MAE = 3.92930, MAPE = 11.55441
Step 9 RMSE = 8.23699, MAE = 4.07667, MAPE = 12.17911
Step 10 RMSE = 8.53119, MAE = 4.21741, MAPE = 12.81489
Step 11 RMSE = 8.78641, MAE = 4.35983, MAPE = 13.24060
Step 12 RMSE = 9.06122, MAE = 4.49826, MAPE = 13.76437
Inference time: 3.27 s
