PEMSBAY
Trainset:	x-(36465, 12, 325, 2)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 2)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 2)	y-(10419, 12, 325, 1)

--------- Attention ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        40
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 325,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 3,
        "with_spatial": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                [64, 12, 325, 1]          --
├─Linear: 1-1                            [64, 12, 325, 64]         128
├─Linear: 1-2                            [64, 12, 325, 64]         128
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 325, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 325, 12, 64]         16,640
│    │    └─Dropout: 3-2                 [64, 325, 12, 64]         --
│    │    └─LayerNorm: 3-3               [64, 325, 12, 64]         128
│    │    └─Sequential: 3-4              [64, 325, 12, 64]         16,576
│    │    └─Dropout: 3-5                 [64, 325, 12, 64]         --
│    │    └─LayerNorm: 3-6               [64, 325, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 12, 325, 64]         --
│    │    └─AttentionLayer: 3-7          [64, 325, 12, 64]         16,640
│    │    └─Dropout: 3-8                 [64, 325, 12, 64]         --
│    │    └─LayerNorm: 3-9               [64, 325, 12, 64]         128
│    │    └─Sequential: 3-10             [64, 325, 12, 64]         16,576
│    │    └─Dropout: 3-11                [64, 325, 12, 64]         --
│    │    └─LayerNorm: 3-12              [64, 325, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 12, 325, 64]         --
│    │    └─AttentionLayer: 3-13         [64, 325, 12, 64]         16,640
│    │    └─Dropout: 3-14                [64, 325, 12, 64]         --
│    │    └─LayerNorm: 3-15              [64, 325, 12, 64]         128
│    │    └─Sequential: 3-16             [64, 325, 12, 64]         16,576
│    │    └─Dropout: 3-17                [64, 325, 12, 64]         --
│    │    └─LayerNorm: 3-18              [64, 325, 12, 64]         128
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 325, 64]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 325, 64]         16,640
│    │    └─Dropout: 3-20                [64, 12, 325, 64]         --
│    │    └─LayerNorm: 3-21              [64, 12, 325, 64]         128
│    │    └─Sequential: 3-22             [64, 12, 325, 64]         16,576
│    │    └─Dropout: 3-23                [64, 12, 325, 64]         --
│    │    └─LayerNorm: 3-24              [64, 12, 325, 64]         128
│    └─SelfAttentionLayer: 2-5           [64, 12, 325, 64]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 325, 64]         16,640
│    │    └─Dropout: 3-26                [64, 12, 325, 64]         --
│    │    └─LayerNorm: 3-27              [64, 12, 325, 64]         128
│    │    └─Sequential: 3-28             [64, 12, 325, 64]         16,576
│    │    └─Dropout: 3-29                [64, 12, 325, 64]         --
│    │    └─LayerNorm: 3-30              [64, 12, 325, 64]         128
│    └─SelfAttentionLayer: 2-6           [64, 12, 325, 64]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 325, 64]         16,640
│    │    └─Dropout: 3-32                [64, 12, 325, 64]         --
│    │    └─LayerNorm: 3-33              [64, 12, 325, 64]         128
│    │    └─Sequential: 3-34             [64, 12, 325, 64]         16,576
│    │    └─Dropout: 3-35                [64, 12, 325, 64]         --
│    │    └─LayerNorm: 3-36              [64, 12, 325, 64]         128
├─Linear: 1-5                            [64, 64, 325, 12]         156
├─Linear: 1-6                            [64, 12, 325, 1]          65
==========================================================================================
Total params: 201,309
Trainable params: 201,309
Non-trainable params: 0
Total mult-adds (M): 12.88
==========================================================================================
Input size (MB): 2.00
Forward/backward pass size (MB): 7286.32
Params size (MB): 0.81
Estimated Total Size (MB): 7289.13
==========================================================================================

Loss: MaskedMAELoss

2023-04-20 20:45:09.084218 Epoch 1  	Train Loss = 2.58407 Val Loss = 2.34291
2023-04-20 20:47:51.230716 Epoch 2  	Train Loss = 1.99097 Val Loss = 2.11417
2023-04-20 20:50:33.050582 Epoch 3  	Train Loss = 1.95265 Val Loss = 2.10448
2023-04-20 20:53:15.095013 Epoch 4  	Train Loss = 1.88284 Val Loss = 2.04471
2023-04-20 20:55:57.185482 Epoch 5  	Train Loss = 1.84135 Val Loss = 2.02069
2023-04-20 20:58:39.202103 Epoch 6  	Train Loss = 1.83801 Val Loss = 2.01023
2023-04-20 21:01:21.353388 Epoch 7  	Train Loss = 1.82613 Val Loss = 1.98596
2023-04-20 21:04:02.963945 Epoch 8  	Train Loss = 1.79718 Val Loss = 1.98452
2023-04-20 21:06:44.802578 Epoch 9  	Train Loss = 1.79941 Val Loss = 1.98692
2023-04-20 21:09:26.609138 Epoch 10  	Train Loss = 1.78385 Val Loss = 1.95834
2023-04-20 21:12:08.349391 Epoch 11  	Train Loss = 1.75141 Val Loss = 1.94038
2023-04-20 21:14:50.291093 Epoch 12  	Train Loss = 1.74703 Val Loss = 1.93807
2023-04-20 21:17:32.310796 Epoch 13  	Train Loss = 1.74493 Val Loss = 1.93863
2023-04-20 21:20:14.221177 Epoch 14  	Train Loss = 1.74278 Val Loss = 1.93485
2023-04-20 21:22:56.117451 Epoch 15  	Train Loss = 1.74251 Val Loss = 1.93394
2023-04-20 21:25:38.152985 Epoch 16  	Train Loss = 1.73941 Val Loss = 1.93400
2023-04-20 21:28:20.272626 Epoch 17  	Train Loss = 1.73792 Val Loss = 1.93501
2023-04-20 21:31:02.331917 Epoch 18  	Train Loss = 1.73634 Val Loss = 1.93694
2023-04-20 21:33:43.911198 Epoch 19  	Train Loss = 1.73500 Val Loss = 1.92894
2023-04-20 21:36:25.892203 Epoch 20  	Train Loss = 1.73383 Val Loss = 1.93257
2023-04-20 21:39:07.731650 Epoch 21  	Train Loss = 1.73214 Val Loss = 1.92684
2023-04-20 21:41:49.548985 Epoch 22  	Train Loss = 1.73094 Val Loss = 1.93068
2023-04-20 21:44:31.674128 Epoch 23  	Train Loss = 1.72996 Val Loss = 1.92586
2023-04-20 21:47:13.346926 Epoch 24  	Train Loss = 1.72853 Val Loss = 1.92473
2023-04-20 21:49:55.197048 Epoch 25  	Train Loss = 1.72739 Val Loss = 1.92394
2023-04-20 21:52:36.999608 Epoch 26  	Train Loss = 1.72653 Val Loss = 1.92326
2023-04-20 21:55:19.067324 Epoch 27  	Train Loss = 1.72538 Val Loss = 1.92290
2023-04-20 21:58:01.354720 Epoch 28  	Train Loss = 1.72463 Val Loss = 1.92078
2023-04-20 22:00:43.331848 Epoch 29  	Train Loss = 1.72293 Val Loss = 1.91950
2023-04-20 22:03:25.141558 Epoch 30  	Train Loss = 1.72190 Val Loss = 1.92214
2023-04-20 22:06:06.902776 Epoch 31  	Train Loss = 1.72107 Val Loss = 1.92349
2023-04-20 22:08:48.704748 Epoch 32  	Train Loss = 1.71982 Val Loss = 1.91836
2023-04-20 22:11:30.904498 Epoch 33  	Train Loss = 1.71957 Val Loss = 1.92185
2023-04-20 22:14:12.719859 Epoch 34  	Train Loss = 1.71855 Val Loss = 1.92064
2023-04-20 22:16:54.676096 Epoch 35  	Train Loss = 1.71767 Val Loss = 1.92517
2023-04-20 22:19:36.284088 Epoch 36  	Train Loss = 1.71681 Val Loss = 1.91798
2023-04-20 22:22:18.276358 Epoch 37  	Train Loss = 1.71564 Val Loss = 1.91925
2023-04-20 22:25:00.072664 Epoch 38  	Train Loss = 1.71496 Val Loss = 1.92470
2023-04-20 22:27:42.174365 Epoch 39  	Train Loss = 1.71486 Val Loss = 1.92186
2023-04-20 22:30:24.120069 Epoch 40  	Train Loss = 1.71395 Val Loss = 1.91638
2023-04-20 22:33:05.892770 Epoch 41  	Train Loss = 1.70740 Val Loss = 1.91385
2023-04-20 22:35:47.734870 Epoch 42  	Train Loss = 1.70704 Val Loss = 1.91378
2023-04-20 22:38:29.409383 Epoch 43  	Train Loss = 1.70681 Val Loss = 1.91477
2023-04-20 22:41:11.390729 Epoch 44  	Train Loss = 1.70656 Val Loss = 1.91341
2023-04-20 22:43:53.324814 Epoch 45  	Train Loss = 1.70647 Val Loss = 1.91397
2023-04-20 22:46:35.175928 Epoch 46  	Train Loss = 1.70661 Val Loss = 1.91452
2023-04-20 22:49:17.341019 Epoch 47  	Train Loss = 1.70620 Val Loss = 1.91392
2023-04-20 22:51:59.142819 Epoch 48  	Train Loss = 1.70617 Val Loss = 1.91422
2023-04-20 22:54:41.158255 Epoch 49  	Train Loss = 1.70590 Val Loss = 1.91432
2023-04-20 22:57:23.115911 Epoch 50  	Train Loss = 1.70584 Val Loss = 1.91340
2023-04-20 23:00:04.937638 Epoch 51  	Train Loss = 1.70561 Val Loss = 1.91252
2023-04-20 23:02:47.020730 Epoch 52  	Train Loss = 1.70563 Val Loss = 1.91548
2023-04-20 23:05:29.122204 Epoch 53  	Train Loss = 1.70534 Val Loss = 1.91365
2023-04-20 23:08:11.057386 Epoch 54  	Train Loss = 1.70526 Val Loss = 1.91196
2023-04-20 23:10:52.896095 Epoch 55  	Train Loss = 1.70515 Val Loss = 1.91364
2023-04-20 23:13:34.683022 Epoch 56  	Train Loss = 1.70502 Val Loss = 1.91297
2023-04-20 23:16:16.683189 Epoch 57  	Train Loss = 1.70499 Val Loss = 1.91235
2023-04-20 23:18:58.994645 Epoch 58  	Train Loss = 1.70482 Val Loss = 1.91314
2023-04-20 23:21:40.798698 Epoch 59  	Train Loss = 1.70470 Val Loss = 1.91393
2023-04-20 23:24:22.934677 Epoch 60  	Train Loss = 1.70466 Val Loss = 1.91504
2023-04-20 23:27:04.784535 Epoch 61  	Train Loss = 1.70455 Val Loss = 1.91495
2023-04-20 23:29:46.627016 Epoch 62  	Train Loss = 1.70434 Val Loss = 1.91331
2023-04-20 23:32:28.926190 Epoch 63  	Train Loss = 1.70426 Val Loss = 1.91378
2023-04-20 23:35:11.086984 Epoch 64  	Train Loss = 1.70425 Val Loss = 1.91328
Early stopping at epoch: 64
Best at epoch 54:
Train Loss = 1.70526
Train RMSE = 3.93932, MAE = 1.70378, MAPE = 3.82941
Val Loss = 1.91196
Val RMSE = 4.46917, MAE = 1.90096, MAPE = 4.51875
--------- Test ---------
All Steps RMSE = 4.13739, MAE = 1.77253, MAPE = 4.10474
Step 1 RMSE = 1.62690, MAE = 0.87999, MAPE = 1.70495
Step 2 RMSE = 2.38434, MAE = 1.17410, MAPE = 2.39084
Step 3 RMSE = 2.99637, MAE = 1.38853, MAPE = 2.93789
Step 4 RMSE = 3.47141, MAE = 1.55730, MAPE = 3.40054
Step 5 RMSE = 3.84480, MAE = 1.69551, MAPE = 3.80391
Step 6 RMSE = 4.14800, MAE = 1.81409, MAPE = 4.16596
Step 7 RMSE = 4.40407, MAE = 1.91876, MAPE = 4.48984
Step 8 RMSE = 4.62738, MAE = 2.01299, MAPE = 4.79269
Step 9 RMSE = 4.82002, MAE = 2.09651, MAPE = 5.05358
Step 10 RMSE = 4.99969, MAE = 2.17318, MAPE = 5.29417
Step 11 RMSE = 5.16559, MAE = 2.24454, MAPE = 5.50988
Step 12 RMSE = 5.32409, MAE = 2.31490, MAPE = 5.71262
Inference time: 13.26 s
