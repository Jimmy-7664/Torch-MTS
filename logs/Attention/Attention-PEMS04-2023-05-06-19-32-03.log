PEMS04
Trainset:	x-(10181, 12, 307, 2)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 2)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 2)	y-(3394, 12, 307, 1)

--------- Attention ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        40
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "with_spatial": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                [64, 12, 307, 1]          --
├─Linear: 1-1                            [64, 12, 307, 64]         128
├─Linear: 1-2                            [64, 12, 307, 64]         128
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 307, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 307, 12, 64]         16,640
│    │    └─Dropout: 3-2                 [64, 307, 12, 64]         --
│    │    └─LayerNorm: 3-3               [64, 307, 12, 64]         128
│    │    └─Sequential: 3-4              [64, 307, 12, 64]         33,088
│    │    └─Dropout: 3-5                 [64, 307, 12, 64]         --
│    │    └─LayerNorm: 3-6               [64, 307, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 12, 307, 64]         --
│    │    └─AttentionLayer: 3-7          [64, 307, 12, 64]         16,640
│    │    └─Dropout: 3-8                 [64, 307, 12, 64]         --
│    │    └─LayerNorm: 3-9               [64, 307, 12, 64]         128
│    │    └─Sequential: 3-10             [64, 307, 12, 64]         33,088
│    │    └─Dropout: 3-11                [64, 307, 12, 64]         --
│    │    └─LayerNorm: 3-12              [64, 307, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 12, 307, 64]         --
│    │    └─AttentionLayer: 3-13         [64, 307, 12, 64]         16,640
│    │    └─Dropout: 3-14                [64, 307, 12, 64]         --
│    │    └─LayerNorm: 3-15              [64, 307, 12, 64]         128
│    │    └─Sequential: 3-16             [64, 307, 12, 64]         33,088
│    │    └─Dropout: 3-17                [64, 307, 12, 64]         --
│    │    └─LayerNorm: 3-18              [64, 307, 12, 64]         128
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 307, 64]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 307, 64]         16,640
│    │    └─Dropout: 3-20                [64, 12, 307, 64]         --
│    │    └─LayerNorm: 3-21              [64, 12, 307, 64]         128
│    │    └─Sequential: 3-22             [64, 12, 307, 64]         33,088
│    │    └─Dropout: 3-23                [64, 12, 307, 64]         --
│    │    └─LayerNorm: 3-24              [64, 12, 307, 64]         128
│    └─SelfAttentionLayer: 2-5           [64, 12, 307, 64]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 307, 64]         16,640
│    │    └─Dropout: 3-26                [64, 12, 307, 64]         --
│    │    └─LayerNorm: 3-27              [64, 12, 307, 64]         128
│    │    └─Sequential: 3-28             [64, 12, 307, 64]         33,088
│    │    └─Dropout: 3-29                [64, 12, 307, 64]         --
│    │    └─LayerNorm: 3-30              [64, 12, 307, 64]         128
│    └─SelfAttentionLayer: 2-6           [64, 12, 307, 64]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 307, 64]         16,640
│    │    └─Dropout: 3-32                [64, 12, 307, 64]         --
│    │    └─LayerNorm: 3-33              [64, 12, 307, 64]         128
│    │    └─Sequential: 3-34             [64, 12, 307, 64]         33,088
│    │    └─Dropout: 3-35                [64, 12, 307, 64]         --
│    │    └─LayerNorm: 3-36              [64, 12, 307, 64]         128
├─Linear: 1-5                            [64, 64, 307, 12]         156
├─Linear: 1-6                            [64, 12, 307, 1]          65
==========================================================================================
Total params: 300,381
Trainable params: 300,381
Non-trainable params: 0
Total mult-adds (M): 19.22
==========================================================================================
Input size (MB): 1.89
Forward/backward pass size (MB): 8331.38
Params size (MB): 1.20
Estimated Total Size (MB): 8334.47
==========================================================================================

Loss: HuberLoss

2023-05-06 19:32:53.716335 Epoch 1  	Train Loss = 40.83407 Val Loss = 30.03355
2023-05-06 19:33:40.684632 Epoch 2  	Train Loss = 27.00743 Val Loss = 26.93111
2023-05-06 19:34:27.838536 Epoch 3  	Train Loss = 25.40814 Val Loss = 29.76587
2023-05-06 19:35:15.170743 Epoch 4  	Train Loss = 24.68391 Val Loss = 29.12067
2023-05-06 19:36:02.579175 Epoch 5  	Train Loss = 24.49836 Val Loss = 27.18561
2023-05-06 19:36:49.942888 Epoch 6  	Train Loss = 23.98876 Val Loss = 25.72933
2023-05-06 19:37:37.298347 Epoch 7  	Train Loss = 23.42897 Val Loss = 24.88351
2023-05-06 19:38:24.621081 Epoch 8  	Train Loss = 23.37777 Val Loss = 24.17874
2023-05-06 19:39:11.958204 Epoch 9  	Train Loss = 23.05224 Val Loss = 25.19584
2023-05-06 19:39:59.253245 Epoch 10  	Train Loss = 22.86080 Val Loss = 24.14040
2023-05-06 19:40:46.537036 Epoch 11  	Train Loss = 22.33502 Val Loss = 23.28596
2023-05-06 19:41:33.827467 Epoch 12  	Train Loss = 22.20983 Val Loss = 23.28825
2023-05-06 19:42:21.134326 Epoch 13  	Train Loss = 22.17176 Val Loss = 23.20272
2023-05-06 19:43:08.471571 Epoch 14  	Train Loss = 22.21225 Val Loss = 23.28717
2023-05-06 19:43:55.755018 Epoch 15  	Train Loss = 22.18712 Val Loss = 23.11692
2023-05-06 19:44:43.067350 Epoch 16  	Train Loss = 22.08555 Val Loss = 23.11820
2023-05-06 19:45:30.353878 Epoch 17  	Train Loss = 22.12358 Val Loss = 23.11931
2023-05-06 19:46:17.656225 Epoch 18  	Train Loss = 22.13357 Val Loss = 23.06811
2023-05-06 19:47:05.008611 Epoch 19  	Train Loss = 22.14474 Val Loss = 23.10094
2023-05-06 19:47:52.453189 Epoch 20  	Train Loss = 22.02741 Val Loss = 23.04847
2023-05-06 19:48:39.850114 Epoch 21  	Train Loss = 22.04719 Val Loss = 23.04765
2023-05-06 19:49:27.248766 Epoch 22  	Train Loss = 22.03334 Val Loss = 23.00555
2023-05-06 19:50:14.662501 Epoch 23  	Train Loss = 21.99621 Val Loss = 23.02163
2023-05-06 19:51:02.031940 Epoch 24  	Train Loss = 21.95371 Val Loss = 23.19132
2023-05-06 19:51:49.392297 Epoch 25  	Train Loss = 21.95710 Val Loss = 22.98725
2023-05-06 19:52:36.730925 Epoch 26  	Train Loss = 21.95091 Val Loss = 22.92746
2023-05-06 19:53:24.048911 Epoch 27  	Train Loss = 21.96508 Val Loss = 23.08149
2023-05-06 19:54:11.331436 Epoch 28  	Train Loss = 21.95464 Val Loss = 22.94100
2023-05-06 19:54:58.616474 Epoch 29  	Train Loss = 21.85906 Val Loss = 23.00069
2023-05-06 19:55:45.891494 Epoch 30  	Train Loss = 21.91509 Val Loss = 22.86599
2023-05-06 19:56:33.202847 Epoch 31  	Train Loss = 21.91655 Val Loss = 23.07429
2023-05-06 19:57:20.523119 Epoch 32  	Train Loss = 21.84996 Val Loss = 22.93647
2023-05-06 19:58:07.842118 Epoch 33  	Train Loss = 21.85433 Val Loss = 22.96251
2023-05-06 19:58:55.161277 Epoch 34  	Train Loss = 21.85317 Val Loss = 22.85108
2023-05-06 19:59:42.448653 Epoch 35  	Train Loss = 21.84764 Val Loss = 22.81587
2023-05-06 20:00:29.733800 Epoch 36  	Train Loss = 21.81718 Val Loss = 22.89791
2023-05-06 20:01:17.083153 Epoch 37  	Train Loss = 21.79888 Val Loss = 23.03792
2023-05-06 20:02:04.414136 Epoch 38  	Train Loss = 21.86361 Val Loss = 22.86626
2023-05-06 20:02:51.794182 Epoch 39  	Train Loss = 21.74865 Val Loss = 22.87511
2023-05-06 20:03:39.175799 Epoch 40  	Train Loss = 21.74579 Val Loss = 22.82396
2023-05-06 20:04:26.633454 Epoch 41  	Train Loss = 21.65007 Val Loss = 22.73767
2023-05-06 20:05:14.032801 Epoch 42  	Train Loss = 21.66795 Val Loss = 22.72792
2023-05-06 20:06:01.344058 Epoch 43  	Train Loss = 21.67621 Val Loss = 22.72751
2023-05-06 20:06:48.616605 Epoch 44  	Train Loss = 21.66485 Val Loss = 22.73355
2023-05-06 20:07:35.891267 Epoch 45  	Train Loss = 21.67949 Val Loss = 22.74230
2023-05-06 20:08:23.182302 Epoch 46  	Train Loss = 21.64380 Val Loss = 22.73650
2023-05-06 20:09:10.453556 Epoch 47  	Train Loss = 21.63800 Val Loss = 22.72916
2023-05-06 20:09:57.729701 Epoch 48  	Train Loss = 21.67529 Val Loss = 22.73223
2023-05-06 20:10:45.005984 Epoch 49  	Train Loss = 21.68274 Val Loss = 22.72792
2023-05-06 20:11:32.288404 Epoch 50  	Train Loss = 21.67346 Val Loss = 22.72488
2023-05-06 20:12:19.584985 Epoch 51  	Train Loss = 21.64309 Val Loss = 22.74562
2023-05-06 20:13:06.746339 Epoch 52  	Train Loss = 21.67030 Val Loss = 22.72760
2023-05-06 20:13:53.937002 Epoch 53  	Train Loss = 21.66566 Val Loss = 22.74393
2023-05-06 20:14:41.095390 Epoch 54  	Train Loss = 21.66843 Val Loss = 22.76108
2023-05-06 20:15:28.310350 Epoch 55  	Train Loss = 21.70842 Val Loss = 22.72065
2023-05-06 20:16:15.508842 Epoch 56  	Train Loss = 21.67978 Val Loss = 22.70745
2023-05-06 20:17:02.733529 Epoch 57  	Train Loss = 21.65972 Val Loss = 22.72474
2023-05-06 20:17:49.965294 Epoch 58  	Train Loss = 21.67810 Val Loss = 22.71585
2023-05-06 20:18:37.267534 Epoch 59  	Train Loss = 21.64996 Val Loss = 22.71747
2023-05-06 20:19:24.598194 Epoch 60  	Train Loss = 21.61252 Val Loss = 22.71526
2023-05-06 20:20:12.035965 Epoch 61  	Train Loss = 21.64744 Val Loss = 22.70641
2023-05-06 20:20:59.324107 Epoch 62  	Train Loss = 21.60617 Val Loss = 22.70579
2023-05-06 20:21:46.663685 Epoch 63  	Train Loss = 21.65286 Val Loss = 22.70618
2023-05-06 20:22:33.988086 Epoch 64  	Train Loss = 21.64536 Val Loss = 22.70301
2023-05-06 20:23:21.333916 Epoch 65  	Train Loss = 21.68452 Val Loss = 22.71657
2023-05-06 20:24:08.659985 Epoch 66  	Train Loss = 21.61639 Val Loss = 22.69904
2023-05-06 20:24:55.950256 Epoch 67  	Train Loss = 21.63903 Val Loss = 22.71946
2023-05-06 20:25:43.225453 Epoch 68  	Train Loss = 21.60066 Val Loss = 22.68930
2023-05-06 20:26:30.552392 Epoch 69  	Train Loss = 21.62793 Val Loss = 22.72237
2023-05-06 20:27:17.758406 Epoch 70  	Train Loss = 21.65004 Val Loss = 22.72346
2023-05-06 20:28:04.924468 Epoch 71  	Train Loss = 21.61315 Val Loss = 22.71125
2023-05-06 20:28:52.086550 Epoch 72  	Train Loss = 21.63145 Val Loss = 22.70011
2023-05-06 20:29:39.263265 Epoch 73  	Train Loss = 21.62501 Val Loss = 22.70334
2023-05-06 20:30:26.509293 Epoch 74  	Train Loss = 21.62241 Val Loss = 22.70429
2023-05-06 20:31:13.748439 Epoch 75  	Train Loss = 21.63652 Val Loss = 22.68764
2023-05-06 20:32:01.032694 Epoch 76  	Train Loss = 21.61335 Val Loss = 22.69883
2023-05-06 20:32:48.306254 Epoch 77  	Train Loss = 21.57793 Val Loss = 22.70632
2023-05-06 20:33:35.714328 Epoch 78  	Train Loss = 21.61393 Val Loss = 22.70771
2023-05-06 20:34:23.173441 Epoch 79  	Train Loss = 21.64286 Val Loss = 22.67792
2023-05-06 20:35:10.522390 Epoch 80  	Train Loss = 21.58759 Val Loss = 22.67945
2023-05-06 20:35:57.799587 Epoch 81  	Train Loss = 21.60566 Val Loss = 22.68066
2023-05-06 20:36:45.174959 Epoch 82  	Train Loss = 21.58326 Val Loss = 22.69631
2023-05-06 20:37:32.465907 Epoch 83  	Train Loss = 21.59872 Val Loss = 22.66956
2023-05-06 20:38:19.774691 Epoch 84  	Train Loss = 21.59814 Val Loss = 22.68925
2023-05-06 20:39:07.062624 Epoch 85  	Train Loss = 21.59753 Val Loss = 22.67012
2023-05-06 20:39:54.342629 Epoch 86  	Train Loss = 21.58217 Val Loss = 22.69000
2023-05-06 20:40:41.632863 Epoch 87  	Train Loss = 21.57781 Val Loss = 22.69119
2023-05-06 20:41:28.876847 Epoch 88  	Train Loss = 21.57721 Val Loss = 22.68891
2023-05-06 20:42:16.142087 Epoch 89  	Train Loss = 21.57605 Val Loss = 22.67035
2023-05-06 20:43:03.419838 Epoch 90  	Train Loss = 21.60061 Val Loss = 22.69348
2023-05-06 20:43:50.711451 Epoch 91  	Train Loss = 21.61304 Val Loss = 22.67389
2023-05-06 20:44:38.128438 Epoch 92  	Train Loss = 21.57744 Val Loss = 22.66120
2023-05-06 20:45:25.409875 Epoch 93  	Train Loss = 21.56384 Val Loss = 22.68449
2023-05-06 20:46:12.694355 Epoch 94  	Train Loss = 21.60566 Val Loss = 22.67162
2023-05-06 20:47:00.064463 Epoch 95  	Train Loss = 21.60061 Val Loss = 22.67030
2023-05-06 20:47:47.476477 Epoch 96  	Train Loss = 21.59041 Val Loss = 22.65804
2023-05-06 20:48:34.862521 Epoch 97  	Train Loss = 21.58626 Val Loss = 22.65168
2023-05-06 20:49:22.302554 Epoch 98  	Train Loss = 21.59220 Val Loss = 22.71579
2023-05-06 20:50:09.799804 Epoch 99  	Train Loss = 21.63771 Val Loss = 22.66690
2023-05-06 20:50:57.193744 Epoch 100  	Train Loss = 21.61825 Val Loss = 22.67887
2023-05-06 20:51:44.478782 Epoch 101  	Train Loss = 21.60224 Val Loss = 22.66990
2023-05-06 20:52:31.760813 Epoch 102  	Train Loss = 21.57206 Val Loss = 22.67561
2023-05-06 20:53:19.037711 Epoch 103  	Train Loss = 21.54751 Val Loss = 22.64959
2023-05-06 20:54:06.318563 Epoch 104  	Train Loss = 21.59398 Val Loss = 22.65062
2023-05-06 20:54:53.583267 Epoch 105  	Train Loss = 21.60821 Val Loss = 22.65989
2023-05-06 20:55:40.857547 Epoch 106  	Train Loss = 21.57313 Val Loss = 22.63508
2023-05-06 20:56:28.134981 Epoch 107  	Train Loss = 21.58286 Val Loss = 22.65024
2023-05-06 20:57:15.403098 Epoch 108  	Train Loss = 21.56169 Val Loss = 22.64668
2023-05-06 20:58:02.681368 Epoch 109  	Train Loss = 21.52954 Val Loss = 22.67073
2023-05-06 20:58:49.958165 Epoch 110  	Train Loss = 21.58259 Val Loss = 22.64320
2023-05-06 20:59:37.232748 Epoch 111  	Train Loss = 21.59053 Val Loss = 22.63696
2023-05-06 21:00:24.499945 Epoch 112  	Train Loss = 21.54483 Val Loss = 22.64329
2023-05-06 21:01:11.767087 Epoch 113  	Train Loss = 21.56617 Val Loss = 22.63390
2023-05-06 21:01:59.060629 Epoch 114  	Train Loss = 21.53128 Val Loss = 22.64390
2023-05-06 21:02:46.330382 Epoch 115  	Train Loss = 21.57665 Val Loss = 22.65043
2023-05-06 21:03:33.611282 Epoch 116  	Train Loss = 21.53970 Val Loss = 22.63580
2023-05-06 21:04:20.891270 Epoch 117  	Train Loss = 21.57916 Val Loss = 22.65815
2023-05-06 21:05:08.176409 Epoch 118  	Train Loss = 21.49748 Val Loss = 22.62731
2023-05-06 21:05:55.443260 Epoch 119  	Train Loss = 21.58774 Val Loss = 22.65986
2023-05-06 21:06:42.573710 Epoch 120  	Train Loss = 21.54957 Val Loss = 22.62647
2023-05-06 21:07:29.754849 Epoch 121  	Train Loss = 21.53726 Val Loss = 22.62304
2023-05-06 21:08:16.903351 Epoch 122  	Train Loss = 21.53028 Val Loss = 22.65224
2023-05-06 21:09:04.121937 Epoch 123  	Train Loss = 21.51806 Val Loss = 22.62081
2023-05-06 21:09:51.287721 Epoch 124  	Train Loss = 21.51058 Val Loss = 22.62934
2023-05-06 21:10:38.453968 Epoch 125  	Train Loss = 21.50864 Val Loss = 22.63056
2023-05-06 21:11:25.622185 Epoch 126  	Train Loss = 21.52058 Val Loss = 22.62823
2023-05-06 21:12:12.800104 Epoch 127  	Train Loss = 21.55403 Val Loss = 22.62615
2023-05-06 21:12:59.956166 Epoch 128  	Train Loss = 21.53812 Val Loss = 22.63195
2023-05-06 21:13:47.132963 Epoch 129  	Train Loss = 21.56849 Val Loss = 22.61357
2023-05-06 21:14:34.345818 Epoch 130  	Train Loss = 21.53898 Val Loss = 22.61459
2023-05-06 21:15:21.511055 Epoch 131  	Train Loss = 21.54800 Val Loss = 22.64303
2023-05-06 21:16:08.786071 Epoch 132  	Train Loss = 21.47493 Val Loss = 22.63849
2023-05-06 21:16:55.948184 Epoch 133  	Train Loss = 21.51962 Val Loss = 22.64221
2023-05-06 21:17:43.146846 Epoch 134  	Train Loss = 21.51675 Val Loss = 22.62033
2023-05-06 21:18:30.299803 Epoch 135  	Train Loss = 21.53456 Val Loss = 22.63341
2023-05-06 21:19:17.558584 Epoch 136  	Train Loss = 21.50498 Val Loss = 22.61922
2023-05-06 21:20:04.795789 Epoch 137  	Train Loss = 21.51807 Val Loss = 22.63154
2023-05-06 21:20:51.924785 Epoch 138  	Train Loss = 21.55824 Val Loss = 22.62303
2023-05-06 21:21:39.159726 Epoch 139  	Train Loss = 21.51790 Val Loss = 22.62465
Early stopping at epoch: 139
Best at epoch 129:
Train Loss = 21.56849
Train RMSE = 35.49660, MAE = 22.20174, MAPE = 15.59062
Val Loss = 22.61357
Val RMSE = 37.56955, MAE = 23.56016, MAPE = 15.15403
--------- Test ---------
All Steps RMSE = 35.55119, MAE = 22.41508, MAPE = 14.65538
Step 1 RMSE = 28.74388, MAE = 18.02024, MAPE = 11.75018
Step 2 RMSE = 30.12481, MAE = 18.88333, MAPE = 12.28990
Step 3 RMSE = 31.55472, MAE = 19.85393, MAPE = 12.92263
Step 4 RMSE = 32.66661, MAE = 20.59042, MAPE = 13.40503
Step 5 RMSE = 33.81606, MAE = 21.35953, MAPE = 13.91778
Step 6 RMSE = 34.94166, MAE = 22.11790, MAPE = 14.40668
Step 7 RMSE = 36.04539, MAE = 22.89017, MAPE = 14.97837
Step 8 RMSE = 37.09274, MAE = 23.59011, MAPE = 15.38685
Step 9 RMSE = 38.09870, MAE = 24.26899, MAPE = 15.91400
Step 10 RMSE = 39.10795, MAE = 24.95118, MAPE = 16.37035
Step 11 RMSE = 40.26359, MAE = 25.76438, MAPE = 16.95294
Step 12 RMSE = 41.57357, MAE = 26.69043, MAPE = 17.56931
Inference time: 4.35 s
