METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 2)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 2)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 2)

--------- Attention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "lr": 0.001,
    "milestones": [
        8,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "load_npz": true,
    "pass_device": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 256,
        "feed_forward_dim": 512,
        "num_heads": 8,
        "num_layers": 3
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                --                        --
├─Linear: 1-1                            [64, 207, 12, 256]        512
├─Linear: 1-2                            [64, 207, 12, 256]        512
├─Sequential: 1-3                        [64, 207, 12, 256]        --
│    └─SelfAttentionLayer: 2-1           [64, 207, 12, 256]        --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 256]        197,376
│    │    └─LayerNorm: 3-2               [64, 207, 12, 256]        512
│    │    └─Sequential: 3-3              [64, 207, 12, 256]        262,912
│    │    └─LayerNorm: 3-4               [64, 207, 12, 256]        512
│    └─SelfAttentionLayer: 2-2           [64, 207, 12, 256]        --
│    │    └─AttentionLayer: 3-5          [64, 207, 12, 256]        197,376
│    │    └─LayerNorm: 3-6               [64, 207, 12, 256]        512
│    │    └─Sequential: 3-7              [64, 207, 12, 256]        262,912
│    │    └─LayerNorm: 3-8               [64, 207, 12, 256]        512
│    └─SelfAttentionLayer: 2-3           [64, 207, 12, 256]        --
│    │    └─AttentionLayer: 3-9          [64, 207, 12, 256]        197,376
│    │    └─LayerNorm: 3-10              [64, 207, 12, 256]        512
│    │    └─Sequential: 3-11             [64, 207, 12, 256]        262,912
│    │    └─LayerNorm: 3-12              [64, 207, 12, 256]        512
├─Linear: 1-4                            [64, 207, 256, 12]        156
├─Linear: 1-5                            [64, 207, 12, 1]          257
==========================================================================================
Total params: 1,385,373
Trainable params: 1,385,373
Non-trainable params: 0
Total mult-adds (M): 88.66
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 8792.01
Params size (MB): 5.54
Estimated Total Size (MB): 8798.82
==========================================================================================

Loss: MaskedMAELoss

2023-04-06 20:13:29.913941 Epoch 1  	Train Loss = 5.23724 Val Loss = 3.89819
2023-04-06 20:15:10.447384 Epoch 2  	Train Loss = 4.03697 Val Loss = 4.21742
2023-04-06 20:16:51.576506 Epoch 3  	Train Loss = 3.87528 Val Loss = 3.79875
2023-04-06 20:18:32.271954 Epoch 4  	Train Loss = 3.65933 Val Loss = 3.44655
2023-04-06 20:20:13.107795 Epoch 5  	Train Loss = 3.60747 Val Loss = 3.45708
2023-04-06 20:21:54.225717 Epoch 6  	Train Loss = 3.58968 Val Loss = 3.37774
2023-04-06 20:23:35.196762 Epoch 7  	Train Loss = 3.58885 Val Loss = 3.38817
2023-04-06 20:25:15.787282 Epoch 8  	Train Loss = 3.58155 Val Loss = 3.33905
2023-04-06 20:26:55.804719 Epoch 9  	Train Loss = 3.51479 Val Loss = 3.31392
2023-04-06 20:28:35.010783 Epoch 10  	Train Loss = 3.50858 Val Loss = 3.31308
2023-04-06 20:30:14.153591 Epoch 11  	Train Loss = 3.50558 Val Loss = 3.31156
2023-04-06 20:31:53.676537 Epoch 12  	Train Loss = 3.50338 Val Loss = 3.31121
2023-04-06 20:33:33.462370 Epoch 13  	Train Loss = 3.50163 Val Loss = 3.30341
2023-04-06 20:35:13.963651 Epoch 14  	Train Loss = 3.49872 Val Loss = 3.30456
2023-04-06 20:36:55.437851 Epoch 15  	Train Loss = 3.49665 Val Loss = 3.29992
2023-04-06 20:38:36.934160 Epoch 16  	Train Loss = 3.49931 Val Loss = 3.30721
2023-04-06 20:40:17.287229 Epoch 17  	Train Loss = 3.49274 Val Loss = 3.29938
2023-04-06 20:41:57.580250 Epoch 18  	Train Loss = 3.49031 Val Loss = 3.29618
2023-04-06 20:43:37.848088 Epoch 19  	Train Loss = 3.48811 Val Loss = 3.31132
2023-04-06 20:45:18.294064 Epoch 20  	Train Loss = 3.48389 Val Loss = 3.29252
2023-04-06 20:46:58.881684 Epoch 21  	Train Loss = 3.48512 Val Loss = 3.28843
2023-04-06 20:48:39.740653 Epoch 22  	Train Loss = 3.48097 Val Loss = 3.29329
2023-04-06 20:50:21.151197 Epoch 23  	Train Loss = 3.48023 Val Loss = 3.29091
2023-04-06 20:52:03.060816 Epoch 24  	Train Loss = 3.47708 Val Loss = 3.28660
2023-04-06 20:53:44.032868 Epoch 25  	Train Loss = 3.47640 Val Loss = 3.29522
2023-04-06 20:55:24.179733 Epoch 26  	Train Loss = 3.47631 Val Loss = 3.28306
2023-04-06 20:57:04.141265 Epoch 27  	Train Loss = 3.47215 Val Loss = 3.29771
2023-04-06 20:58:44.051711 Epoch 28  	Train Loss = 3.47208 Val Loss = 3.29777
2023-04-06 21:00:24.218178 Epoch 29  	Train Loss = 3.46896 Val Loss = 3.28655
2023-04-06 21:02:04.646821 Epoch 30  	Train Loss = 3.46973 Val Loss = 3.28779
2023-04-06 21:03:45.567857 Epoch 31  	Train Loss = 3.45759 Val Loss = 3.27802
2023-04-06 21:05:27.129891 Epoch 32  	Train Loss = 3.45582 Val Loss = 3.28059
2023-04-06 21:07:08.979450 Epoch 33  	Train Loss = 3.45594 Val Loss = 3.27862
2023-04-06 21:08:49.644125 Epoch 34  	Train Loss = 3.45544 Val Loss = 3.27927
2023-04-06 21:10:29.937578 Epoch 35  	Train Loss = 3.45507 Val Loss = 3.27928
2023-04-06 21:12:10.104965 Epoch 36  	Train Loss = 3.45479 Val Loss = 3.27858
2023-04-06 21:13:50.305846 Epoch 37  	Train Loss = 3.45483 Val Loss = 3.27818
2023-04-06 21:15:30.653378 Epoch 38  	Train Loss = 3.45410 Val Loss = 3.27847
2023-04-06 21:17:11.235453 Epoch 39  	Train Loss = 3.45376 Val Loss = 3.28154
2023-04-06 21:18:52.346541 Epoch 40  	Train Loss = 3.45411 Val Loss = 3.28020
2023-04-06 21:20:34.160746 Epoch 41  	Train Loss = 3.45315 Val Loss = 3.27998
Early stopping at epoch: 41
Best at epoch 31:
Train Loss = 3.45759
Train RMSE = 7.08581, MAE = 3.45209, MAPE = 9.54959
Val Loss = 3.27802
Val RMSE = 6.99903, MAE = 3.32063, MAPE = 9.55828
--------- Test ---------
All Steps RMSE = 7.43413, MAE = 3.64941, MAPE = 10.44931
Step 1 RMSE = 4.68667, MAE = 2.56943, MAPE = 6.79796
Step 2 RMSE = 5.40424, MAE = 2.82139, MAPE = 7.62169
Step 3 RMSE = 5.96987, MAE = 3.03138, MAPE = 8.31822
Step 4 RMSE = 6.46397, MAE = 3.23078, MAPE = 9.01822
Step 5 RMSE = 6.89812, MAE = 3.41164, MAPE = 9.60990
Step 6 RMSE = 7.30287, MAE = 3.59772, MAPE = 10.26561
Step 7 RMSE = 7.68805, MAE = 3.77439, MAPE = 10.84877
Step 8 RMSE = 8.04736, MAE = 3.94993, MAPE = 11.43036
Step 9 RMSE = 8.31605, MAE = 4.09695, MAPE = 12.10948
Step 10 RMSE = 8.62600, MAE = 4.26029, MAPE = 12.58998
Step 11 RMSE = 8.99367, MAE = 4.45068, MAPE = 13.10849
Step 12 RMSE = 9.23977, MAE = 4.59842, MAPE = 13.67343
Inference time: 10.76 s
