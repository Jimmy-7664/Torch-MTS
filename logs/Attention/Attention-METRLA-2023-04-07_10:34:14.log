METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 2)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 2)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 2)

--------- Attention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "lr": 0.001,
    "milestones": [
        10,
        60
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "load_npz": true,
    "pass_device": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                --                        --
├─Linear: 1-1                            [64, 207, 12, 64]         128
├─Linear: 1-2                            [64, 207, 12, 64]         128
├─Sequential: 1-3                        [64, 207, 12, 64]         --
│    └─SelfAttentionLayer: 2-1           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-2               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-3              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-4               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-5          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-6               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-7              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-8               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-9          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-10              [64, 207, 12, 64]         128
│    │    └─Sequential: 3-11             [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-12              [64, 207, 12, 64]         128
├─Linear: 1-4                            [64, 207, 64, 12]         156
├─Linear: 1-5                            [64, 207, 12, 1]          65
==========================================================================================
Total params: 150,429
Trainable params: 150,429
Non-trainable params: 0
Total mult-adds (M): 9.63
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 2931.52
Params size (MB): 0.60
Estimated Total Size (MB): 2933.39
==========================================================================================

Loss: MaskedMAELoss

2023-04-07 10:34:59.904791 Epoch 1  	Train Loss = 4.64866 Val Loss = 3.93895
2023-04-07 10:35:33.356169 Epoch 2  	Train Loss = 3.81175 Val Loss = 3.40180
2023-04-07 10:36:07.076886 Epoch 3  	Train Loss = 3.66506 Val Loss = 3.38507
2023-04-07 10:36:41.182607 Epoch 4  	Train Loss = 3.58594 Val Loss = 3.37907
2023-04-07 10:37:15.608970 Epoch 5  	Train Loss = 3.58299 Val Loss = 3.35353
2023-04-07 10:37:50.238524 Epoch 6  	Train Loss = 3.55174 Val Loss = 3.33703
2023-04-07 10:38:24.949657 Epoch 7  	Train Loss = 3.54866 Val Loss = 3.35162
2023-04-07 10:38:59.713111 Epoch 8  	Train Loss = 3.54765 Val Loss = 3.36730
2023-04-07 10:39:34.496519 Epoch 9  	Train Loss = 3.53295 Val Loss = 3.33054
2023-04-07 10:40:09.281173 Epoch 10  	Train Loss = 3.52793 Val Loss = 3.34524
2023-04-07 10:40:44.071752 Epoch 11  	Train Loss = 3.48801 Val Loss = 3.29281
2023-04-07 10:41:18.897278 Epoch 12  	Train Loss = 3.48108 Val Loss = 3.28813
2023-04-07 10:41:53.766071 Epoch 13  	Train Loss = 3.47864 Val Loss = 3.28938
2023-04-07 10:42:28.739547 Epoch 14  	Train Loss = 3.47834 Val Loss = 3.28770
2023-04-07 10:43:03.791201 Epoch 15  	Train Loss = 3.47724 Val Loss = 3.28441
2023-04-07 10:43:38.960701 Epoch 16  	Train Loss = 3.47650 Val Loss = 3.28538
2023-04-07 10:44:14.170336 Epoch 17  	Train Loss = 3.47524 Val Loss = 3.28759
2023-04-07 10:44:49.399916 Epoch 18  	Train Loss = 3.47313 Val Loss = 3.28360
2023-04-07 10:45:24.639637 Epoch 19  	Train Loss = 3.47132 Val Loss = 3.29370
2023-04-07 10:45:59.776211 Epoch 20  	Train Loss = 3.47076 Val Loss = 3.28746
2023-04-07 10:46:34.863376 Epoch 21  	Train Loss = 3.46876 Val Loss = 3.28235
2023-04-07 10:47:09.943126 Epoch 22  	Train Loss = 3.46776 Val Loss = 3.28203
2023-04-07 10:47:45.007668 Epoch 23  	Train Loss = 3.48375 Val Loss = 3.34574
2023-04-07 10:48:20.073755 Epoch 24  	Train Loss = 3.50771 Val Loss = 3.31142
2023-04-07 10:48:55.173340 Epoch 25  	Train Loss = 3.48798 Val Loss = 3.29610
2023-04-07 10:49:30.271031 Epoch 26  	Train Loss = 3.47958 Val Loss = 3.29099
2023-04-07 10:50:05.359688 Epoch 27  	Train Loss = 3.47415 Val Loss = 3.28221
2023-04-07 10:50:40.443012 Epoch 28  	Train Loss = 3.46889 Val Loss = 3.27691
2023-04-07 10:51:15.506490 Epoch 29  	Train Loss = 3.46364 Val Loss = 3.27786
2023-04-07 10:51:50.589944 Epoch 30  	Train Loss = 3.46062 Val Loss = 3.26959
2023-04-07 10:52:25.663758 Epoch 31  	Train Loss = 3.45688 Val Loss = 3.26587
2023-04-07 10:53:00.731216 Epoch 32  	Train Loss = 3.45250 Val Loss = 3.26666
2023-04-07 10:53:35.798317 Epoch 33  	Train Loss = 3.45011 Val Loss = 3.26723
2023-04-07 10:54:10.867961 Epoch 34  	Train Loss = 3.44727 Val Loss = 3.25853
2023-04-07 10:54:45.922615 Epoch 35  	Train Loss = 3.44622 Val Loss = 3.27098
2023-04-07 10:55:20.973543 Epoch 36  	Train Loss = 3.44912 Val Loss = 3.25876
2023-04-07 10:55:56.038901 Epoch 37  	Train Loss = 3.44123 Val Loss = 3.25770
2023-04-07 10:56:31.115827 Epoch 38  	Train Loss = 3.43898 Val Loss = 3.26447
2023-04-07 10:57:06.246757 Epoch 39  	Train Loss = 3.43792 Val Loss = 3.26049
2023-04-07 10:57:41.444512 Epoch 40  	Train Loss = 3.43549 Val Loss = 3.25148
2023-04-07 10:58:16.692390 Epoch 41  	Train Loss = 3.43331 Val Loss = 3.25116
2023-04-07 10:58:51.984210 Epoch 42  	Train Loss = 3.43092 Val Loss = 3.25605
2023-04-07 10:59:27.350388 Epoch 43  	Train Loss = 3.43113 Val Loss = 3.25444
2023-04-07 11:00:02.776842 Epoch 44  	Train Loss = 3.42773 Val Loss = 3.25110
2023-04-07 11:00:38.097992 Epoch 45  	Train Loss = 3.42790 Val Loss = 3.24869
2023-04-07 11:01:13.218689 Epoch 46  	Train Loss = 3.42735 Val Loss = 3.25770
2023-04-07 11:01:48.294137 Epoch 47  	Train Loss = 3.42601 Val Loss = 3.25191
2023-04-07 11:02:23.317696 Epoch 48  	Train Loss = 3.42516 Val Loss = 3.24581
2023-04-07 11:02:58.265508 Epoch 49  	Train Loss = 3.42581 Val Loss = 3.25010
2023-04-07 11:03:33.164652 Epoch 50  	Train Loss = 3.42346 Val Loss = 3.24488
2023-04-07 11:04:08.064143 Epoch 51  	Train Loss = 3.42217 Val Loss = 3.24704
2023-04-07 11:04:42.962448 Epoch 52  	Train Loss = 3.42257 Val Loss = 3.24380
2023-04-07 11:05:17.872372 Epoch 53  	Train Loss = 3.42039 Val Loss = 3.24877
2023-04-07 11:05:52.759234 Epoch 54  	Train Loss = 3.42265 Val Loss = 3.26071
2023-04-07 11:06:27.621151 Epoch 55  	Train Loss = 3.41958 Val Loss = 3.24434
2023-04-07 11:07:02.515553 Epoch 56  	Train Loss = 3.41958 Val Loss = 3.23917
2023-04-07 11:07:37.382774 Epoch 57  	Train Loss = 3.41797 Val Loss = 3.24321
2023-04-07 11:08:12.227791 Epoch 58  	Train Loss = 3.41730 Val Loss = 3.23757
2023-04-07 11:08:47.087928 Epoch 59  	Train Loss = 3.41866 Val Loss = 3.24825
2023-04-07 11:09:21.948926 Epoch 60  	Train Loss = 3.41565 Val Loss = 3.24676
2023-04-07 11:09:56.806847 Epoch 61  	Train Loss = 3.40971 Val Loss = 3.23663
2023-04-07 11:10:31.649078 Epoch 62  	Train Loss = 3.40769 Val Loss = 3.23626
2023-04-07 11:11:06.480879 Epoch 63  	Train Loss = 3.40817 Val Loss = 3.23769
2023-04-07 11:11:41.387389 Epoch 64  	Train Loss = 3.40750 Val Loss = 3.23682
2023-04-07 11:12:16.386052 Epoch 65  	Train Loss = 3.40693 Val Loss = 3.23771
2023-04-07 11:12:51.507392 Epoch 66  	Train Loss = 3.40752 Val Loss = 3.23816
2023-04-07 11:13:26.672349 Epoch 67  	Train Loss = 3.40736 Val Loss = 3.23550
2023-04-07 11:14:01.916442 Epoch 68  	Train Loss = 3.40743 Val Loss = 3.23636
2023-04-07 11:14:37.202592 Epoch 69  	Train Loss = 3.40713 Val Loss = 3.23697
2023-04-07 11:15:12.348034 Epoch 70  	Train Loss = 3.40709 Val Loss = 3.23668
2023-04-07 11:15:47.279955 Epoch 71  	Train Loss = 3.40640 Val Loss = 3.23613
2023-04-07 11:16:22.143835 Epoch 72  	Train Loss = 3.40758 Val Loss = 3.23756
2023-04-07 11:16:57.083471 Epoch 73  	Train Loss = 3.40692 Val Loss = 3.23724
2023-04-07 11:17:32.059155 Epoch 74  	Train Loss = 3.40636 Val Loss = 3.23572
2023-04-07 11:18:07.037082 Epoch 75  	Train Loss = 3.40681 Val Loss = 3.23636
2023-04-07 11:18:42.028303 Epoch 76  	Train Loss = 3.40629 Val Loss = 3.23579
2023-04-07 11:19:16.846911 Epoch 77  	Train Loss = 3.40588 Val Loss = 3.23812
Early stopping at epoch: 77
Best at epoch 67:
Train Loss = 3.40736
Train RMSE = 6.96714, MAE = 3.40568, MAPE = 9.46944
Val Loss = 3.23550
Val RMSE = 6.88292, MAE = 3.27619, MAPE = 9.50683
--------- Test ---------
All Steps RMSE = 7.30168, MAE = 3.59305, MAPE = 10.36872
Step 1 RMSE = 4.30499, MAE = 2.41158, MAPE = 5.99965
Step 2 RMSE = 5.22961, MAE = 2.73724, MAPE = 7.08927
Step 3 RMSE = 5.91542, MAE = 2.99872, MAPE = 8.03170
Step 4 RMSE = 6.41474, MAE = 3.21284, MAPE = 8.84233
Step 5 RMSE = 6.86862, MAE = 3.40893, MAPE = 9.57716
Step 6 RMSE = 7.24149, MAE = 3.59539, MAPE = 10.29371
Step 7 RMSE = 7.60561, MAE = 3.76189, MAPE = 11.00281
Step 8 RMSE = 7.92144, MAE = 3.91756, MAPE = 11.60670
Step 9 RMSE = 8.20976, MAE = 4.06289, MAPE = 12.21304
Step 10 RMSE = 8.49350, MAE = 4.20164, MAPE = 12.77927
Step 11 RMSE = 8.75373, MAE = 4.33529, MAPE = 13.25680
Step 12 RMSE = 9.02090, MAE = 4.47279, MAPE = 13.73287
Inference time: 3.25 s
