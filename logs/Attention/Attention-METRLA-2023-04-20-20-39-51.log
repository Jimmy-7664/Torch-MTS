METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 1)

--------- Attention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        60
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 3,
        "with_spatial": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                [64, 12, 207, 1]          --
├─Linear: 1-1                            [64, 12, 207, 64]         128
├─Linear: 1-2                            [64, 12, 207, 64]         128
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 64]         16,640
│    │    └─Dropout: 3-2                 [64, 207, 12, 64]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-4              [64, 207, 12, 64]         16,576
│    │    └─Dropout: 3-5                 [64, 207, 12, 64]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 64]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 64]         16,640
│    │    └─Dropout: 3-8                 [64, 207, 12, 64]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-10             [64, 207, 12, 64]         16,576
│    │    └─Dropout: 3-11                [64, 207, 12, 64]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 64]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 64]         16,640
│    │    └─Dropout: 3-14                [64, 207, 12, 64]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 64]         128
│    │    └─Sequential: 3-16             [64, 207, 12, 64]         16,576
│    │    └─Dropout: 3-17                [64, 207, 12, 64]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 64]         128
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 64]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 64]         16,640
│    │    └─Dropout: 3-20                [64, 12, 207, 64]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 64]         128
│    │    └─Sequential: 3-22             [64, 12, 207, 64]         16,576
│    │    └─Dropout: 3-23                [64, 12, 207, 64]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 64]         128
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 64]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 64]         16,640
│    │    └─Dropout: 3-26                [64, 12, 207, 64]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 64]         128
│    │    └─Sequential: 3-28             [64, 12, 207, 64]         16,576
│    │    └─Dropout: 3-29                [64, 12, 207, 64]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 64]         128
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 64]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 64]         16,640
│    │    └─Dropout: 3-32                [64, 12, 207, 64]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 64]         128
│    │    └─Sequential: 3-34             [64, 12, 207, 64]         16,576
│    │    └─Dropout: 3-35                [64, 12, 207, 64]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 64]         128
├─Linear: 1-5                            [64, 64, 207, 12]         156
├─Linear: 1-6                            [64, 12, 207, 1]          65
==========================================================================================
Total params: 201,309
Trainable params: 201,309
Non-trainable params: 0
Total mult-adds (M): 12.88
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 4640.83
Params size (MB): 0.81
Estimated Total Size (MB): 4642.90
==========================================================================================

Loss: MaskedMAELoss

2023-04-20 20:41:13.959199 Epoch 1  	Train Loss = 4.66757 Val Loss = 4.00912
2023-04-20 20:42:36.016840 Epoch 2  	Train Loss = 3.98216 Val Loss = 3.66379
2023-04-20 20:44:04.063874 Epoch 3  	Train Loss = 3.62438 Val Loss = 3.37654
2023-04-20 20:45:31.259389 Epoch 4  	Train Loss = 3.56027 Val Loss = 3.33196
2023-04-20 20:46:56.074268 Epoch 5  	Train Loss = 3.59234 Val Loss = 3.34424
2023-04-20 20:48:20.775786 Epoch 6  	Train Loss = 3.52129 Val Loss = 3.29868
2023-04-20 20:49:45.585457 Epoch 7  	Train Loss = 3.48648 Val Loss = 3.24719
2023-04-20 20:51:10.002067 Epoch 8  	Train Loss = 3.46410 Val Loss = 3.25153
2023-04-20 20:52:34.026746 Epoch 9  	Train Loss = 3.44691 Val Loss = 3.22381
2023-04-20 20:53:57.873252 Epoch 10  	Train Loss = 3.42961 Val Loss = 3.23452
2023-04-20 20:55:22.075792 Epoch 11  	Train Loss = 3.38919 Val Loss = 3.19307
2023-04-20 20:56:48.696999 Epoch 12  	Train Loss = 3.38252 Val Loss = 3.19272
2023-04-20 20:58:17.292681 Epoch 13  	Train Loss = 3.38087 Val Loss = 3.19034
2023-04-20 20:59:45.262012 Epoch 14  	Train Loss = 3.37801 Val Loss = 3.19248
2023-04-20 21:01:10.152090 Epoch 15  	Train Loss = 3.37601 Val Loss = 3.19407
2023-04-20 21:02:34.185324 Epoch 16  	Train Loss = 3.37353 Val Loss = 3.18488
2023-04-20 21:03:58.060676 Epoch 17  	Train Loss = 3.37208 Val Loss = 3.18521
2023-04-20 21:05:21.628479 Epoch 18  	Train Loss = 3.37003 Val Loss = 3.18297
2023-04-20 21:06:45.170426 Epoch 19  	Train Loss = 3.36819 Val Loss = 3.19591
2023-04-20 21:08:08.806995 Epoch 20  	Train Loss = 3.36545 Val Loss = 3.18199
2023-04-20 21:09:33.007619 Epoch 21  	Train Loss = 3.36328 Val Loss = 3.18599
2023-04-20 21:10:59.260761 Epoch 22  	Train Loss = 3.36278 Val Loss = 3.18412
2023-04-20 21:12:27.551171 Epoch 23  	Train Loss = 3.35915 Val Loss = 3.17710
2023-04-20 21:13:55.887515 Epoch 24  	Train Loss = 3.35796 Val Loss = 3.17703
2023-04-20 21:15:21.356638 Epoch 25  	Train Loss = 3.35610 Val Loss = 3.18010
2023-04-20 21:16:45.848387 Epoch 26  	Train Loss = 3.35368 Val Loss = 3.17659
2023-04-20 21:18:10.207612 Epoch 27  	Train Loss = 3.35235 Val Loss = 3.17578
2023-04-20 21:19:34.376470 Epoch 28  	Train Loss = 3.35135 Val Loss = 3.17626
2023-04-20 21:20:58.362542 Epoch 29  	Train Loss = 3.34964 Val Loss = 3.17587
2023-04-20 21:22:22.229488 Epoch 30  	Train Loss = 3.34834 Val Loss = 3.17351
2023-04-20 21:23:46.424543 Epoch 31  	Train Loss = 3.34673 Val Loss = 3.17207
2023-04-20 21:25:12.419211 Epoch 32  	Train Loss = 3.34512 Val Loss = 3.17126
2023-04-20 21:26:40.615774 Epoch 33  	Train Loss = 3.34322 Val Loss = 3.17585
2023-04-20 21:28:09.281636 Epoch 34  	Train Loss = 3.34171 Val Loss = 3.16986
2023-04-20 21:29:35.249541 Epoch 35  	Train Loss = 3.34063 Val Loss = 3.16922
2023-04-20 21:31:00.007597 Epoch 36  	Train Loss = 3.33884 Val Loss = 3.16635
2023-04-20 21:32:24.505202 Epoch 37  	Train Loss = 3.33930 Val Loss = 3.16969
2023-04-20 21:33:48.767461 Epoch 38  	Train Loss = 3.33667 Val Loss = 3.16885
2023-04-20 21:35:12.803782 Epoch 39  	Train Loss = 3.33561 Val Loss = 3.17723
2023-04-20 21:36:36.497677 Epoch 40  	Train Loss = 3.33375 Val Loss = 3.17096
2023-04-20 21:38:00.565291 Epoch 41  	Train Loss = 3.33395 Val Loss = 3.16814
2023-04-20 21:39:26.181364 Epoch 42  	Train Loss = 3.33144 Val Loss = 3.17690
2023-04-20 21:40:54.226111 Epoch 43  	Train Loss = 3.33122 Val Loss = 3.16553
2023-04-20 21:42:22.810373 Epoch 44  	Train Loss = 3.32939 Val Loss = 3.16154
2023-04-20 21:43:48.489859 Epoch 45  	Train Loss = 3.32827 Val Loss = 3.16882
2023-04-20 21:45:12.012544 Epoch 46  	Train Loss = 3.32830 Val Loss = 3.16869
2023-04-20 21:46:35.225927 Epoch 47  	Train Loss = 3.32615 Val Loss = 3.17149
2023-04-20 21:47:57.972605 Epoch 48  	Train Loss = 3.32638 Val Loss = 3.17084
2023-04-20 21:49:20.505605 Epoch 49  	Train Loss = 3.32310 Val Loss = 3.16833
2023-04-20 21:50:42.738778 Epoch 50  	Train Loss = 3.32276 Val Loss = 3.16379
2023-04-20 21:52:05.230339 Epoch 51  	Train Loss = 3.32187 Val Loss = 3.16748
2023-04-20 21:53:28.362340 Epoch 52  	Train Loss = 3.32043 Val Loss = 3.16754
2023-04-20 21:54:54.459779 Epoch 53  	Train Loss = 3.32081 Val Loss = 3.17403
2023-04-20 21:56:22.486152 Epoch 54  	Train Loss = 3.31957 Val Loss = 3.16578
Early stopping at epoch: 54
Best at epoch 44:
Train Loss = 3.32939
Train RMSE = 6.78551, MAE = 3.31685, MAPE = 9.19184
Val Loss = 3.16154
Val RMSE = 6.71403, MAE = 3.20201, MAPE = 9.24428
--------- Test ---------
All Steps RMSE = 7.18878, MAE = 3.54467, MAPE = 10.18591
Step 1 RMSE = 4.28650, MAE = 2.40628, MAPE = 6.02661
Step 2 RMSE = 5.18611, MAE = 2.72302, MAPE = 7.07274
Step 3 RMSE = 5.85821, MAE = 2.98266, MAPE = 8.03266
Step 4 RMSE = 6.34031, MAE = 3.18659, MAPE = 8.76737
Step 5 RMSE = 6.78163, MAE = 3.37594, MAPE = 9.49526
Step 6 RMSE = 7.12764, MAE = 3.55640, MAPE = 10.19471
Step 7 RMSE = 7.48503, MAE = 3.70625, MAPE = 10.77370
Step 8 RMSE = 7.79499, MAE = 3.85719, MAPE = 11.40078
Step 9 RMSE = 8.07184, MAE = 3.99113, MAPE = 11.91988
Step 10 RMSE = 8.34277, MAE = 4.12107, MAPE = 12.39605
Step 11 RMSE = 8.59130, MAE = 4.24783, MAPE = 12.85051
Step 12 RMSE = 8.84358, MAE = 4.38176, MAPE = 13.30108
Inference time: 7.80 s
