PEMS03
Trainset:	x-(15711, 12, 358, 2)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 2)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 2)	y-(5237, 12, 358, 1)

--------- Attention ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        40
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 358,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "with_spatial": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                [64, 12, 358, 1]          --
├─Linear: 1-1                            [64, 12, 358, 64]         128
├─Linear: 1-2                            [64, 12, 358, 64]         128
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 358, 12, 64]         16,640
│    │    └─Dropout: 3-2                 [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-3               [64, 358, 12, 64]         128
│    │    └─Sequential: 3-4              [64, 358, 12, 64]         33,088
│    │    └─Dropout: 3-5                 [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-6               [64, 358, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-7          [64, 358, 12, 64]         16,640
│    │    └─Dropout: 3-8                 [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-9               [64, 358, 12, 64]         128
│    │    └─Sequential: 3-10             [64, 358, 12, 64]         33,088
│    │    └─Dropout: 3-11                [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-12              [64, 358, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-13         [64, 358, 12, 64]         16,640
│    │    └─Dropout: 3-14                [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-15              [64, 358, 12, 64]         128
│    │    └─Sequential: 3-16             [64, 358, 12, 64]         33,088
│    │    └─Dropout: 3-17                [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-18              [64, 358, 12, 64]         128
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 358, 64]         16,640
│    │    └─Dropout: 3-20                [64, 12, 358, 64]         --
│    │    └─LayerNorm: 3-21              [64, 12, 358, 64]         128
│    │    └─Sequential: 3-22             [64, 12, 358, 64]         33,088
│    │    └─Dropout: 3-23                [64, 12, 358, 64]         --
│    │    └─LayerNorm: 3-24              [64, 12, 358, 64]         128
│    └─SelfAttentionLayer: 2-5           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 358, 64]         16,640
│    │    └─Dropout: 3-26                [64, 12, 358, 64]         --
│    │    └─LayerNorm: 3-27              [64, 12, 358, 64]         128
│    │    └─Sequential: 3-28             [64, 12, 358, 64]         33,088
│    │    └─Dropout: 3-29                [64, 12, 358, 64]         --
│    │    └─LayerNorm: 3-30              [64, 12, 358, 64]         128
│    └─SelfAttentionLayer: 2-6           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 358, 64]         16,640
│    │    └─Dropout: 3-32                [64, 12, 358, 64]         --
│    │    └─LayerNorm: 3-33              [64, 12, 358, 64]         128
│    │    └─Sequential: 3-34             [64, 12, 358, 64]         33,088
│    │    └─Dropout: 3-35                [64, 12, 358, 64]         --
│    │    └─LayerNorm: 3-36              [64, 12, 358, 64]         128
├─Linear: 1-5                            [64, 64, 358, 12]         156
├─Linear: 1-6                            [64, 12, 358, 1]          65
==========================================================================================
Total params: 300,381
Trainable params: 300,381
Non-trainable params: 0
Total mult-adds (M): 19.22
==========================================================================================
Input size (MB): 2.20
Forward/backward pass size (MB): 9715.42
Params size (MB): 1.20
Estimated Total Size (MB): 9718.82
==========================================================================================

Loss: HuberLoss

2023-05-06 19:33:30.565930 Epoch 1  	Train Loss = 29.75673 Val Loss = 20.63612
2023-05-06 19:34:57.679678 Epoch 2  	Train Loss = 19.79641 Val Loss = 18.99350
2023-05-06 19:36:24.956220 Epoch 3  	Train Loss = 19.02975 Val Loss = 19.00009
2023-05-06 19:37:52.167483 Epoch 4  	Train Loss = 18.54505 Val Loss = 18.92450
2023-05-06 19:39:19.349432 Epoch 5  	Train Loss = 18.24493 Val Loss = 17.92127
2023-05-06 19:40:46.456928 Epoch 6  	Train Loss = 17.84076 Val Loss = 17.49104
2023-05-06 19:42:13.544432 Epoch 7  	Train Loss = 17.49957 Val Loss = 17.41646
2023-05-06 19:43:40.696085 Epoch 8  	Train Loss = 17.33639 Val Loss = 16.85154
2023-05-06 19:45:07.844414 Epoch 9  	Train Loss = 17.11449 Val Loss = 17.05067
2023-05-06 19:46:35.041099 Epoch 10  	Train Loss = 17.12423 Val Loss = 17.09200
2023-05-06 19:48:02.271619 Epoch 11  	Train Loss = 16.60314 Val Loss = 16.62589
2023-05-06 19:49:29.578121 Epoch 12  	Train Loss = 16.55614 Val Loss = 16.59342
2023-05-06 19:50:56.839020 Epoch 13  	Train Loss = 16.53031 Val Loss = 16.57168
2023-05-06 19:52:24.036144 Epoch 14  	Train Loss = 16.51029 Val Loss = 16.56062
2023-05-06 19:53:51.147142 Epoch 15  	Train Loss = 16.49526 Val Loss = 16.50028
2023-05-06 19:55:18.298515 Epoch 16  	Train Loss = 16.47005 Val Loss = 16.55917
2023-05-06 19:56:45.444821 Epoch 17  	Train Loss = 16.45322 Val Loss = 16.52947
2023-05-06 19:58:12.595599 Epoch 18  	Train Loss = 16.43582 Val Loss = 16.48086
2023-05-06 19:59:39.752007 Epoch 19  	Train Loss = 16.42632 Val Loss = 16.55545
2023-05-06 20:01:06.916126 Epoch 20  	Train Loss = 16.40685 Val Loss = 16.49338
2023-05-06 20:02:34.179092 Epoch 21  	Train Loss = 16.38841 Val Loss = 16.40760
2023-05-06 20:04:01.419162 Epoch 22  	Train Loss = 16.37617 Val Loss = 16.49922
2023-05-06 20:05:28.728339 Epoch 23  	Train Loss = 16.36061 Val Loss = 16.37798
2023-05-06 20:06:55.989185 Epoch 24  	Train Loss = 16.34961 Val Loss = 16.41651
2023-05-06 20:08:23.181591 Epoch 25  	Train Loss = 16.33286 Val Loss = 16.37691
2023-05-06 20:09:50.303691 Epoch 26  	Train Loss = 16.32113 Val Loss = 16.36452
2023-05-06 20:11:17.460106 Epoch 27  	Train Loss = 16.30359 Val Loss = 16.35680
2023-05-06 20:12:44.648903 Epoch 28  	Train Loss = 16.28922 Val Loss = 16.41589
2023-05-06 20:14:11.782168 Epoch 29  	Train Loss = 16.28262 Val Loss = 16.30579
2023-05-06 20:15:38.926977 Epoch 30  	Train Loss = 16.28224 Val Loss = 16.32710
2023-05-06 20:17:06.116542 Epoch 31  	Train Loss = 16.25222 Val Loss = 16.37003
2023-05-06 20:18:33.373035 Epoch 32  	Train Loss = 16.25971 Val Loss = 16.35536
2023-05-06 20:20:00.666513 Epoch 33  	Train Loss = 16.22993 Val Loss = 16.29461
2023-05-06 20:21:27.863830 Epoch 34  	Train Loss = 16.22530 Val Loss = 16.29299
2023-05-06 20:22:54.993868 Epoch 35  	Train Loss = 16.19905 Val Loss = 16.33413
2023-05-06 20:24:22.152182 Epoch 36  	Train Loss = 16.18962 Val Loss = 16.31368
2023-05-06 20:25:49.303407 Epoch 37  	Train Loss = 16.19056 Val Loss = 16.40653
2023-05-06 20:27:16.497714 Epoch 38  	Train Loss = 16.17895 Val Loss = 16.26794
2023-05-06 20:28:43.605827 Epoch 39  	Train Loss = 16.16371 Val Loss = 16.37140
2023-05-06 20:30:10.784269 Epoch 40  	Train Loss = 16.17450 Val Loss = 16.30011
2023-05-06 20:31:37.937351 Epoch 41  	Train Loss = 16.08117 Val Loss = 16.20714
2023-05-06 20:33:05.162338 Epoch 42  	Train Loss = 16.07929 Val Loss = 16.21395
2023-05-06 20:34:32.454132 Epoch 43  	Train Loss = 16.07671 Val Loss = 16.21946
2023-05-06 20:35:59.680435 Epoch 44  	Train Loss = 16.07399 Val Loss = 16.20540
2023-05-06 20:37:26.845067 Epoch 45  	Train Loss = 16.07447 Val Loss = 16.21444
2023-05-06 20:38:53.941515 Epoch 46  	Train Loss = 16.07148 Val Loss = 16.21320
2023-05-06 20:40:21.019201 Epoch 47  	Train Loss = 16.07278 Val Loss = 16.22652
2023-05-06 20:41:48.105354 Epoch 48  	Train Loss = 16.06334 Val Loss = 16.22066
2023-05-06 20:43:15.257469 Epoch 49  	Train Loss = 16.06723 Val Loss = 16.20647
2023-05-06 20:44:42.470127 Epoch 50  	Train Loss = 16.06640 Val Loss = 16.21352
2023-05-06 20:46:09.701860 Epoch 51  	Train Loss = 16.06595 Val Loss = 16.19982
2023-05-06 20:47:36.927898 Epoch 52  	Train Loss = 16.06165 Val Loss = 16.23809
2023-05-06 20:49:04.238340 Epoch 53  	Train Loss = 16.05943 Val Loss = 16.19487
2023-05-06 20:50:31.619739 Epoch 54  	Train Loss = 16.06229 Val Loss = 16.22496
2023-05-06 20:51:58.809998 Epoch 55  	Train Loss = 16.05920 Val Loss = 16.20072
2023-05-06 20:53:25.945164 Epoch 56  	Train Loss = 16.05457 Val Loss = 16.22678
2023-05-06 20:54:53.080082 Epoch 57  	Train Loss = 16.05339 Val Loss = 16.18962
2023-05-06 20:56:20.219677 Epoch 58  	Train Loss = 16.05614 Val Loss = 16.19933
2023-05-06 20:57:47.306027 Epoch 59  	Train Loss = 16.05409 Val Loss = 16.18992
2023-05-06 20:59:14.463341 Epoch 60  	Train Loss = 16.04796 Val Loss = 16.18969
2023-05-06 21:00:41.589785 Epoch 61  	Train Loss = 16.04759 Val Loss = 16.18231
2023-05-06 21:02:08.721845 Epoch 62  	Train Loss = 16.04909 Val Loss = 16.19379
2023-05-06 21:03:35.886755 Epoch 63  	Train Loss = 16.04597 Val Loss = 16.18153
2023-05-06 21:05:03.019407 Epoch 64  	Train Loss = 16.04174 Val Loss = 16.19963
2023-05-06 21:06:30.186142 Epoch 65  	Train Loss = 16.04113 Val Loss = 16.20924
2023-05-06 21:07:57.260960 Epoch 66  	Train Loss = 16.04356 Val Loss = 16.18230
2023-05-06 21:09:24.398574 Epoch 67  	Train Loss = 16.03947 Val Loss = 16.19211
2023-05-06 21:10:51.533393 Epoch 68  	Train Loss = 16.03936 Val Loss = 16.19742
2023-05-06 21:12:18.613628 Epoch 69  	Train Loss = 16.03560 Val Loss = 16.19113
2023-05-06 21:13:45.776658 Epoch 70  	Train Loss = 16.03644 Val Loss = 16.21436
2023-05-06 21:15:12.884351 Epoch 71  	Train Loss = 16.03317 Val Loss = 16.19441
2023-05-06 21:16:40.058877 Epoch 72  	Train Loss = 16.03563 Val Loss = 16.21450
2023-05-06 21:18:07.081760 Epoch 73  	Train Loss = 16.03433 Val Loss = 16.20065
Early stopping at epoch: 73
Best at epoch 63:
Train Loss = 16.04597
Train RMSE = 26.41953, MAE = 16.56511, MAPE = 14.68151
Val Loss = 16.18153
Val RMSE = 26.54271, MAE = 16.74765, MAPE = 14.94039
--------- Test ---------
All Steps RMSE = 28.37624, MAE = 16.59082, MAPE = 15.59498
Step 1 RMSE = 23.30377, MAE = 13.36203, MAPE = 13.11896
Step 2 RMSE = 24.42015, MAE = 14.01891, MAPE = 13.44339
Step 3 RMSE = 25.57548, MAE = 14.75473, MAPE = 13.94702
Step 4 RMSE = 26.51322, MAE = 15.36613, MAPE = 14.48426
Step 5 RMSE = 27.36553, MAE = 15.95522, MAPE = 15.01755
Step 6 RMSE = 28.15415, MAE = 16.47812, MAPE = 15.40137
Step 7 RMSE = 28.94785, MAE = 17.06933, MAPE = 15.94604
Step 8 RMSE = 29.57661, MAE = 17.47565, MAPE = 16.24190
Step 9 RMSE = 30.22587, MAE = 17.94210, MAPE = 16.68300
Step 10 RMSE = 30.82268, MAE = 18.35287, MAPE = 17.11083
Step 11 RMSE = 31.54408, MAE = 18.85009, MAPE = 17.58389
Step 12 RMSE = 32.45177, MAE = 19.46455, MAPE = 18.16181
Inference time: 8.14 s
