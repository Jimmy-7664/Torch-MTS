PEMS03
Trainset:	x-(15711, 12, 358, 2)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 2)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 2)	y-(5237, 12, 358, 1)

--------- Attention ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        40
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 358,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                [64, 12, 358, 1]          --
├─Linear: 1-1                            [64, 12, 358, 64]         128
├─Linear: 1-2                            [64, 12, 358, 64]         128
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 358, 12, 64]         16,640
│    │    └─Dropout: 3-2                 [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-3               [64, 358, 12, 64]         128
│    │    └─Sequential: 3-4              [64, 358, 12, 64]         33,088
│    │    └─Dropout: 3-5                 [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-6               [64, 358, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-7          [64, 358, 12, 64]         16,640
│    │    └─Dropout: 3-8                 [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-9               [64, 358, 12, 64]         128
│    │    └─Sequential: 3-10             [64, 358, 12, 64]         33,088
│    │    └─Dropout: 3-11                [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-12              [64, 358, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 12, 358, 64]         --
│    │    └─AttentionLayer: 3-13         [64, 358, 12, 64]         16,640
│    │    └─Dropout: 3-14                [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-15              [64, 358, 12, 64]         128
│    │    └─Sequential: 3-16             [64, 358, 12, 64]         33,088
│    │    └─Dropout: 3-17                [64, 358, 12, 64]         --
│    │    └─LayerNorm: 3-18              [64, 358, 12, 64]         128
├─Linear: 1-4                            [64, 64, 358, 12]         156
├─Linear: 1-5                            [64, 12, 358, 1]          65
==========================================================================================
Total params: 150,429
Trainable params: 150,429
Non-trainable params: 0
Total mult-adds (M): 9.63
==========================================================================================
Input size (MB): 2.20
Forward/backward pass size (MB): 5069.97
Params size (MB): 0.60
Estimated Total Size (MB): 5072.77
==========================================================================================

Loss: HuberLoss

2023-05-06 18:17:41.202108 Epoch 1  	Train Loss = 29.61862 Val Loss = 20.87447
2023-05-06 18:18:12.064788 Epoch 2  	Train Loss = 20.35013 Val Loss = 19.88678
2023-05-06 18:18:42.966576 Epoch 3  	Train Loss = 19.88328 Val Loss = 19.58176
2023-05-06 18:19:13.942271 Epoch 4  	Train Loss = 19.60417 Val Loss = 19.47973
2023-05-06 18:19:45.084630 Epoch 5  	Train Loss = 19.31355 Val Loss = 19.13574
2023-05-06 18:20:16.062974 Epoch 6  	Train Loss = 19.18333 Val Loss = 18.64777
2023-05-06 18:20:47.198356 Epoch 7  	Train Loss = 18.82307 Val Loss = 18.76558
2023-05-06 18:21:18.200518 Epoch 8  	Train Loss = 18.57588 Val Loss = 18.18460
2023-05-06 18:21:49.320926 Epoch 9  	Train Loss = 18.38745 Val Loss = 18.56592
2023-05-06 18:22:20.433765 Epoch 10  	Train Loss = 18.30887 Val Loss = 18.07304
2023-05-06 18:22:51.507274 Epoch 11  	Train Loss = 17.70247 Val Loss = 17.51887
2023-05-06 18:23:22.474367 Epoch 12  	Train Loss = 17.66502 Val Loss = 17.49381
2023-05-06 18:23:53.552076 Epoch 13  	Train Loss = 17.63909 Val Loss = 17.46321
2023-05-06 18:24:24.590279 Epoch 14  	Train Loss = 17.62921 Val Loss = 17.42731
2023-05-06 18:24:55.610160 Epoch 15  	Train Loss = 17.61272 Val Loss = 17.41494
2023-05-06 18:25:26.607809 Epoch 16  	Train Loss = 17.58632 Val Loss = 17.45825
2023-05-06 18:25:57.573552 Epoch 17  	Train Loss = 17.57851 Val Loss = 17.40924
2023-05-06 18:26:28.532442 Epoch 18  	Train Loss = 17.55635 Val Loss = 17.38705
2023-05-06 18:26:59.479726 Epoch 19  	Train Loss = 17.55375 Val Loss = 17.35278
2023-05-06 18:27:30.424108 Epoch 20  	Train Loss = 17.53294 Val Loss = 17.46741
2023-05-06 18:28:01.432570 Epoch 21  	Train Loss = 17.53419 Val Loss = 17.33147
2023-05-06 18:28:32.470409 Epoch 22  	Train Loss = 17.51420 Val Loss = 17.42867
2023-05-06 18:29:03.499913 Epoch 23  	Train Loss = 17.50508 Val Loss = 17.34039
2023-05-06 18:29:34.496698 Epoch 24  	Train Loss = 17.49630 Val Loss = 17.29632
2023-05-06 18:30:05.490299 Epoch 25  	Train Loss = 17.48444 Val Loss = 17.31761
2023-05-06 18:30:36.442150 Epoch 26  	Train Loss = 17.47142 Val Loss = 17.30634
2023-05-06 18:31:07.567969 Epoch 27  	Train Loss = 17.45641 Val Loss = 17.25587
2023-05-06 18:31:38.533789 Epoch 28  	Train Loss = 17.43814 Val Loss = 17.26386
2023-05-06 18:32:09.462771 Epoch 29  	Train Loss = 17.43944 Val Loss = 17.30009
2023-05-06 18:32:40.428613 Epoch 30  	Train Loss = 17.43862 Val Loss = 17.35837
2023-05-06 18:33:11.404829 Epoch 31  	Train Loss = 17.43158 Val Loss = 17.25602
2023-05-06 18:33:42.331898 Epoch 32  	Train Loss = 17.42230 Val Loss = 17.26686
2023-05-06 18:34:13.349624 Epoch 33  	Train Loss = 17.39944 Val Loss = 17.30812
2023-05-06 18:34:44.298474 Epoch 34  	Train Loss = 17.40743 Val Loss = 17.29829
2023-05-06 18:35:15.228870 Epoch 35  	Train Loss = 17.39255 Val Loss = 17.25739
2023-05-06 18:35:46.284260 Epoch 36  	Train Loss = 17.37009 Val Loss = 17.21299
2023-05-06 18:36:17.277928 Epoch 37  	Train Loss = 17.37883 Val Loss = 17.18122
2023-05-06 18:36:48.270333 Epoch 38  	Train Loss = 17.37368 Val Loss = 17.21216
2023-05-06 18:37:19.294207 Epoch 39  	Train Loss = 17.37520 Val Loss = 17.18820
2023-05-06 18:37:50.306822 Epoch 40  	Train Loss = 17.34327 Val Loss = 17.27105
2023-05-06 18:38:21.318106 Epoch 41  	Train Loss = 17.27542 Val Loss = 17.12732
2023-05-06 18:38:52.352604 Epoch 42  	Train Loss = 17.26629 Val Loss = 17.12293
2023-05-06 18:39:23.417889 Epoch 43  	Train Loss = 17.26462 Val Loss = 17.12625
2023-05-06 18:39:54.450811 Epoch 44  	Train Loss = 17.26286 Val Loss = 17.13296
2023-05-06 18:40:25.489044 Epoch 45  	Train Loss = 17.26378 Val Loss = 17.12013
2023-05-06 18:40:56.451142 Epoch 46  	Train Loss = 17.26313 Val Loss = 17.11216
2023-05-06 18:41:27.499275 Epoch 47  	Train Loss = 17.26018 Val Loss = 17.12546
2023-05-06 18:41:58.542659 Epoch 48  	Train Loss = 17.25666 Val Loss = 17.11362
2023-05-06 18:42:29.493936 Epoch 49  	Train Loss = 17.25423 Val Loss = 17.12067
2023-05-06 18:43:00.499386 Epoch 50  	Train Loss = 17.25498 Val Loss = 17.12257
2023-05-06 18:43:31.532831 Epoch 51  	Train Loss = 17.25744 Val Loss = 17.12117
2023-05-06 18:44:02.683210 Epoch 52  	Train Loss = 17.26038 Val Loss = 17.11795
2023-05-06 18:44:33.652378 Epoch 53  	Train Loss = 17.25107 Val Loss = 17.11037
2023-05-06 18:45:04.621761 Epoch 54  	Train Loss = 17.24780 Val Loss = 17.10686
2023-05-06 18:45:35.649920 Epoch 55  	Train Loss = 17.25445 Val Loss = 17.10967
2023-05-06 18:46:06.571394 Epoch 56  	Train Loss = 17.25097 Val Loss = 17.11479
2023-05-06 18:46:37.543262 Epoch 57  	Train Loss = 17.24926 Val Loss = 17.11262
2023-05-06 18:47:08.553428 Epoch 58  	Train Loss = 17.24902 Val Loss = 17.10483
2023-05-06 18:47:39.470413 Epoch 59  	Train Loss = 17.24108 Val Loss = 17.12799
2023-05-06 18:48:10.402946 Epoch 60  	Train Loss = 17.24459 Val Loss = 17.10031
2023-05-06 18:48:41.374739 Epoch 61  	Train Loss = 17.23946 Val Loss = 17.11284
2023-05-06 18:49:12.383346 Epoch 62  	Train Loss = 17.24019 Val Loss = 17.10246
2023-05-06 18:49:43.355383 Epoch 63  	Train Loss = 17.24176 Val Loss = 17.12341
2023-05-06 18:50:14.442690 Epoch 64  	Train Loss = 17.24159 Val Loss = 17.10404
2023-05-06 18:50:45.481087 Epoch 65  	Train Loss = 17.23901 Val Loss = 17.09206
2023-05-06 18:51:16.458023 Epoch 66  	Train Loss = 17.23376 Val Loss = 17.09798
2023-05-06 18:51:47.485146 Epoch 67  	Train Loss = 17.23660 Val Loss = 17.11104
2023-05-06 18:52:18.454044 Epoch 68  	Train Loss = 17.23830 Val Loss = 17.09777
2023-05-06 18:52:49.564088 Epoch 69  	Train Loss = 17.23697 Val Loss = 17.10590
2023-05-06 18:53:20.567703 Epoch 70  	Train Loss = 17.22848 Val Loss = 17.10244
2023-05-06 18:53:51.675536 Epoch 71  	Train Loss = 17.23096 Val Loss = 17.09903
2023-05-06 18:54:22.561860 Epoch 72  	Train Loss = 17.23788 Val Loss = 17.10095
2023-05-06 18:54:53.552313 Epoch 73  	Train Loss = 17.22758 Val Loss = 17.08882
2023-05-06 18:55:24.573646 Epoch 74  	Train Loss = 17.23118 Val Loss = 17.09498
2023-05-06 18:55:55.589442 Epoch 75  	Train Loss = 17.23262 Val Loss = 17.10220
2023-05-06 18:56:26.440821 Epoch 76  	Train Loss = 17.22609 Val Loss = 17.09473
2023-05-06 18:56:57.375928 Epoch 77  	Train Loss = 17.22697 Val Loss = 17.09624
2023-05-06 18:57:28.251524 Epoch 78  	Train Loss = 17.22499 Val Loss = 17.08736
2023-05-06 18:57:59.124669 Epoch 79  	Train Loss = 17.22115 Val Loss = 17.08447
2023-05-06 18:58:30.034040 Epoch 80  	Train Loss = 17.21506 Val Loss = 17.09481
2023-05-06 18:59:00.979457 Epoch 81  	Train Loss = 17.21880 Val Loss = 17.09702
2023-05-06 18:59:31.886271 Epoch 82  	Train Loss = 17.22060 Val Loss = 17.09834
2023-05-06 19:00:02.821243 Epoch 83  	Train Loss = 17.21824 Val Loss = 17.09232
2023-05-06 19:00:33.760057 Epoch 84  	Train Loss = 17.22016 Val Loss = 17.09596
2023-05-06 19:01:04.618811 Epoch 85  	Train Loss = 17.21761 Val Loss = 17.08924
2023-05-06 19:01:35.477790 Epoch 86  	Train Loss = 17.21541 Val Loss = 17.07841
2023-05-06 19:02:06.354014 Epoch 87  	Train Loss = 17.21323 Val Loss = 17.08409
2023-05-06 19:02:37.315720 Epoch 88  	Train Loss = 17.21484 Val Loss = 17.08364
2023-05-06 19:03:08.266453 Epoch 89  	Train Loss = 17.21067 Val Loss = 17.07604
2023-05-06 19:03:39.204846 Epoch 90  	Train Loss = 17.21699 Val Loss = 17.08852
2023-05-06 19:04:10.079335 Epoch 91  	Train Loss = 17.21278 Val Loss = 17.08278
2023-05-06 19:04:40.958114 Epoch 92  	Train Loss = 17.21428 Val Loss = 17.08919
2023-05-06 19:05:11.903307 Epoch 93  	Train Loss = 17.20765 Val Loss = 17.08199
2023-05-06 19:05:42.864226 Epoch 94  	Train Loss = 17.20503 Val Loss = 17.07678
2023-05-06 19:06:13.860150 Epoch 95  	Train Loss = 17.20690 Val Loss = 17.08424
2023-05-06 19:06:44.818624 Epoch 96  	Train Loss = 17.19830 Val Loss = 17.06878
2023-05-06 19:07:15.775515 Epoch 97  	Train Loss = 17.20099 Val Loss = 17.09015
2023-05-06 19:07:46.751542 Epoch 98  	Train Loss = 17.20476 Val Loss = 17.06675
2023-05-06 19:08:17.741124 Epoch 99  	Train Loss = 17.20652 Val Loss = 17.07767
2023-05-06 19:08:48.678673 Epoch 100  	Train Loss = 17.20409 Val Loss = 17.07582
2023-05-06 19:09:19.556391 Epoch 101  	Train Loss = 17.20316 Val Loss = 17.08558
2023-05-06 19:09:50.424523 Epoch 102  	Train Loss = 17.19890 Val Loss = 17.06533
2023-05-06 19:10:21.296914 Epoch 103  	Train Loss = 17.20076 Val Loss = 17.08459
2023-05-06 19:10:52.368598 Epoch 104  	Train Loss = 17.20047 Val Loss = 17.06844
2023-05-06 19:11:23.348085 Epoch 105  	Train Loss = 17.19808 Val Loss = 17.06414
2023-05-06 19:11:54.205015 Epoch 106  	Train Loss = 17.20175 Val Loss = 17.07557
2023-05-06 19:12:25.101802 Epoch 107  	Train Loss = 17.19346 Val Loss = 17.05965
2023-05-06 19:12:56.029133 Epoch 108  	Train Loss = 17.19818 Val Loss = 17.06102
2023-05-06 19:13:26.976766 Epoch 109  	Train Loss = 17.19640 Val Loss = 17.06977
2023-05-06 19:13:57.916532 Epoch 110  	Train Loss = 17.19677 Val Loss = 17.06058
2023-05-06 19:14:28.860667 Epoch 111  	Train Loss = 17.18984 Val Loss = 17.06583
2023-05-06 19:14:59.808866 Epoch 112  	Train Loss = 17.19409 Val Loss = 17.06804
2023-05-06 19:15:30.737575 Epoch 113  	Train Loss = 17.19213 Val Loss = 17.06192
2023-05-06 19:16:01.591755 Epoch 114  	Train Loss = 17.19343 Val Loss = 17.07032
2023-05-06 19:16:32.495681 Epoch 115  	Train Loss = 17.19507 Val Loss = 17.07503
2023-05-06 19:17:03.446225 Epoch 116  	Train Loss = 17.18626 Val Loss = 17.05170
2023-05-06 19:17:34.298854 Epoch 117  	Train Loss = 17.18853 Val Loss = 17.06899
2023-05-06 19:18:05.174240 Epoch 118  	Train Loss = 17.18968 Val Loss = 17.06602
2023-05-06 19:18:36.047900 Epoch 119  	Train Loss = 17.18464 Val Loss = 17.05722
2023-05-06 19:19:07.021690 Epoch 120  	Train Loss = 17.18325 Val Loss = 17.05447
2023-05-06 19:19:38.168250 Epoch 121  	Train Loss = 17.18873 Val Loss = 17.05870
2023-05-06 19:20:09.161603 Epoch 122  	Train Loss = 17.18306 Val Loss = 17.06557
2023-05-06 19:20:40.161953 Epoch 123  	Train Loss = 17.18921 Val Loss = 17.06301
2023-05-06 19:21:11.143409 Epoch 124  	Train Loss = 17.18119 Val Loss = 17.05381
2023-05-06 19:21:42.042313 Epoch 125  	Train Loss = 17.17948 Val Loss = 17.05585
2023-05-06 19:22:13.074831 Epoch 126  	Train Loss = 17.18586 Val Loss = 17.05386
Early stopping at epoch: 126
Best at epoch 116:
Train Loss = 17.18626
Train RMSE = 28.04616, MAE = 17.73040, MAPE = 15.68608
Val Loss = 17.05170
Val RMSE = 27.74047, MAE = 17.61277, MAPE = 15.64884
--------- Test ---------
All Steps RMSE = 29.06286, MAE = 17.26111, MAPE = 15.99383
Step 1 RMSE = 23.01622, MAE = 13.37824, MAPE = 12.98297
Step 2 RMSE = 24.44696, MAE = 14.29017, MAPE = 13.68070
Step 3 RMSE = 25.83532, MAE = 15.16946, MAPE = 14.26439
Step 4 RMSE = 26.89090, MAE = 15.87809, MAPE = 14.85003
Step 5 RMSE = 27.82593, MAE = 16.49515, MAPE = 15.29465
Step 6 RMSE = 28.80259, MAE = 17.14017, MAPE = 15.83663
Step 7 RMSE = 29.62015, MAE = 17.71342, MAPE = 16.26659
Step 8 RMSE = 30.49036, MAE = 18.30290, MAPE = 16.72481
Step 9 RMSE = 31.13427, MAE = 18.79412, MAPE = 17.21892
Step 10 RMSE = 31.83261, MAE = 19.29438, MAPE = 17.71738
Step 11 RMSE = 32.74940, MAE = 19.93752, MAPE = 18.24046
Step 12 RMSE = 33.93728, MAE = 20.73981, MAPE = 18.84830
Inference time: 2.73 s
