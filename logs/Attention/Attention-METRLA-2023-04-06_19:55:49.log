METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 2)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 2)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 2)

--------- Attention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "lr": 0.001,
    "milestones": [
        10
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "load_npz": true,
    "pass_device": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                --                        --
├─Linear: 1-1                            [64, 207, 12, 64]         128
├─Linear: 1-2                            [64, 207, 12, 64]         128
├─Sequential: 1-3                        [64, 207, 12, 64]         --
│    └─SelfAttentionLayer: 2-1           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 64]         12,480
│    │    └─LayerNorm: 3-2               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-3              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-4               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-5          [64, 207, 12, 64]         12,480
│    │    └─LayerNorm: 3-6               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-7              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-8               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-9          [64, 207, 12, 64]         12,480
│    │    └─LayerNorm: 3-10              [64, 207, 12, 64]         128
│    │    └─Sequential: 3-11             [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-12              [64, 207, 12, 64]         128
├─Linear: 1-4                            [64, 207, 64, 12]         156
├─Linear: 1-5                            [64, 207, 12, 1]          65
==========================================================================================
Total params: 137,949
Trainable params: 137,949
Non-trainable params: 0
Total mult-adds (M): 8.83
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 2687.33
Params size (MB): 0.55
Estimated Total Size (MB): 2689.15
==========================================================================================

Loss: MaskedMAELoss

2023-04-06 19:56:34.331536 Epoch 1  	Train Loss = 4.57138 Val Loss = 3.90671
2023-04-06 19:57:07.874267 Epoch 2  	Train Loss = 3.85656 Val Loss = 3.48849
2023-04-06 19:57:41.670113 Epoch 3  	Train Loss = 3.65344 Val Loss = 3.39374
2023-04-06 19:58:15.666137 Epoch 4  	Train Loss = 3.58965 Val Loss = 3.38457
2023-04-06 19:58:49.762922 Epoch 5  	Train Loss = 3.60118 Val Loss = 3.39779
2023-04-06 19:59:23.866763 Epoch 6  	Train Loss = 3.55688 Val Loss = 3.36458
2023-04-06 19:59:58.022574 Epoch 7  	Train Loss = 3.56462 Val Loss = 3.38970
2023-04-06 20:00:32.175016 Epoch 8  	Train Loss = 3.53275 Val Loss = 3.33400
2023-04-06 20:01:06.266884 Epoch 9  	Train Loss = 3.52247 Val Loss = 3.66390
2023-04-06 20:01:40.309286 Epoch 10  	Train Loss = 3.53348 Val Loss = 3.34687
2023-04-06 20:02:14.337984 Epoch 11  	Train Loss = 3.48210 Val Loss = 3.28993
2023-04-06 20:02:48.334568 Epoch 12  	Train Loss = 3.47934 Val Loss = 3.28996
2023-04-06 20:03:22.327095 Epoch 13  	Train Loss = 3.47756 Val Loss = 3.28770
2023-04-06 20:03:56.320920 Epoch 14  	Train Loss = 3.47590 Val Loss = 3.28359
2023-04-06 20:04:30.281092 Epoch 15  	Train Loss = 3.47502 Val Loss = 3.28245
2023-04-06 20:05:04.294917 Epoch 16  	Train Loss = 3.47423 Val Loss = 3.28162
2023-04-06 20:05:38.326626 Epoch 17  	Train Loss = 3.47207 Val Loss = 3.28223
2023-04-06 20:06:12.329652 Epoch 18  	Train Loss = 3.47152 Val Loss = 3.28174
2023-04-06 20:06:46.333123 Epoch 19  	Train Loss = 3.46980 Val Loss = 3.27610
2023-04-06 20:07:20.405134 Epoch 20  	Train Loss = 3.46742 Val Loss = 3.27511
2023-04-06 20:07:54.475609 Epoch 21  	Train Loss = 3.46481 Val Loss = 3.27207
2023-04-06 20:08:28.528841 Epoch 22  	Train Loss = 3.46268 Val Loss = 3.28420
2023-04-06 20:09:02.599903 Epoch 23  	Train Loss = 3.46100 Val Loss = 3.28382
2023-04-06 20:09:36.655501 Epoch 24  	Train Loss = 3.45838 Val Loss = 3.26937
2023-04-06 20:10:10.736484 Epoch 25  	Train Loss = 3.45222 Val Loss = 3.26572
2023-04-06 20:10:44.798440 Epoch 26  	Train Loss = 3.45106 Val Loss = 3.26486
2023-04-06 20:11:18.854506 Epoch 27  	Train Loss = 3.45092 Val Loss = 3.26885
2023-04-06 20:11:52.915126 Epoch 28  	Train Loss = 3.44588 Val Loss = 3.26924
2023-04-06 20:12:26.998355 Epoch 29  	Train Loss = 3.44254 Val Loss = 3.25568
2023-04-06 20:13:01.125863 Epoch 30  	Train Loss = 3.44151 Val Loss = 3.25988
2023-04-06 20:13:35.180698 Epoch 31  	Train Loss = 3.43986 Val Loss = 3.25320
2023-04-06 20:14:09.248634 Epoch 32  	Train Loss = 3.43728 Val Loss = 3.25705
2023-04-06 20:14:43.312648 Epoch 33  	Train Loss = 3.43602 Val Loss = 3.24976
2023-04-06 20:15:17.326371 Epoch 34  	Train Loss = 3.43312 Val Loss = 3.25001
2023-04-06 20:15:51.317778 Epoch 35  	Train Loss = 3.43271 Val Loss = 3.24759
2023-04-06 20:16:25.306828 Epoch 36  	Train Loss = 3.43149 Val Loss = 3.24794
2023-04-06 20:16:59.271803 Epoch 37  	Train Loss = 3.42918 Val Loss = 3.24622
2023-04-06 20:17:33.223081 Epoch 38  	Train Loss = 3.42901 Val Loss = 3.25291
2023-04-06 20:18:07.180767 Epoch 39  	Train Loss = 3.42773 Val Loss = 3.24321
2023-04-06 20:18:41.104246 Epoch 40  	Train Loss = 3.42644 Val Loss = 3.24693
2023-04-06 20:19:15.097813 Epoch 41  	Train Loss = 3.42489 Val Loss = 3.24760
2023-04-06 20:19:49.073056 Epoch 42  	Train Loss = 3.42394 Val Loss = 3.24843
2023-04-06 20:20:23.043674 Epoch 43  	Train Loss = 3.42448 Val Loss = 3.24237
2023-04-06 20:20:57.017566 Epoch 44  	Train Loss = 3.42273 Val Loss = 3.24137
2023-04-06 20:21:30.999953 Epoch 45  	Train Loss = 3.42266 Val Loss = 3.25408
2023-04-06 20:22:05.030887 Epoch 46  	Train Loss = 3.42226 Val Loss = 3.24649
2023-04-06 20:22:39.009238 Epoch 47  	Train Loss = 3.42031 Val Loss = 3.24187
2023-04-06 20:23:12.997219 Epoch 48  	Train Loss = 3.41874 Val Loss = 3.24651
2023-04-06 20:23:46.957705 Epoch 49  	Train Loss = 3.42022 Val Loss = 3.24433
2023-04-06 20:24:20.932908 Epoch 50  	Train Loss = 3.41943 Val Loss = 3.24620
2023-04-06 20:24:54.899025 Epoch 51  	Train Loss = 3.41770 Val Loss = 3.23688
2023-04-06 20:25:28.879412 Epoch 52  	Train Loss = 3.42083 Val Loss = 3.23508
2023-04-06 20:26:02.818400 Epoch 53  	Train Loss = 3.41624 Val Loss = 3.24042
2023-04-06 20:26:36.744630 Epoch 54  	Train Loss = 3.41660 Val Loss = 3.24296
2023-04-06 20:27:10.652090 Epoch 55  	Train Loss = 3.41622 Val Loss = 3.24215
2023-04-06 20:27:44.537074 Epoch 56  	Train Loss = 3.41562 Val Loss = 3.23736
2023-04-06 20:28:18.362076 Epoch 57  	Train Loss = 3.41453 Val Loss = 3.24044
2023-04-06 20:28:52.185955 Epoch 58  	Train Loss = 3.41367 Val Loss = 3.23977
2023-04-06 20:29:26.036276 Epoch 59  	Train Loss = 3.41336 Val Loss = 3.23795
2023-04-06 20:29:59.914146 Epoch 60  	Train Loss = 3.41364 Val Loss = 3.23526
2023-04-06 20:30:33.777347 Epoch 61  	Train Loss = 3.41275 Val Loss = 3.23608
2023-04-06 20:31:07.670959 Epoch 62  	Train Loss = 3.41299 Val Loss = 3.24123
Early stopping at epoch: 62
Best at epoch 52:
Train Loss = 3.42083
Train RMSE = 6.98269, MAE = 3.41193, MAPE = 9.56295
Val Loss = 3.23508
Val RMSE = 6.89810, MAE = 3.28039, MAPE = 9.58824
--------- Test ---------
All Steps RMSE = 7.32539, MAE = 3.60246, MAPE = 10.48422
Step 1 RMSE = 4.35485, MAE = 2.42401, MAPE = 6.08247
Step 2 RMSE = 5.25669, MAE = 2.74627, MAPE = 7.16704
Step 3 RMSE = 5.95033, MAE = 3.01047, MAPE = 8.13640
Step 4 RMSE = 6.44306, MAE = 3.22299, MAPE = 8.93749
Step 5 RMSE = 6.89697, MAE = 3.41918, MAPE = 9.69580
Step 6 RMSE = 7.25580, MAE = 3.60327, MAPE = 10.40282
Step 7 RMSE = 7.63387, MAE = 3.77110, MAPE = 11.12568
Step 8 RMSE = 7.94290, MAE = 3.92524, MAPE = 11.73667
Step 9 RMSE = 8.23431, MAE = 4.07219, MAPE = 12.34562
Step 10 RMSE = 8.51474, MAE = 4.21091, MAPE = 12.90733
Step 11 RMSE = 8.76786, MAE = 4.34328, MAPE = 13.40581
Step 12 RMSE = 9.03673, MAE = 4.48079, MAPE = 13.86794
Inference time: 3.14 s
