PEMS08
Trainset:	x-(10700, 12, 170, 2)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 2)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 2)	y-(3566, 12, 170, 1)

--------- Attention ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        60
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "with_spatial": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                [64, 12, 170, 1]          --
├─Linear: 1-1                            [64, 12, 170, 64]         128
├─Linear: 1-2                            [64, 12, 170, 64]         128
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 170, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 170, 12, 64]         16,640
│    │    └─Dropout: 3-2                 [64, 170, 12, 64]         --
│    │    └─LayerNorm: 3-3               [64, 170, 12, 64]         128
│    │    └─Sequential: 3-4              [64, 170, 12, 64]         33,088
│    │    └─Dropout: 3-5                 [64, 170, 12, 64]         --
│    │    └─LayerNorm: 3-6               [64, 170, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 12, 170, 64]         --
│    │    └─AttentionLayer: 3-7          [64, 170, 12, 64]         16,640
│    │    └─Dropout: 3-8                 [64, 170, 12, 64]         --
│    │    └─LayerNorm: 3-9               [64, 170, 12, 64]         128
│    │    └─Sequential: 3-10             [64, 170, 12, 64]         33,088
│    │    └─Dropout: 3-11                [64, 170, 12, 64]         --
│    │    └─LayerNorm: 3-12              [64, 170, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 12, 170, 64]         --
│    │    └─AttentionLayer: 3-13         [64, 170, 12, 64]         16,640
│    │    └─Dropout: 3-14                [64, 170, 12, 64]         --
│    │    └─LayerNorm: 3-15              [64, 170, 12, 64]         128
│    │    └─Sequential: 3-16             [64, 170, 12, 64]         33,088
│    │    └─Dropout: 3-17                [64, 170, 12, 64]         --
│    │    └─LayerNorm: 3-18              [64, 170, 12, 64]         128
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 170, 64]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 170, 64]         16,640
│    │    └─Dropout: 3-20                [64, 12, 170, 64]         --
│    │    └─LayerNorm: 3-21              [64, 12, 170, 64]         128
│    │    └─Sequential: 3-22             [64, 12, 170, 64]         33,088
│    │    └─Dropout: 3-23                [64, 12, 170, 64]         --
│    │    └─LayerNorm: 3-24              [64, 12, 170, 64]         128
│    └─SelfAttentionLayer: 2-5           [64, 12, 170, 64]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 170, 64]         16,640
│    │    └─Dropout: 3-26                [64, 12, 170, 64]         --
│    │    └─LayerNorm: 3-27              [64, 12, 170, 64]         128
│    │    └─Sequential: 3-28             [64, 12, 170, 64]         33,088
│    │    └─Dropout: 3-29                [64, 12, 170, 64]         --
│    │    └─LayerNorm: 3-30              [64, 12, 170, 64]         128
│    └─SelfAttentionLayer: 2-6           [64, 12, 170, 64]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 170, 64]         16,640
│    │    └─Dropout: 3-32                [64, 12, 170, 64]         --
│    │    └─LayerNorm: 3-33              [64, 12, 170, 64]         128
│    │    └─Sequential: 3-34             [64, 12, 170, 64]         33,088
│    │    └─Dropout: 3-35                [64, 12, 170, 64]         --
│    │    └─LayerNorm: 3-36              [64, 12, 170, 64]         128
├─Linear: 1-5                            [64, 64, 170, 12]         156
├─Linear: 1-6                            [64, 12, 170, 1]          65
==========================================================================================
Total params: 300,381
Trainable params: 300,381
Non-trainable params: 0
Total mult-adds (M): 19.22
==========================================================================================
Input size (MB): 1.04
Forward/backward pass size (MB): 4613.47
Params size (MB): 1.20
Estimated Total Size (MB): 4615.71
==========================================================================================

Loss: HuberLoss

2023-05-06 19:32:30.738203 Epoch 1  	Train Loss = 33.98353 Val Loss = 23.96955
2023-05-06 19:32:52.953087 Epoch 2  	Train Loss = 22.02482 Val Loss = 20.82345
2023-05-06 19:33:15.179439 Epoch 3  	Train Loss = 20.66404 Val Loss = 20.60210
2023-05-06 19:33:37.465637 Epoch 4  	Train Loss = 20.36416 Val Loss = 20.09226
2023-05-06 19:33:59.792431 Epoch 5  	Train Loss = 19.79214 Val Loss = 19.62390
2023-05-06 19:34:22.152831 Epoch 6  	Train Loss = 19.57763 Val Loss = 19.90427
2023-05-06 19:34:44.545868 Epoch 7  	Train Loss = 19.08143 Val Loss = 19.07455
2023-05-06 19:35:06.962233 Epoch 8  	Train Loss = 18.79108 Val Loss = 19.20961
2023-05-06 19:35:29.376999 Epoch 9  	Train Loss = 18.72564 Val Loss = 18.92085
2023-05-06 19:35:51.781126 Epoch 10  	Train Loss = 18.37309 Val Loss = 18.36415
2023-05-06 19:36:14.210659 Epoch 11  	Train Loss = 17.73035 Val Loss = 17.89408
2023-05-06 19:36:36.621766 Epoch 12  	Train Loss = 17.66928 Val Loss = 17.82899
2023-05-06 19:36:59.033317 Epoch 13  	Train Loss = 17.63898 Val Loss = 17.85368
2023-05-06 19:37:21.371971 Epoch 14  	Train Loss = 17.59598 Val Loss = 17.80943
2023-05-06 19:37:43.750042 Epoch 15  	Train Loss = 17.57667 Val Loss = 17.79794
2023-05-06 19:38:06.145275 Epoch 16  	Train Loss = 17.55109 Val Loss = 17.81313
2023-05-06 19:38:28.556959 Epoch 17  	Train Loss = 17.51736 Val Loss = 17.70133
2023-05-06 19:38:50.959536 Epoch 18  	Train Loss = 17.50262 Val Loss = 17.78628
2023-05-06 19:39:13.378708 Epoch 19  	Train Loss = 17.47946 Val Loss = 17.69455
2023-05-06 19:39:35.762668 Epoch 20  	Train Loss = 17.43876 Val Loss = 17.58645
2023-05-06 19:39:58.124925 Epoch 21  	Train Loss = 17.39877 Val Loss = 17.57067
2023-05-06 19:40:20.507754 Epoch 22  	Train Loss = 17.39198 Val Loss = 17.66956
2023-05-06 19:40:42.894910 Epoch 23  	Train Loss = 17.38082 Val Loss = 17.57002
2023-05-06 19:41:05.250806 Epoch 24  	Train Loss = 17.36096 Val Loss = 17.62414
2023-05-06 19:41:27.640916 Epoch 25  	Train Loss = 17.30908 Val Loss = 17.57330
2023-05-06 19:41:50.021242 Epoch 26  	Train Loss = 17.32344 Val Loss = 17.63354
2023-05-06 19:42:12.349201 Epoch 27  	Train Loss = 17.29262 Val Loss = 17.59910
2023-05-06 19:42:34.723899 Epoch 28  	Train Loss = 17.24466 Val Loss = 17.53664
2023-05-06 19:42:57.115745 Epoch 29  	Train Loss = 17.22009 Val Loss = 17.46037
2023-05-06 19:43:19.510368 Epoch 30  	Train Loss = 17.23391 Val Loss = 17.71210
2023-05-06 19:43:41.906299 Epoch 31  	Train Loss = 17.23853 Val Loss = 17.57063
2023-05-06 19:44:04.310959 Epoch 32  	Train Loss = 17.17773 Val Loss = 17.44806
2023-05-06 19:44:26.678539 Epoch 33  	Train Loss = 17.16667 Val Loss = 17.55505
2023-05-06 19:44:49.083490 Epoch 34  	Train Loss = 17.17353 Val Loss = 17.47270
2023-05-06 19:45:11.485046 Epoch 35  	Train Loss = 17.11650 Val Loss = 17.48502
2023-05-06 19:45:33.892640 Epoch 36  	Train Loss = 17.13206 Val Loss = 17.31634
2023-05-06 19:45:56.302200 Epoch 37  	Train Loss = 17.12663 Val Loss = 17.38253
2023-05-06 19:46:18.712978 Epoch 38  	Train Loss = 17.12178 Val Loss = 17.29674
2023-05-06 19:46:41.127919 Epoch 39  	Train Loss = 17.07794 Val Loss = 17.33242
2023-05-06 19:47:03.567926 Epoch 40  	Train Loss = 17.07774 Val Loss = 17.34549
2023-05-06 19:47:25.985691 Epoch 41  	Train Loss = 17.05657 Val Loss = 17.44023
2023-05-06 19:47:48.342954 Epoch 42  	Train Loss = 17.04961 Val Loss = 17.26587
2023-05-06 19:48:10.758253 Epoch 43  	Train Loss = 17.03833 Val Loss = 17.31751
2023-05-06 19:48:33.126319 Epoch 44  	Train Loss = 17.01638 Val Loss = 17.45842
2023-05-06 19:48:55.493489 Epoch 45  	Train Loss = 16.99598 Val Loss = 17.22340
2023-05-06 19:49:17.859418 Epoch 46  	Train Loss = 16.96355 Val Loss = 17.26205
2023-05-06 19:49:40.253868 Epoch 47  	Train Loss = 16.95942 Val Loss = 17.24138
2023-05-06 19:50:02.684622 Epoch 48  	Train Loss = 16.95713 Val Loss = 17.23384
2023-05-06 19:50:25.155186 Epoch 49  	Train Loss = 16.96569 Val Loss = 17.20444
2023-05-06 19:50:47.597891 Epoch 50  	Train Loss = 16.93379 Val Loss = 17.25860
2023-05-06 19:51:09.987421 Epoch 51  	Train Loss = 16.91840 Val Loss = 17.25462
2023-05-06 19:51:32.420624 Epoch 52  	Train Loss = 16.94521 Val Loss = 17.16630
2023-05-06 19:51:54.852601 Epoch 53  	Train Loss = 16.90203 Val Loss = 17.21499
2023-05-06 19:52:17.290073 Epoch 54  	Train Loss = 16.89731 Val Loss = 17.26540
2023-05-06 19:52:39.713821 Epoch 55  	Train Loss = 16.91527 Val Loss = 17.12147
2023-05-06 19:53:02.130683 Epoch 56  	Train Loss = 16.86848 Val Loss = 17.33684
2023-05-06 19:53:24.538246 Epoch 57  	Train Loss = 16.88804 Val Loss = 17.23029
2023-05-06 19:53:46.946081 Epoch 58  	Train Loss = 16.81435 Val Loss = 17.23119
2023-05-06 19:54:09.341701 Epoch 59  	Train Loss = 16.83005 Val Loss = 17.14426
2023-05-06 19:54:31.744146 Epoch 60  	Train Loss = 16.83507 Val Loss = 17.15759
2023-05-06 19:54:54.135748 Epoch 61  	Train Loss = 16.70191 Val Loss = 17.02865
2023-05-06 19:55:16.532330 Epoch 62  	Train Loss = 16.70462 Val Loss = 17.03780
2023-05-06 19:55:38.921962 Epoch 63  	Train Loss = 16.68450 Val Loss = 17.04130
2023-05-06 19:56:01.310432 Epoch 64  	Train Loss = 16.70319 Val Loss = 17.02668
2023-05-06 19:56:23.617616 Epoch 65  	Train Loss = 16.68752 Val Loss = 17.04019
2023-05-06 19:56:45.966045 Epoch 66  	Train Loss = 16.69203 Val Loss = 17.01923
2023-05-06 19:57:08.368018 Epoch 67  	Train Loss = 16.69153 Val Loss = 17.01185
2023-05-06 19:57:30.775638 Epoch 68  	Train Loss = 16.67435 Val Loss = 17.02118
2023-05-06 19:57:53.157051 Epoch 69  	Train Loss = 16.68657 Val Loss = 17.02101
2023-05-06 19:58:15.484673 Epoch 70  	Train Loss = 16.67472 Val Loss = 17.04435
2023-05-06 19:58:37.836893 Epoch 71  	Train Loss = 16.68401 Val Loss = 17.06006
2023-05-06 19:59:00.241357 Epoch 72  	Train Loss = 16.65873 Val Loss = 17.03004
2023-05-06 19:59:22.628095 Epoch 73  	Train Loss = 16.66299 Val Loss = 17.01819
2023-05-06 19:59:45.030370 Epoch 74  	Train Loss = 16.67505 Val Loss = 17.04588
2023-05-06 20:00:07.434018 Epoch 75  	Train Loss = 16.66213 Val Loss = 17.03301
2023-05-06 20:00:29.810776 Epoch 76  	Train Loss = 16.67005 Val Loss = 17.00375
2023-05-06 20:00:52.225152 Epoch 77  	Train Loss = 16.67243 Val Loss = 17.02482
2023-05-06 20:01:14.645911 Epoch 78  	Train Loss = 16.65861 Val Loss = 16.99790
2023-05-06 20:01:37.059852 Epoch 79  	Train Loss = 16.65475 Val Loss = 17.03688
2023-05-06 20:01:59.484285 Epoch 80  	Train Loss = 16.65269 Val Loss = 17.01534
2023-05-06 20:02:21.960311 Epoch 81  	Train Loss = 16.66979 Val Loss = 17.01363
2023-05-06 20:02:44.413561 Epoch 82  	Train Loss = 16.64760 Val Loss = 17.02520
2023-05-06 20:03:06.841518 Epoch 83  	Train Loss = 16.65302 Val Loss = 17.05097
2023-05-06 20:03:29.294559 Epoch 84  	Train Loss = 16.64686 Val Loss = 17.03638
2023-05-06 20:03:51.736243 Epoch 85  	Train Loss = 16.65253 Val Loss = 17.01410
2023-05-06 20:04:14.182439 Epoch 86  	Train Loss = 16.65284 Val Loss = 17.03654
2023-05-06 20:04:36.637932 Epoch 87  	Train Loss = 16.63955 Val Loss = 17.01371
2023-05-06 20:04:59.092187 Epoch 88  	Train Loss = 16.64288 Val Loss = 17.00538
Early stopping at epoch: 88
Best at epoch 78:
Train Loss = 16.65861
Train RMSE = 27.99078, MAE = 17.12203, MAPE = 10.76785
Val Loss = 16.99790
Val RMSE = 28.32804, MAE = 17.47653, MAPE = 11.34061
--------- Test ---------
All Steps RMSE = 27.10447, MAE = 16.94673, MAPE = 10.60526
Step 1 RMSE = 21.49215, MAE = 13.85262, MAPE = 8.82463
Step 2 RMSE = 22.69191, MAE = 14.43731, MAPE = 9.07663
Step 3 RMSE = 24.07807, MAE = 15.24381, MAPE = 9.48175
Step 4 RMSE = 25.04669, MAE = 15.75063, MAPE = 9.79877
Step 5 RMSE = 25.99771, MAE = 16.32168, MAPE = 10.14738
Step 6 RMSE = 26.98919, MAE = 16.91454, MAPE = 10.50086
Step 7 RMSE = 27.63321, MAE = 17.34472, MAPE = 10.80061
Step 8 RMSE = 28.42579, MAE = 17.80197, MAPE = 11.06102
Step 9 RMSE = 29.01420, MAE = 18.16608, MAPE = 11.35849
Step 10 RMSE = 29.65537, MAE = 18.54184, MAPE = 11.62920
Step 11 RMSE = 30.52625, MAE = 19.10261, MAPE = 12.02043
Step 12 RMSE = 31.66819, MAE = 19.88305, MAPE = 12.56332
Inference time: 2.17 s
