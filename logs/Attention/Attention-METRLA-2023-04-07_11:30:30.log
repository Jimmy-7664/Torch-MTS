METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 2)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 2)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 2)

--------- Attention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "lr": 0.001,
    "milestones": [
        10,
        60
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "load_npz": true,
    "pass_device": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                --                        --
├─Linear: 1-1                            [64, 207, 12, 64]         128
├─Linear: 1-2                            [64, 207, 12, 64]         128
├─Sequential: 1-3                        [64, 207, 12, 64]         --
│    └─SelfAttentionLayer: 2-1           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-2               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-3              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-4               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-5          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-6               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-7              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-8               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-9          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-10              [64, 207, 12, 64]         128
│    │    └─Sequential: 3-11             [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-12              [64, 207, 12, 64]         128
├─Linear: 1-4                            [64, 207, 64, 12]         156
├─Linear: 1-5                            [64, 207, 12, 1]          65
==========================================================================================
Total params: 137,949
Trainable params: 137,949
Non-trainable params: 0
Total mult-adds (M): 8.83
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 2687.33
Params size (MB): 0.55
Estimated Total Size (MB): 2689.15
==========================================================================================

Loss: MaskedMAELoss

2023-04-07 11:31:14.955637 Epoch 1  	Train Loss = 4.62776 Val Loss = 3.83583
2023-04-07 11:31:47.253013 Epoch 2  	Train Loss = 3.91266 Val Loss = 3.45790
2023-04-07 11:32:19.804489 Epoch 3  	Train Loss = 3.75822 Val Loss = 3.70070
2023-04-07 11:32:52.799036 Epoch 4  	Train Loss = 3.64635 Val Loss = 3.38786
2023-04-07 11:33:26.144965 Epoch 5  	Train Loss = 3.58702 Val Loss = 3.53132
2023-04-07 11:33:59.668885 Epoch 6  	Train Loss = 3.56283 Val Loss = 3.35017
2023-04-07 11:34:33.319656 Epoch 7  	Train Loss = 3.54727 Val Loss = 3.34901
2023-04-07 11:35:07.003734 Epoch 8  	Train Loss = 3.66808 Val Loss = 3.40584
2023-04-07 11:35:40.686540 Epoch 9  	Train Loss = 3.55665 Val Loss = 3.34951
2023-04-07 11:36:14.352812 Epoch 10  	Train Loss = 3.54177 Val Loss = 3.33843
2023-04-07 11:36:48.046953 Epoch 11  	Train Loss = 3.49945 Val Loss = 3.30821
2023-04-07 11:37:21.711381 Epoch 12  	Train Loss = 3.49614 Val Loss = 3.30392
2023-04-07 11:37:55.336844 Epoch 13  	Train Loss = 3.49349 Val Loss = 3.30399
2023-04-07 11:38:29.032131 Epoch 14  	Train Loss = 3.49314 Val Loss = 3.30206
2023-04-07 11:39:02.730340 Epoch 15  	Train Loss = 3.49170 Val Loss = 3.29849
2023-04-07 11:39:36.413435 Epoch 16  	Train Loss = 3.49044 Val Loss = 3.30156
2023-04-07 11:40:10.082426 Epoch 17  	Train Loss = 3.48845 Val Loss = 3.30442
2023-04-07 11:40:43.748512 Epoch 18  	Train Loss = 3.48653 Val Loss = 3.29433
2023-04-07 11:41:17.401305 Epoch 19  	Train Loss = 3.48423 Val Loss = 3.30286
2023-04-07 11:41:51.031297 Epoch 20  	Train Loss = 3.48370 Val Loss = 3.29741
2023-04-07 11:42:24.702227 Epoch 21  	Train Loss = 3.48236 Val Loss = 3.29524
2023-04-07 11:42:58.362419 Epoch 22  	Train Loss = 3.48118 Val Loss = 3.29080
2023-04-07 11:43:32.018378 Epoch 23  	Train Loss = 3.47978 Val Loss = 3.29119
2023-04-07 11:44:05.661525 Epoch 24  	Train Loss = 3.47843 Val Loss = 3.29118
2023-04-07 11:44:39.300991 Epoch 25  	Train Loss = 3.47707 Val Loss = 3.29298
2023-04-07 11:45:12.888136 Epoch 26  	Train Loss = 3.47514 Val Loss = 3.29180
2023-04-07 11:45:46.508584 Epoch 27  	Train Loss = 3.47407 Val Loss = 3.28432
2023-04-07 11:46:20.103176 Epoch 28  	Train Loss = 3.47231 Val Loss = 3.28083
2023-04-07 11:46:53.727064 Epoch 29  	Train Loss = 3.47043 Val Loss = 3.28073
2023-04-07 11:47:27.358621 Epoch 30  	Train Loss = 3.46838 Val Loss = 3.27707
2023-04-07 11:48:01.010353 Epoch 31  	Train Loss = 3.46673 Val Loss = 3.27675
2023-04-07 11:48:34.692906 Epoch 32  	Train Loss = 3.46391 Val Loss = 3.28039
2023-04-07 11:49:08.373604 Epoch 33  	Train Loss = 3.46171 Val Loss = 3.28189
2023-04-07 11:49:42.060905 Epoch 34  	Train Loss = 3.45899 Val Loss = 3.27057
2023-04-07 11:50:15.793933 Epoch 35  	Train Loss = 3.45790 Val Loss = 3.28465
2023-04-07 11:50:49.519059 Epoch 36  	Train Loss = 3.45447 Val Loss = 3.26903
2023-04-07 11:51:23.249149 Epoch 37  	Train Loss = 3.45164 Val Loss = 3.26782
2023-04-07 11:51:56.983245 Epoch 38  	Train Loss = 3.44913 Val Loss = 3.26524
2023-04-07 11:52:30.674857 Epoch 39  	Train Loss = 3.44833 Val Loss = 3.26220
2023-04-07 11:53:04.332637 Epoch 40  	Train Loss = 3.44567 Val Loss = 3.25699
2023-04-07 11:53:38.079264 Epoch 41  	Train Loss = 3.44209 Val Loss = 3.25736
2023-04-07 11:54:11.750226 Epoch 42  	Train Loss = 3.43912 Val Loss = 3.26178
2023-04-07 11:54:45.412908 Epoch 43  	Train Loss = 3.43846 Val Loss = 3.26130
2023-04-07 11:55:19.112258 Epoch 44  	Train Loss = 3.43430 Val Loss = 3.25581
2023-04-07 11:55:52.816331 Epoch 45  	Train Loss = 3.43357 Val Loss = 3.25382
2023-04-07 11:56:26.500531 Epoch 46  	Train Loss = 3.43255 Val Loss = 3.25148
2023-04-07 11:57:00.198609 Epoch 47  	Train Loss = 3.43027 Val Loss = 3.25422
2023-04-07 11:57:33.906000 Epoch 48  	Train Loss = 3.42972 Val Loss = 3.24836
2023-04-07 11:58:07.561106 Epoch 49  	Train Loss = 3.42772 Val Loss = 3.24992
2023-04-07 11:58:41.211106 Epoch 50  	Train Loss = 3.42657 Val Loss = 3.24506
2023-04-07 11:59:14.909732 Epoch 51  	Train Loss = 3.42487 Val Loss = 3.25067
2023-04-07 11:59:48.615158 Epoch 52  	Train Loss = 3.42560 Val Loss = 3.24511
2023-04-07 12:00:22.298937 Epoch 53  	Train Loss = 3.42268 Val Loss = 3.25298
2023-04-07 12:00:56.022909 Epoch 54  	Train Loss = 3.42465 Val Loss = 3.24985
2023-04-07 12:01:29.735563 Epoch 55  	Train Loss = 3.42144 Val Loss = 3.24335
2023-04-07 12:02:03.451755 Epoch 56  	Train Loss = 3.42188 Val Loss = 3.24102
2023-04-07 12:02:37.136360 Epoch 57  	Train Loss = 3.41973 Val Loss = 3.25066
2023-04-07 12:03:10.851265 Epoch 58  	Train Loss = 3.41836 Val Loss = 3.23916
2023-04-07 12:03:44.566838 Epoch 59  	Train Loss = 3.41964 Val Loss = 3.24946
2023-04-07 12:04:18.307138 Epoch 60  	Train Loss = 3.41717 Val Loss = 3.24831
2023-04-07 12:04:52.021763 Epoch 61  	Train Loss = 3.41148 Val Loss = 3.23841
2023-04-07 12:05:25.738995 Epoch 62  	Train Loss = 3.40960 Val Loss = 3.23684
2023-04-07 12:05:59.528984 Epoch 63  	Train Loss = 3.41001 Val Loss = 3.23771
2023-04-07 12:06:33.235193 Epoch 64  	Train Loss = 3.40943 Val Loss = 3.23704
2023-04-07 12:07:06.930527 Epoch 65  	Train Loss = 3.40885 Val Loss = 3.23800
2023-04-07 12:07:40.583664 Epoch 66  	Train Loss = 3.40949 Val Loss = 3.23849
2023-04-07 12:08:14.244457 Epoch 67  	Train Loss = 3.40933 Val Loss = 3.23647
2023-04-07 12:08:47.934273 Epoch 68  	Train Loss = 3.40939 Val Loss = 3.23761
2023-04-07 12:09:21.642042 Epoch 69  	Train Loss = 3.40910 Val Loss = 3.23844
2023-04-07 12:09:55.316391 Epoch 70  	Train Loss = 3.40903 Val Loss = 3.23818
2023-04-07 12:10:29.002915 Epoch 71  	Train Loss = 3.40835 Val Loss = 3.23621
2023-04-07 12:11:02.732267 Epoch 72  	Train Loss = 3.40948 Val Loss = 3.23985
2023-04-07 12:11:36.430673 Epoch 73  	Train Loss = 3.40886 Val Loss = 3.23836
2023-04-07 12:12:10.131117 Epoch 74  	Train Loss = 3.40822 Val Loss = 3.23799
2023-04-07 12:12:43.822645 Epoch 75  	Train Loss = 3.40878 Val Loss = 3.23694
2023-04-07 12:13:17.499338 Epoch 76  	Train Loss = 3.40835 Val Loss = 3.23729
2023-04-07 12:13:51.171459 Epoch 77  	Train Loss = 3.40783 Val Loss = 3.23911
2023-04-07 12:14:24.841993 Epoch 78  	Train Loss = 3.40828 Val Loss = 3.23820
2023-04-07 12:14:58.515805 Epoch 79  	Train Loss = 3.40893 Val Loss = 3.23710
2023-04-07 12:15:32.197233 Epoch 80  	Train Loss = 3.40807 Val Loss = 3.23622
2023-04-07 12:16:05.862667 Epoch 81  	Train Loss = 3.40758 Val Loss = 3.23709
Early stopping at epoch: 81
Best at epoch 71:
Train Loss = 3.40835
Train RMSE = 6.97590, MAE = 3.40725, MAPE = 9.44192
Val Loss = 3.23621
Val RMSE = 6.88898, MAE = 3.27584, MAPE = 9.47104
--------- Test ---------
All Steps RMSE = 7.31437, MAE = 3.59636, MAPE = 10.34570
Step 1 RMSE = 4.31581, MAE = 2.41419, MAPE = 6.01267
Step 2 RMSE = 5.24766, MAE = 2.74358, MAPE = 7.10341
Step 3 RMSE = 5.93054, MAE = 3.00245, MAPE = 8.01961
Step 4 RMSE = 6.43114, MAE = 3.21815, MAPE = 8.85108
Step 5 RMSE = 6.88526, MAE = 3.41329, MAPE = 9.55964
Step 6 RMSE = 7.26132, MAE = 3.60012, MAPE = 10.27885
Step 7 RMSE = 7.61858, MAE = 3.76485, MAPE = 10.97794
Step 8 RMSE = 7.93554, MAE = 3.92059, MAPE = 11.56530
Step 9 RMSE = 8.22200, MAE = 4.06539, MAPE = 12.16276
Step 10 RMSE = 8.50415, MAE = 4.20384, MAPE = 12.72929
Step 11 RMSE = 8.76250, MAE = 4.33719, MAPE = 13.20337
Step 12 RMSE = 9.02530, MAE = 4.47276, MAPE = 13.68495
Inference time: 3.16 s
