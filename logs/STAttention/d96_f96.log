 METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 96,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         18,624
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         18,624
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         18,624
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         18,624
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         18,624
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         18,624
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 341,353
Trainable params: 341,353
Non-trainable params: 0
Total mult-adds (M): 21.85
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 6029.64
Params size (MB): 1.37
Estimated Total Size (MB): 6032.91
==========================================================================================

Loss: MaskedMAELoss

2023-04-13 15:59:50.788128 Epoch 1  	Train Loss = 4.83438 Val Loss = 3.42492
2023-04-13 16:01:32.812152 Epoch 2  	Train Loss = 3.44324 Val Loss = 3.21767
2023-04-13 16:03:16.753273 Epoch 3  	Train Loss = 3.31458 Val Loss = 3.08317
2023-04-13 16:05:01.176277 Epoch 4  	Train Loss = 3.20268 Val Loss = 3.00326
2023-04-13 16:06:46.190905 Epoch 5  	Train Loss = 3.10200 Val Loss = 2.94627
2023-04-13 16:08:31.110495 Epoch 6  	Train Loss = 3.04297 Val Loss = 2.92718
2023-04-13 16:10:16.779103 Epoch 7  	Train Loss = 3.00007 Val Loss = 2.89686
2023-04-13 16:12:03.356306 Epoch 8  	Train Loss = 2.96141 Val Loss = 2.87185
2023-04-13 16:13:49.557110 Epoch 9  	Train Loss = 2.92850 Val Loss = 2.84768
2023-04-13 16:15:35.454875 Epoch 10  	Train Loss = 2.89585 Val Loss = 2.85084
2023-04-13 16:17:21.528553 Epoch 11  	Train Loss = 2.81872 Val Loss = 2.79322
2023-04-13 16:19:07.724756 Epoch 12  	Train Loss = 2.80594 Val Loss = 2.78964
2023-04-13 16:20:54.407606 Epoch 13  	Train Loss = 2.79808 Val Loss = 2.78156
2023-04-13 16:22:41.784573 Epoch 14  	Train Loss = 2.79287 Val Loss = 2.78202
2023-04-13 16:24:29.427188 Epoch 15  	Train Loss = 2.78683 Val Loss = 2.77455
2023-04-13 16:26:17.009638 Epoch 16  	Train Loss = 2.78201 Val Loss = 2.77765
2023-04-13 16:28:03.816127 Epoch 17  	Train Loss = 2.77642 Val Loss = 2.77056
2023-04-13 16:29:50.057639 Epoch 18  	Train Loss = 2.76959 Val Loss = 2.76888
2023-04-13 16:31:36.322985 Epoch 19  	Train Loss = 2.76514 Val Loss = 2.77310
2023-04-13 16:33:22.516460 Epoch 20  	Train Loss = 2.76035 Val Loss = 2.76726
2023-04-13 16:35:09.596599 Epoch 21  	Train Loss = 2.75582 Val Loss = 2.75981
2023-04-13 16:36:56.339002 Epoch 22  	Train Loss = 2.75088 Val Loss = 2.76244
2023-04-13 16:38:43.594623 Epoch 23  	Train Loss = 2.74533 Val Loss = 2.76236
2023-04-13 16:40:30.801603 Epoch 24  	Train Loss = 2.74136 Val Loss = 2.75918
2023-04-13 16:42:17.258381 Epoch 25  	Train Loss = 2.73695 Val Loss = 2.75840
2023-04-13 16:44:03.702953 Epoch 26  	Train Loss = 2.73309 Val Loss = 2.75516
2023-04-13 16:45:50.146930 Epoch 27  	Train Loss = 2.72783 Val Loss = 2.76604
2023-04-13 16:47:36.975410 Epoch 28  	Train Loss = 2.72387 Val Loss = 2.77094
2023-04-13 16:49:24.134776 Epoch 29  	Train Loss = 2.72057 Val Loss = 2.76327
2023-04-13 16:51:11.750307 Epoch 30  	Train Loss = 2.71673 Val Loss = 2.75942
2023-04-13 16:52:59.393932 Epoch 31  	Train Loss = 2.71202 Val Loss = 2.76611
2023-04-13 16:54:47.028537 Epoch 32  	Train Loss = 2.70920 Val Loss = 2.75726
2023-04-13 16:56:33.503984 Epoch 33  	Train Loss = 2.70452 Val Loss = 2.76162
2023-04-13 16:58:19.876104 Epoch 34  	Train Loss = 2.70105 Val Loss = 2.76511
2023-04-13 17:00:06.057258 Epoch 35  	Train Loss = 2.69740 Val Loss = 2.75706
2023-04-13 17:01:52.828831 Epoch 36  	Train Loss = 2.69423 Val Loss = 2.76116
2023-04-13 17:03:40.255890 Epoch 37  	Train Loss = 2.69016 Val Loss = 2.76518
2023-04-13 17:05:27.734165 Epoch 38  	Train Loss = 2.68779 Val Loss = 2.76737
2023-04-13 17:07:15.307497 Epoch 39  	Train Loss = 2.68371 Val Loss = 2.76601
2023-04-13 17:09:02.653615 Epoch 40  	Train Loss = 2.68032 Val Loss = 2.76465
2023-04-13 17:10:48.367985 Epoch 41  	Train Loss = 2.67788 Val Loss = 2.75511
2023-04-13 17:12:33.120193 Epoch 42  	Train Loss = 2.67450 Val Loss = 2.76592
2023-04-13 17:14:17.795012 Epoch 43  	Train Loss = 2.67142 Val Loss = 2.75096
2023-04-13 17:16:02.862201 Epoch 44  	Train Loss = 2.66822 Val Loss = 2.76003
2023-04-13 17:17:48.386886 Epoch 45  	Train Loss = 2.66584 Val Loss = 2.76270
2023-04-13 17:19:34.107041 Epoch 46  	Train Loss = 2.66145 Val Loss = 2.75370
2023-04-13 17:21:19.816604 Epoch 47  	Train Loss = 2.66099 Val Loss = 2.75948
2023-04-13 17:23:05.490076 Epoch 48  	Train Loss = 2.65644 Val Loss = 2.76213
2023-04-13 17:24:50.689766 Epoch 49  	Train Loss = 2.65390 Val Loss = 2.75480
2023-04-13 17:26:35.073680 Epoch 50  	Train Loss = 2.65122 Val Loss = 2.75882
2023-04-13 17:28:19.331241 Epoch 51  	Train Loss = 2.63714 Val Loss = 2.75237
2023-04-13 17:30:03.671545 Epoch 52  	Train Loss = 2.63575 Val Loss = 2.75251
2023-04-13 17:31:48.239392 Epoch 53  	Train Loss = 2.63534 Val Loss = 2.75203
2023-04-13 17:33:33.243540 Epoch 54  	Train Loss = 2.63456 Val Loss = 2.75207
2023-04-13 17:35:18.258587 Epoch 55  	Train Loss = 2.63404 Val Loss = 2.75185
2023-04-13 17:37:03.540019 Epoch 56  	Train Loss = 2.63402 Val Loss = 2.75562
2023-04-13 17:38:48.965421 Epoch 57  	Train Loss = 2.63358 Val Loss = 2.75255
2023-04-13 17:40:33.755792 Epoch 58  	Train Loss = 2.63313 Val Loss = 2.75348
2023-04-13 17:42:18.430138 Epoch 59  	Train Loss = 2.63302 Val Loss = 2.75309
2023-04-13 17:44:03.078838 Epoch 60  	Train Loss = 2.63244 Val Loss = 2.75253
2023-04-13 17:45:47.763772 Epoch 61  	Train Loss = 2.63166 Val Loss = 2.74892
2023-04-13 17:47:32.406347 Epoch 62  	Train Loss = 2.63131 Val Loss = 2.75248
2023-04-13 17:49:17.466325 Epoch 63  	Train Loss = 2.63116 Val Loss = 2.75254
2023-04-13 17:51:01.902341 Epoch 64  	Train Loss = 2.63117 Val Loss = 2.75096
2023-04-13 17:52:46.202481 Epoch 65  	Train Loss = 2.63089 Val Loss = 2.75176
2023-04-13 17:54:30.691757 Epoch 66  	Train Loss = 2.63008 Val Loss = 2.75106
2023-04-13 17:56:15.588873 Epoch 67  	Train Loss = 2.62931 Val Loss = 2.75027
2023-04-13 17:58:00.498367 Epoch 68  	Train Loss = 2.62966 Val Loss = 2.75260
2023-04-13 17:59:46.134796 Epoch 69  	Train Loss = 2.62924 Val Loss = 2.75369
2023-04-13 18:01:31.394991 Epoch 70  	Train Loss = 2.62948 Val Loss = 2.75408
2023-04-13 18:03:15.995799 Epoch 71  	Train Loss = 2.62864 Val Loss = 2.75298
2023-04-13 18:05:00.509901 Epoch 72  	Train Loss = 2.62797 Val Loss = 2.75355
2023-04-13 18:06:45.048320 Epoch 73  	Train Loss = 2.62796 Val Loss = 2.75095
2023-04-13 18:08:29.681484 Epoch 74  	Train Loss = 2.62737 Val Loss = 2.74987
2023-04-13 18:10:14.483302 Epoch 75  	Train Loss = 2.62733 Val Loss = 2.75084
2023-04-13 18:11:59.430483 Epoch 76  	Train Loss = 2.62644 Val Loss = 2.75443
2023-04-13 18:13:44.625192 Epoch 77  	Train Loss = 2.62622 Val Loss = 2.75630
2023-04-13 18:15:29.933641 Epoch 78  	Train Loss = 2.62611 Val Loss = 2.75319
2023-04-13 18:17:14.736117 Epoch 79  	Train Loss = 2.62579 Val Loss = 2.74831
2023-04-13 18:18:59.364638 Epoch 80  	Train Loss = 2.62549 Val Loss = 2.74946
2023-04-13 18:20:44.020554 Epoch 81  	Train Loss = 2.62523 Val Loss = 2.75408
2023-04-13 18:22:28.655541 Epoch 82  	Train Loss = 2.62453 Val Loss = 2.75398
2023-04-13 18:24:13.472388 Epoch 83  	Train Loss = 2.62472 Val Loss = 2.75155
2023-04-13 18:25:58.398355 Epoch 84  	Train Loss = 2.62407 Val Loss = 2.74935
2023-04-13 18:27:43.516994 Epoch 85  	Train Loss = 2.62400 Val Loss = 2.75485
2023-04-13 18:29:28.676557 Epoch 86  	Train Loss = 2.62306 Val Loss = 2.75488
2023-04-13 18:31:13.487156 Epoch 87  	Train Loss = 2.62354 Val Loss = 2.75193
2023-04-13 18:32:57.718737 Epoch 88  	Train Loss = 2.62330 Val Loss = 2.75616
2023-04-13 18:34:42.093198 Epoch 89  	Train Loss = 2.62193 Val Loss = 2.75257
2023-04-13 18:36:26.618733 Epoch 90  	Train Loss = 2.62139 Val Loss = 2.75277
2023-04-13 18:38:11.309295 Epoch 91  	Train Loss = 2.62167 Val Loss = 2.74996
2023-04-13 18:39:56.182893 Epoch 92  	Train Loss = 2.62133 Val Loss = 2.75077
2023-04-13 18:41:41.009724 Epoch 93  	Train Loss = 2.62085 Val Loss = 2.75414
2023-04-13 18:43:25.741080 Epoch 94  	Train Loss = 2.62123 Val Loss = 2.75541
2023-04-13 18:45:10.057380 Epoch 95  	Train Loss = 2.62055 Val Loss = 2.75005
2023-04-13 18:46:53.718087 Epoch 96  	Train Loss = 2.62021 Val Loss = 2.75571
2023-04-13 18:48:37.459748 Epoch 97  	Train Loss = 2.61991 Val Loss = 2.75270
2023-04-13 18:50:21.351805 Epoch 98  	Train Loss = 2.61973 Val Loss = 2.75259
2023-04-13 18:52:05.660440 Epoch 99  	Train Loss = 2.61916 Val Loss = 2.75155
Early stopping at epoch: 99
Best at epoch 79:
Train Loss = 2.62579
Train RMSE = 5.16856, MAE = 2.59755, MAPE = 6.67049
Val Loss = 2.74831
Val RMSE = 5.85329, MAE = 2.78924, MAPE = 7.62627
--------- Test ---------
All Steps RMSE = 6.16349, MAE = 2.99377, MAPE = 8.09356
Step 1 RMSE = 4.03308, MAE = 2.27837, MAPE = 5.44938
Step 2 RMSE = 4.71077, MAE = 2.51471, MAPE = 6.23486
Step 3 RMSE = 5.18212, MAE = 2.68109, MAPE = 6.85422
Step 4 RMSE = 5.58854, MAE = 2.81464, MAPE = 7.37356
Step 5 RMSE = 5.90589, MAE = 2.92865, MAPE = 7.81473
Step 6 RMSE = 6.19337, MAE = 3.02909, MAPE = 8.21321
Step 7 RMSE = 6.44485, MAE = 3.11806, MAPE = 8.56428
Step 8 RMSE = 6.64192, MAE = 3.19254, MAPE = 8.85850
Step 9 RMSE = 6.83751, MAE = 3.26112, MAPE = 9.12723
Step 10 RMSE = 7.00433, MAE = 3.31662, MAPE = 9.34974
Step 11 RMSE = 7.15950, MAE = 3.36676, MAPE = 9.54582
Step 12 RMSE = 7.30495, MAE = 3.42360, MAPE = 9.73738
Inference time: 9.36 s
