METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        40
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 192,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         37,152
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         37,152
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         37,152
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         37,152
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         37,152
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         37,152
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 452,521
Trainable params: 452,521
Non-trainable params: 0
Total mult-adds (M): 28.96
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 6762.20
Params size (MB): 1.81
Estimated Total Size (MB): 6765.92
==========================================================================================

Loss: MaskedMAELoss

2023-04-13 15:56:23.320924 Epoch 1  	Train Loss = 4.68930 Val Loss = 3.34003
2023-04-13 15:58:16.519906 Epoch 2  	Train Loss = 3.40669 Val Loss = 3.17766
2023-04-13 16:00:09.012617 Epoch 3  	Train Loss = 3.26772 Val Loss = 3.07300
2023-04-13 16:02:02.917036 Epoch 4  	Train Loss = 3.13866 Val Loss = 2.94458
2023-04-13 16:03:57.798769 Epoch 5  	Train Loss = 3.06482 Val Loss = 2.91543
2023-04-13 16:05:55.083816 Epoch 6  	Train Loss = 3.01820 Val Loss = 2.90892
2023-04-13 16:07:52.829696 Epoch 7  	Train Loss = 2.97104 Val Loss = 2.88082
2023-04-13 16:09:50.864878 Epoch 8  	Train Loss = 2.93406 Val Loss = 2.83682
2023-04-13 16:11:49.501739 Epoch 9  	Train Loss = 2.90089 Val Loss = 2.83131
2023-04-13 16:13:46.444070 Epoch 10  	Train Loss = 2.86561 Val Loss = 2.79372
2023-04-13 16:15:43.391637 Epoch 11  	Train Loss = 2.78107 Val Loss = 2.75539
2023-04-13 16:17:40.719640 Epoch 12  	Train Loss = 2.76807 Val Loss = 2.76216
2023-04-13 16:19:39.337287 Epoch 13  	Train Loss = 2.76095 Val Loss = 2.75833
2023-04-13 16:21:38.480851 Epoch 14  	Train Loss = 2.75456 Val Loss = 2.75316
2023-04-13 16:23:38.608007 Epoch 15  	Train Loss = 2.74875 Val Loss = 2.75210
2023-04-13 16:25:39.443235 Epoch 16  	Train Loss = 2.74290 Val Loss = 2.75460
2023-04-13 16:27:38.502634 Epoch 17  	Train Loss = 2.73766 Val Loss = 2.76395
2023-04-13 16:29:35.496368 Epoch 18  	Train Loss = 2.73148 Val Loss = 2.75879
2023-04-13 16:31:32.934106 Epoch 19  	Train Loss = 2.72567 Val Loss = 2.74671
2023-04-13 16:33:31.236395 Epoch 20  	Train Loss = 2.71977 Val Loss = 2.75551
2023-04-13 16:35:31.299616 Epoch 21  	Train Loss = 2.71512 Val Loss = 2.75262
2023-04-13 16:37:31.374854 Epoch 22  	Train Loss = 2.70932 Val Loss = 2.75587
2023-04-13 16:39:31.574414 Epoch 23  	Train Loss = 2.70538 Val Loss = 2.74480
2023-04-13 16:41:30.718972 Epoch 24  	Train Loss = 2.70167 Val Loss = 2.75288
2023-04-13 16:43:27.470372 Epoch 25  	Train Loss = 2.69589 Val Loss = 2.76197
2023-04-13 16:45:24.980802 Epoch 26  	Train Loss = 2.69135 Val Loss = 2.74924
2023-04-13 16:47:23.535107 Epoch 27  	Train Loss = 2.68620 Val Loss = 2.75072
2023-04-13 16:49:23.472731 Epoch 28  	Train Loss = 2.68113 Val Loss = 2.75323
2023-04-13 16:51:24.115831 Epoch 29  	Train Loss = 2.67715 Val Loss = 2.75158
2023-04-13 16:53:25.243703 Epoch 30  	Train Loss = 2.67316 Val Loss = 2.74930
2023-04-13 16:55:25.195571 Epoch 31  	Train Loss = 2.66946 Val Loss = 2.76516
2023-04-13 16:57:22.308620 Epoch 32  	Train Loss = 2.66497 Val Loss = 2.75825
2023-04-13 16:59:19.451272 Epoch 33  	Train Loss = 2.66135 Val Loss = 2.75974
2023-04-13 17:01:17.315279 Epoch 34  	Train Loss = 2.65705 Val Loss = 2.74634
2023-04-13 17:03:16.842780 Epoch 35  	Train Loss = 2.65265 Val Loss = 2.76775
2023-04-13 17:05:17.876168 Epoch 36  	Train Loss = 2.64962 Val Loss = 2.75229
2023-04-13 17:07:18.377524 Epoch 37  	Train Loss = 2.64516 Val Loss = 2.76202
2023-04-13 17:09:18.425839 Epoch 38  	Train Loss = 2.64242 Val Loss = 2.75037
2023-04-13 17:11:15.624448 Epoch 39  	Train Loss = 2.63850 Val Loss = 2.75239
2023-04-13 17:13:12.111794 Epoch 40  	Train Loss = 2.63434 Val Loss = 2.75113
2023-04-13 17:15:09.139548 Epoch 41  	Train Loss = 2.62013 Val Loss = 2.75127
2023-04-13 17:17:07.244165 Epoch 42  	Train Loss = 2.61782 Val Loss = 2.74846
2023-04-13 17:19:07.134993 Epoch 43  	Train Loss = 2.61721 Val Loss = 2.75078
Early stopping at epoch: 43
Best at epoch 23:
Train Loss = 2.70538
Train RMSE = 5.15871, MAE = 2.59607, MAPE = 6.66275
Val Loss = 2.74480
Val RMSE = 5.83370, MAE = 2.78849, MAPE = 7.62257
--------- Test ---------
All Steps RMSE = 6.12019, MAE = 2.98440, MAPE = 8.06836
Step 1 RMSE = 4.02741, MAE = 2.27811, MAPE = 5.45258
Step 2 RMSE = 4.69257, MAE = 2.51436, MAPE = 6.24531
Step 3 RMSE = 5.16526, MAE = 2.68284, MAPE = 6.85935
Step 4 RMSE = 5.57048, MAE = 2.81582, MAPE = 7.37850
Step 5 RMSE = 5.88933, MAE = 2.92617, MAPE = 7.79888
Step 6 RMSE = 6.16771, MAE = 3.02504, MAPE = 8.19506
Step 7 RMSE = 6.41258, MAE = 3.11038, MAPE = 8.53507
Step 8 RMSE = 6.59153, MAE = 3.18034, MAPE = 8.83411
Step 9 RMSE = 6.78514, MAE = 3.24647, MAPE = 9.09163
Step 10 RMSE = 6.93473, MAE = 3.29577, MAPE = 9.29153
Step 11 RMSE = 7.07581, MAE = 3.34253, MAPE = 9.47471
Step 12 RMSE = 7.21402, MAE = 3.39504, MAPE = 9.66376
Inference time: 10.89 s
