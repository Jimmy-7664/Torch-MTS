METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.001,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.5
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 526,633
Trainable params: 526,633
Non-trainable params: 0
Total mult-adds (M): 33.70
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 7250.58
Params size (MB): 2.11
Estimated Total Size (MB): 7254.59
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 15:00:41.800529 Epoch 1  	Train Loss = 5.10045 Val Loss = 3.96734
2023-04-12 15:02:37.685032 Epoch 2  	Train Loss = 3.56224 Val Loss = 4.09233
2023-04-12 15:04:33.602775 Epoch 3  	Train Loss = 3.46259 Val Loss = 3.48189
2023-04-12 15:06:29.405129 Epoch 4  	Train Loss = 3.41251 Val Loss = 3.51343
2023-04-12 15:08:25.227304 Epoch 5  	Train Loss = 3.37773 Val Loss = 3.59218
2023-04-12 15:10:21.407886 Epoch 6  	Train Loss = 3.33370 Val Loss = 3.41213
2023-04-12 15:12:17.796843 Epoch 7  	Train Loss = 3.26604 Val Loss = 3.29227
2023-04-12 15:14:14.506226 Epoch 8  	Train Loss = 3.21372 Val Loss = 3.38229
2023-04-12 15:16:11.439487 Epoch 9  	Train Loss = 3.17098 Val Loss = 3.07592
2023-04-12 15:18:07.955979 Epoch 10  	Train Loss = 3.12758 Val Loss = 3.07555
2023-04-12 15:20:03.572314 Epoch 11  	Train Loss = 3.05621 Val Loss = 3.01262
2023-04-12 15:21:59.214541 Epoch 12  	Train Loss = 3.04322 Val Loss = 3.01384
2023-04-12 15:23:54.949938 Epoch 13  	Train Loss = 3.03775 Val Loss = 2.98929
2023-04-12 15:25:51.065510 Epoch 14  	Train Loss = 3.03205 Val Loss = 2.97978
2023-04-12 15:27:47.274264 Epoch 15  	Train Loss = 3.02782 Val Loss = 2.98166
2023-04-12 15:29:43.618404 Epoch 16  	Train Loss = 3.02238 Val Loss = 2.95495
2023-04-12 15:31:39.572616 Epoch 17  	Train Loss = 3.01658 Val Loss = 2.95659
2023-04-12 15:33:32.774081 Epoch 18  	Train Loss = 3.01373 Val Loss = 2.95857
2023-04-12 15:35:28.007923 Epoch 19  	Train Loss = 3.00863 Val Loss = 2.96424
2023-04-12 15:37:23.548629 Epoch 20  	Train Loss = 3.00457 Val Loss = 2.95908
2023-04-12 15:39:19.556251 Epoch 21  	Train Loss = 3.00008 Val Loss = 2.93668
2023-04-12 15:41:15.979280 Epoch 22  	Train Loss = 2.99510 Val Loss = 2.94594
2023-04-12 15:43:12.578263 Epoch 23  	Train Loss = 2.99110 Val Loss = 2.95124
2023-04-12 15:45:09.101089 Epoch 24  	Train Loss = 2.98829 Val Loss = 2.93818
2023-04-12 15:47:04.964432 Epoch 25  	Train Loss = 2.98480 Val Loss = 2.98125
2023-04-12 15:49:00.262109 Epoch 26  	Train Loss = 2.98217 Val Loss = 2.95418
2023-04-12 15:50:55.473005 Epoch 27  	Train Loss = 2.97766 Val Loss = 2.91145
2023-04-12 15:52:51.218824 Epoch 28  	Train Loss = 2.97489 Val Loss = 2.93401
2023-04-12 15:54:47.565119 Epoch 29  	Train Loss = 2.97396 Val Loss = 2.93220
2023-04-12 15:56:44.142072 Epoch 30  	Train Loss = 2.97018 Val Loss = 2.90669
2023-04-12 15:58:40.683868 Epoch 31  	Train Loss = 2.95621 Val Loss = 2.90294
2023-04-12 16:00:37.145652 Epoch 32  	Train Loss = 2.95506 Val Loss = 2.90692
2023-04-12 16:02:32.697373 Epoch 33  	Train Loss = 2.95397 Val Loss = 2.91607
2023-04-12 16:04:27.961245 Epoch 34  	Train Loss = 2.95314 Val Loss = 2.90562
2023-04-12 16:06:23.510102 Epoch 35  	Train Loss = 2.95357 Val Loss = 2.90600
2023-04-12 16:08:19.484120 Epoch 36  	Train Loss = 2.95309 Val Loss = 2.89960
2023-04-12 16:10:15.899713 Epoch 37  	Train Loss = 2.95242 Val Loss = 2.91386
2023-04-12 16:12:12.382078 Epoch 38  	Train Loss = 2.95212 Val Loss = 2.90299
2023-04-12 16:14:08.793173 Epoch 39  	Train Loss = 2.95155 Val Loss = 2.90042
2023-04-12 16:16:04.092115 Epoch 40  	Train Loss = 2.95109 Val Loss = 2.90279
2023-04-12 16:17:58.442723 Epoch 41  	Train Loss = 2.95053 Val Loss = 2.90558
2023-04-12 16:19:53.321287 Epoch 42  	Train Loss = 2.94985 Val Loss = 2.90380
2023-04-12 16:21:48.931828 Epoch 43  	Train Loss = 2.94937 Val Loss = 2.91666
2023-04-12 16:23:45.243194 Epoch 44  	Train Loss = 2.94844 Val Loss = 2.90474
2023-04-12 16:25:41.442650 Epoch 45  	Train Loss = 2.94894 Val Loss = 2.91452
2023-04-12 16:27:37.618197 Epoch 46  	Train Loss = 2.94815 Val Loss = 2.90139
Early stopping at epoch: 46
Best at epoch 36:
Train Loss = 2.95309
Train RMSE = 6.02351, MAE = 2.95517, MAPE = 7.91346
Val Loss = 2.89960
Val RMSE = 6.15966, MAE = 2.93592, MAPE = 8.20345
--------- Test ---------
All Steps RMSE = 6.50449, MAE = 3.17830, MAPE = 8.85487
Step 1 RMSE = 4.40637, MAE = 2.44393, MAPE = 6.07911
Step 2 RMSE = 5.13466, MAE = 2.68501, MAPE = 6.89141
Step 3 RMSE = 5.62000, MAE = 2.86108, MAPE = 7.53301
Step 4 RMSE = 5.99819, MAE = 3.00688, MAPE = 8.13505
Step 5 RMSE = 6.28028, MAE = 3.12988, MAPE = 8.63017
Step 6 RMSE = 6.55127, MAE = 3.23363, MAPE = 9.05848
Step 7 RMSE = 6.79253, MAE = 3.31066, MAPE = 9.42434
Step 8 RMSE = 6.98945, MAE = 3.38094, MAPE = 9.69650
Step 9 RMSE = 7.15934, MAE = 3.43703, MAPE = 9.91072
Step 10 RMSE = 7.28800, MAE = 3.48733, MAPE = 10.09360
Step 11 RMSE = 7.41500, MAE = 3.54258, MAPE = 10.28620
Step 12 RMSE = 7.59302, MAE = 3.62074, MAPE = 10.52006
Inference time: 10.48 s
