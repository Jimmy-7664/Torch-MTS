METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-9               [64, 12, 207, 96]         192
│    │    └─Sequential: 3-10             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-11                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-12              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 178,089
Trainable params: 178,089
Non-trainable params: 0
Total mult-adds (M): 11.40
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 2529.63
Params size (MB): 0.71
Estimated Total Size (MB): 2532.25
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 20:47:10.081436 Epoch 1  	Train Loss = 4.60332 Val Loss = 3.45621
2023-04-12 20:47:50.083664 Epoch 2  	Train Loss = 3.50756 Val Loss = 3.28814
2023-04-12 20:48:30.614086 Epoch 3  	Train Loss = 3.35086 Val Loss = 3.17318
2023-04-12 20:49:12.019355 Epoch 4  	Train Loss = 3.25073 Val Loss = 3.11259
2023-04-12 20:49:53.822264 Epoch 5  	Train Loss = 3.17735 Val Loss = 3.07696
2023-04-12 20:50:35.853640 Epoch 6  	Train Loss = 3.10205 Val Loss = 3.03130
2023-04-12 20:51:17.494178 Epoch 7  	Train Loss = 3.05707 Val Loss = 2.96294
2023-04-12 20:51:59.356817 Epoch 8  	Train Loss = 3.01148 Val Loss = 2.97269
2023-04-12 20:52:40.760479 Epoch 9  	Train Loss = 2.98177 Val Loss = 3.00138
2023-04-12 20:53:22.480014 Epoch 10  	Train Loss = 2.95457 Val Loss = 2.93360
2023-04-12 20:54:04.273882 Epoch 11  	Train Loss = 2.89760 Val Loss = 2.90447
2023-04-12 20:54:45.821907 Epoch 12  	Train Loss = 2.88831 Val Loss = 2.90863
2023-04-12 20:55:27.590891 Epoch 13  	Train Loss = 2.88269 Val Loss = 2.90374
2023-04-12 20:56:09.166909 Epoch 14  	Train Loss = 2.87856 Val Loss = 2.91374
2023-04-12 20:56:50.830702 Epoch 15  	Train Loss = 2.87496 Val Loss = 2.90556
2023-04-12 20:57:32.618887 Epoch 16  	Train Loss = 2.87059 Val Loss = 2.90925
2023-04-12 20:58:13.950128 Epoch 17  	Train Loss = 2.86639 Val Loss = 2.91352
2023-04-12 20:58:55.918470 Epoch 18  	Train Loss = 2.86181 Val Loss = 2.90466
2023-04-12 20:59:37.700347 Epoch 19  	Train Loss = 2.85842 Val Loss = 2.90321
2023-04-12 21:00:19.549061 Epoch 20  	Train Loss = 2.85461 Val Loss = 2.90069
2023-04-12 21:01:01.531405 Epoch 21  	Train Loss = 2.85068 Val Loss = 2.90590
2023-04-12 21:01:43.621279 Epoch 22  	Train Loss = 2.84690 Val Loss = 2.89794
2023-04-12 21:02:25.447578 Epoch 23  	Train Loss = 2.84366 Val Loss = 2.90041
2023-04-12 21:03:07.608501 Epoch 24  	Train Loss = 2.84083 Val Loss = 2.90359
2023-04-12 21:03:49.714790 Epoch 25  	Train Loss = 2.83702 Val Loss = 2.90086
2023-04-12 21:04:32.102523 Epoch 26  	Train Loss = 2.83256 Val Loss = 2.90132
2023-04-12 21:05:14.263354 Epoch 27  	Train Loss = 2.82958 Val Loss = 2.91538
2023-04-12 21:05:55.749787 Epoch 28  	Train Loss = 2.82600 Val Loss = 2.89989
2023-04-12 21:06:37.572873 Epoch 29  	Train Loss = 2.82305 Val Loss = 2.89863
2023-04-12 21:07:19.215856 Epoch 30  	Train Loss = 2.82001 Val Loss = 2.89678
2023-04-12 21:08:00.885675 Epoch 31  	Train Loss = 2.80932 Val Loss = 2.90082
2023-04-12 21:08:42.080889 Epoch 32  	Train Loss = 2.80843 Val Loss = 2.90188
2023-04-12 21:09:22.887916 Epoch 33  	Train Loss = 2.80784 Val Loss = 2.90014
2023-04-12 21:10:03.850755 Epoch 34  	Train Loss = 2.80759 Val Loss = 2.89935
2023-04-12 21:10:45.549984 Epoch 35  	Train Loss = 2.80709 Val Loss = 2.90150
2023-04-12 21:11:27.294522 Epoch 36  	Train Loss = 2.80707 Val Loss = 2.89886
2023-04-12 21:12:09.161755 Epoch 37  	Train Loss = 2.80632 Val Loss = 2.90293
2023-04-12 21:12:51.032574 Epoch 38  	Train Loss = 2.80573 Val Loss = 2.89915
2023-04-12 21:13:32.915372 Epoch 39  	Train Loss = 2.80599 Val Loss = 2.90283
2023-04-12 21:14:14.831056 Epoch 40  	Train Loss = 2.80507 Val Loss = 2.90131
Early stopping at epoch: 40
Best at epoch 30:
Train Loss = 2.82001
Train RMSE = 5.70675, MAE = 2.80424, MAPE = 7.61189
Val Loss = 2.89678
Val RMSE = 6.18091, MAE = 2.94443, MAPE = 8.51877
--------- Test ---------
All Steps RMSE = 6.44363, MAE = 3.14030, MAPE = 8.88947
Step 1 RMSE = 4.24765, MAE = 2.37413, MAPE = 5.97096
Step 2 RMSE = 5.04463, MAE = 2.64893, MAPE = 6.93043
Step 3 RMSE = 5.53371, MAE = 2.82984, MAPE = 7.60633
Step 4 RMSE = 5.97720, MAE = 2.98507, MAPE = 8.24371
Step 5 RMSE = 6.26382, MAE = 3.09022, MAPE = 8.67765
Step 6 RMSE = 6.49969, MAE = 3.18677, MAPE = 9.07616
Step 7 RMSE = 6.72941, MAE = 3.28220, MAPE = 9.44442
Step 8 RMSE = 6.90994, MAE = 3.34212, MAPE = 9.71836
Step 9 RMSE = 7.11732, MAE = 3.40801, MAPE = 9.96394
Step 10 RMSE = 7.23494, MAE = 3.44874, MAPE = 10.13854
Step 11 RMSE = 7.36209, MAE = 3.50743, MAPE = 10.31957
Step 12 RMSE = 7.52946, MAE = 3.58028, MAPE = 10.58388
Inference time: 3.92 s
