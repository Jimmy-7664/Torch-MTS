METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.001,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 526,633
Trainable params: 526,633
Non-trainable params: 0
Total mult-adds (M): 33.70
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 7250.58
Params size (MB): 2.11
Estimated Total Size (MB): 7254.59
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 16:54:24.719008 Epoch 1  	Train Loss = 5.06153 Val Loss = 3.40884
2023-04-12 16:56:19.117036 Epoch 2  	Train Loss = 3.50947 Val Loss = 3.25791
2023-04-12 16:58:14.197267 Epoch 3  	Train Loss = 3.39636 Val Loss = 3.24025
2023-04-12 17:00:09.158743 Epoch 4  	Train Loss = 3.31262 Val Loss = 3.08882
2023-04-12 17:02:04.024956 Epoch 5  	Train Loss = 3.23666 Val Loss = 3.06686
2023-04-12 17:03:58.855801 Epoch 6  	Train Loss = 3.16979 Val Loss = 3.02043
2023-04-12 17:05:53.773326 Epoch 7  	Train Loss = 3.12244 Val Loss = 2.96479
2023-04-12 17:07:48.870179 Epoch 8  	Train Loss = 3.09708 Val Loss = 2.92626
2023-04-12 17:09:44.147240 Epoch 9  	Train Loss = 3.07091 Val Loss = 2.95246
2023-04-12 17:11:39.342568 Epoch 10  	Train Loss = 3.03565 Val Loss = 2.90690
2023-04-12 17:13:34.519446 Epoch 11  	Train Loss = 2.97203 Val Loss = 2.85119
2023-04-12 17:15:29.395326 Epoch 12  	Train Loss = 2.95918 Val Loss = 2.84883
2023-04-12 17:17:24.121795 Epoch 13  	Train Loss = 2.95544 Val Loss = 2.84876
2023-04-12 17:19:19.074153 Epoch 14  	Train Loss = 2.95175 Val Loss = 2.84587
2023-04-12 17:21:14.398291 Epoch 15  	Train Loss = 2.94864 Val Loss = 2.84219
2023-04-12 17:23:09.821334 Epoch 16  	Train Loss = 2.94529 Val Loss = 2.84410
2023-04-12 17:25:05.308404 Epoch 17  	Train Loss = 2.94152 Val Loss = 2.84336
2023-04-12 17:27:00.705522 Epoch 18  	Train Loss = 2.93890 Val Loss = 2.84522
2023-04-12 17:28:55.410091 Epoch 19  	Train Loss = 2.93598 Val Loss = 2.84339
2023-04-12 17:30:49.795838 Epoch 20  	Train Loss = 2.93382 Val Loss = 2.84578
2023-04-12 17:32:42.878972 Epoch 21  	Train Loss = 2.93011 Val Loss = 2.84089
2023-04-12 17:34:37.345799 Epoch 22  	Train Loss = 2.92722 Val Loss = 2.83455
2023-04-12 17:36:31.817454 Epoch 23  	Train Loss = 2.92346 Val Loss = 2.85137
2023-04-12 17:38:26.150223 Epoch 24  	Train Loss = 2.92162 Val Loss = 2.83784
2023-04-12 17:40:20.343401 Epoch 25  	Train Loss = 2.92010 Val Loss = 2.85337
2023-04-12 17:42:14.584901 Epoch 26  	Train Loss = 2.91597 Val Loss = 2.83686
2023-04-12 17:44:08.986943 Epoch 27  	Train Loss = 2.91452 Val Loss = 2.83361
2023-04-12 17:46:03.469119 Epoch 28  	Train Loss = 2.91175 Val Loss = 2.83279
2023-04-12 17:47:58.246186 Epoch 29  	Train Loss = 2.90983 Val Loss = 2.83247
2023-04-12 17:49:53.043080 Epoch 30  	Train Loss = 2.90827 Val Loss = 2.82855
2023-04-12 17:51:47.489408 Epoch 31  	Train Loss = 2.89443 Val Loss = 2.82515
2023-04-12 17:53:41.693962 Epoch 32  	Train Loss = 2.89353 Val Loss = 2.82395
2023-04-12 17:55:35.966057 Epoch 33  	Train Loss = 2.89256 Val Loss = 2.82555
2023-04-12 17:57:30.338199 Epoch 34  	Train Loss = 2.89190 Val Loss = 2.82456
2023-04-12 17:59:24.905267 Epoch 35  	Train Loss = 2.89267 Val Loss = 2.82279
2023-04-12 18:01:19.505986 Epoch 36  	Train Loss = 2.89163 Val Loss = 2.82352
2023-04-12 18:03:14.442107 Epoch 37  	Train Loss = 2.89162 Val Loss = 2.82356
2023-04-12 18:05:08.993793 Epoch 38  	Train Loss = 2.89094 Val Loss = 2.82488
2023-04-12 18:07:03.281067 Epoch 39  	Train Loss = 2.89055 Val Loss = 2.82345
2023-04-12 18:08:57.554364 Epoch 40  	Train Loss = 2.89042 Val Loss = 2.82251
2023-04-12 18:10:51.782591 Epoch 41  	Train Loss = 2.88993 Val Loss = 2.82314
2023-04-12 18:12:45.955608 Epoch 42  	Train Loss = 2.88940 Val Loss = 2.82294
2023-04-12 18:14:40.342091 Epoch 43  	Train Loss = 2.88919 Val Loss = 2.82820
2023-04-12 18:16:34.845348 Epoch 44  	Train Loss = 2.88884 Val Loss = 2.82385
2023-04-12 18:18:29.385989 Epoch 45  	Train Loss = 2.88828 Val Loss = 2.82318
2023-04-12 18:20:23.833189 Epoch 46  	Train Loss = 2.88831 Val Loss = 2.82406
2023-04-12 18:22:18.495764 Epoch 47  	Train Loss = 2.88828 Val Loss = 2.82540
2023-04-12 18:24:13.180508 Epoch 48  	Train Loss = 2.88748 Val Loss = 2.82553
2023-04-12 18:26:07.779748 Epoch 49  	Train Loss = 2.88717 Val Loss = 2.82683
2023-04-12 18:28:02.291284 Epoch 50  	Train Loss = 2.88662 Val Loss = 2.82552
Early stopping at epoch: 50
Best at epoch 40:
Train Loss = 2.89042
Train RMSE = 5.84388, MAE = 2.86609, MAPE = 7.67987
Val Loss = 2.82251
Val RMSE = 6.00692, MAE = 2.86554, MAPE = 8.00026
--------- Test ---------
All Steps RMSE = 6.34497, MAE = 3.09410, MAPE = 8.60936
Step 1 RMSE = 4.19858, MAE = 2.35416, MAPE = 5.79489
Step 2 RMSE = 5.00097, MAE = 2.62121, MAPE = 6.70973
Step 3 RMSE = 5.50449, MAE = 2.80606, MAPE = 7.40015
Step 4 RMSE = 5.89927, MAE = 2.94476, MAPE = 7.97717
Step 5 RMSE = 6.18217, MAE = 3.05448, MAPE = 8.41659
Step 6 RMSE = 6.42328, MAE = 3.15042, MAPE = 8.80994
Step 7 RMSE = 6.64729, MAE = 3.22442, MAPE = 9.13514
Step 8 RMSE = 6.79383, MAE = 3.28661, MAPE = 9.40797
Step 9 RMSE = 6.96448, MAE = 3.34633, MAPE = 9.63799
Step 10 RMSE = 7.09340, MAE = 3.39065, MAPE = 9.81703
Step 11 RMSE = 7.22310, MAE = 3.44458, MAPE = 10.00507
Step 12 RMSE = 7.38596, MAE = 3.50550, MAPE = 10.20084
Inference time: 10.48 s
