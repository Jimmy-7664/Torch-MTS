METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 526,633
Trainable params: 526,633
Non-trainable params: 0
Total mult-adds (M): 33.70
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 7250.58
Params size (MB): 2.11
Estimated Total Size (MB): 7254.59
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 11:42:18.665270 Epoch 1  	Train Loss = 5.01258 Val Loss = 3.41834
2023-04-12 11:44:13.017646 Epoch 2  	Train Loss = 3.44404 Val Loss = 3.19902
2023-04-12 11:46:08.122035 Epoch 3  	Train Loss = 3.30079 Val Loss = 3.13307
2023-04-12 11:48:03.235551 Epoch 4  	Train Loss = 3.19397 Val Loss = 2.97906
2023-04-12 11:49:58.454344 Epoch 5  	Train Loss = 3.09757 Val Loss = 2.99442
2023-04-12 11:51:53.617254 Epoch 6  	Train Loss = 3.03057 Val Loss = 2.88342
2023-04-12 11:53:48.518737 Epoch 7  	Train Loss = 2.99292 Val Loss = 2.90215
2023-04-12 11:55:43.653571 Epoch 8  	Train Loss = 2.95912 Val Loss = 2.85317
2023-04-12 11:57:38.378538 Epoch 9  	Train Loss = 2.92664 Val Loss = 2.87028
2023-04-12 11:59:33.114422 Epoch 10  	Train Loss = 2.89414 Val Loss = 2.81803
2023-04-12 12:01:27.913420 Epoch 11  	Train Loss = 2.81518 Val Loss = 2.77983
2023-04-12 12:03:22.525839 Epoch 12  	Train Loss = 2.79875 Val Loss = 2.78316
2023-04-12 12:05:18.007822 Epoch 13  	Train Loss = 2.79113 Val Loss = 2.77838
2023-04-12 12:07:13.501918 Epoch 14  	Train Loss = 2.78420 Val Loss = 2.77500
2023-04-12 12:09:08.204285 Epoch 15  	Train Loss = 2.77735 Val Loss = 2.76960
2023-04-12 12:11:03.546478 Epoch 16  	Train Loss = 2.77118 Val Loss = 2.76714
2023-04-12 12:12:58.276203 Epoch 17  	Train Loss = 2.76389 Val Loss = 2.77885
2023-04-12 12:14:53.541516 Epoch 18  	Train Loss = 2.75927 Val Loss = 2.78218
2023-04-12 12:16:49.092797 Epoch 19  	Train Loss = 2.75273 Val Loss = 2.76830
2023-04-12 12:18:44.778396 Epoch 20  	Train Loss = 2.74675 Val Loss = 2.77332
2023-04-12 12:20:40.391372 Epoch 21  	Train Loss = 2.74105 Val Loss = 2.77089
2023-04-12 12:22:35.846647 Epoch 22  	Train Loss = 2.73435 Val Loss = 2.76709
2023-04-12 12:24:32.253709 Epoch 23  	Train Loss = 2.72972 Val Loss = 2.76902
2023-04-12 12:26:26.941592 Epoch 24  	Train Loss = 2.72467 Val Loss = 2.76616
2023-04-12 12:28:21.659823 Epoch 25  	Train Loss = 2.71976 Val Loss = 2.77193
2023-04-12 12:30:16.504024 Epoch 26  	Train Loss = 2.71389 Val Loss = 2.76969
2023-04-12 12:32:11.148397 Epoch 27  	Train Loss = 2.70948 Val Loss = 2.76906
2023-04-12 12:34:06.726077 Epoch 28  	Train Loss = 2.70456 Val Loss = 2.77198
2023-04-12 12:36:01.351485 Epoch 29  	Train Loss = 2.70154 Val Loss = 2.76813
2023-04-12 12:37:56.894210 Epoch 30  	Train Loss = 2.69575 Val Loss = 2.77145
2023-04-12 12:39:53.042305 Epoch 31  	Train Loss = 2.68066 Val Loss = 2.76438
2023-04-12 12:41:49.399708 Epoch 32  	Train Loss = 2.67843 Val Loss = 2.76283
2023-04-12 12:43:44.696181 Epoch 33  	Train Loss = 2.67777 Val Loss = 2.76287
2023-04-12 12:45:39.592375 Epoch 34  	Train Loss = 2.67626 Val Loss = 2.76223
2023-04-12 12:47:34.950068 Epoch 35  	Train Loss = 2.67650 Val Loss = 2.76011
2023-04-12 12:49:29.584894 Epoch 36  	Train Loss = 2.67581 Val Loss = 2.75960
2023-04-12 12:51:24.105924 Epoch 37  	Train Loss = 2.67536 Val Loss = 2.76002
2023-04-12 12:53:19.475641 Epoch 38  	Train Loss = 2.67453 Val Loss = 2.76312
2023-04-12 12:55:14.282681 Epoch 39  	Train Loss = 2.67389 Val Loss = 2.76159
2023-04-12 12:57:09.920908 Epoch 40  	Train Loss = 2.67326 Val Loss = 2.76054
2023-04-12 12:59:04.577567 Epoch 41  	Train Loss = 2.67264 Val Loss = 2.76278
2023-04-12 13:00:58.931679 Epoch 42  	Train Loss = 2.67163 Val Loss = 2.76281
2023-04-12 13:02:53.519963 Epoch 43  	Train Loss = 2.67144 Val Loss = 2.76429
2023-04-12 13:04:47.949273 Epoch 44  	Train Loss = 2.67083 Val Loss = 2.76386
2023-04-12 13:06:43.775472 Epoch 45  	Train Loss = 2.67055 Val Loss = 2.76261
2023-04-12 13:08:39.711063 Epoch 46  	Train Loss = 2.66960 Val Loss = 2.76420
Early stopping at epoch: 46
Best at epoch 36:
Train Loss = 2.67581
Train RMSE = 5.30753, MAE = 2.64599, MAPE = 6.89034
Val Loss = 2.75960
Val RMSE = 5.87498, MAE = 2.79965, MAPE = 7.71748
--------- Test ---------
All Steps RMSE = 6.15351, MAE = 2.99192, MAPE = 8.20498
Step 1 RMSE = 4.07378, MAE = 2.29100, MAPE = 5.51936
Step 2 RMSE = 4.73599, MAE = 2.52596, MAPE = 6.32018
Step 3 RMSE = 5.20336, MAE = 2.69499, MAPE = 6.95594
Step 4 RMSE = 5.60567, MAE = 2.82602, MAPE = 7.49057
Step 5 RMSE = 5.92372, MAE = 2.93477, MAPE = 7.92806
Step 6 RMSE = 6.19687, MAE = 3.03328, MAPE = 8.34137
Step 7 RMSE = 6.43668, MAE = 3.11415, MAPE = 8.67736
Step 8 RMSE = 6.60993, MAE = 3.18309, MAPE = 8.99073
Step 9 RMSE = 6.80833, MAE = 3.24894, MAPE = 9.25692
Step 10 RMSE = 6.96398, MAE = 3.29786, MAPE = 9.47049
Step 11 RMSE = 7.11191, MAE = 3.34707, MAPE = 9.65282
Step 12 RMSE = 7.26796, MAE = 3.40601, MAPE = 9.85618
Inference time: 10.38 s
