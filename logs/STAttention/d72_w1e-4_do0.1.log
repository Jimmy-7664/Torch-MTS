METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 36,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 72]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 72]         21,024
│    │    └─Dropout: 3-2                 [64, 207, 12, 72]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 72]         144
│    │    └─Sequential: 3-4              [64, 207, 12, 72]         37,192
│    │    └─Dropout: 3-5                 [64, 207, 12, 72]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 72]         144
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 72]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 72]         21,024
│    │    └─Dropout: 3-8                 [64, 207, 12, 72]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 72]         144
│    │    └─Sequential: 3-10             [64, 207, 12, 72]         37,192
│    │    └─Dropout: 3-11                [64, 207, 12, 72]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 72]         144
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 72]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 72]         21,024
│    │    └─Dropout: 3-14                [64, 207, 12, 72]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 72]         144
│    │    └─Sequential: 3-16             [64, 207, 12, 72]         37,192
│    │    └─Dropout: 3-17                [64, 207, 12, 72]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 72]         144
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 72]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 72]         21,024
│    │    └─Dropout: 3-20                [64, 12, 207, 72]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 72]         144
│    │    └─Sequential: 3-22             [64, 12, 207, 72]         37,192
│    │    └─Dropout: 3-23                [64, 12, 207, 72]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 72]         144
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 72]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 72]         21,024
│    │    └─Dropout: 3-26                [64, 12, 207, 72]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 72]         144
│    │    └─Sequential: 3-28             [64, 12, 207, 72]         37,192
│    │    └─Dropout: 3-29                [64, 12, 207, 72]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 72]         144
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 72]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 72]         21,024
│    │    └─Dropout: 3-32                [64, 12, 207, 72]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 72]         144
│    │    └─Sequential: 3-34             [64, 12, 207, 72]         37,192
│    │    └─Dropout: 3-35                [64, 12, 207, 72]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 72]         144
├─Linear: 1-6                            [64, 72, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          73
==========================================================================================
Total params: 354,817
Trainable params: 354,817
Non-trainable params: 0
Total mult-adds (M): 22.71
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 5938.07
Params size (MB): 1.42
Estimated Total Size (MB): 5941.40
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 20:10:43.729109 Epoch 1  	Train Loss = 4.68734 Val Loss = 3.37987
2023-04-12 20:12:36.303644 Epoch 2  	Train Loss = 3.48217 Val Loss = 3.22813
2023-04-12 20:14:29.152642 Epoch 3  	Train Loss = 3.35215 Val Loss = 3.14203
2023-04-12 20:16:19.254740 Epoch 4  	Train Loss = 3.27375 Val Loss = 3.05704
2023-04-12 20:18:09.982616 Epoch 5  	Train Loss = 3.17964 Val Loss = 3.03269
2023-04-12 20:20:01.069592 Epoch 6  	Train Loss = 3.10250 Val Loss = 2.98953
2023-04-12 20:21:53.756616 Epoch 7  	Train Loss = 3.04569 Val Loss = 2.94050
2023-04-12 20:23:47.378922 Epoch 8  	Train Loss = 3.00576 Val Loss = 2.95622
2023-04-12 20:25:41.221505 Epoch 9  	Train Loss = 2.97553 Val Loss = 2.88881
2023-04-12 20:27:35.188703 Epoch 10  	Train Loss = 2.94657 Val Loss = 2.88501
2023-04-12 20:29:27.362289 Epoch 11  	Train Loss = 2.87642 Val Loss = 2.83531
2023-04-12 20:31:17.656672 Epoch 12  	Train Loss = 2.86466 Val Loss = 2.83646
2023-04-12 20:33:08.221223 Epoch 13  	Train Loss = 2.85832 Val Loss = 2.82732
2023-04-12 20:34:59.075928 Epoch 14  	Train Loss = 2.85231 Val Loss = 2.83726
2023-04-12 20:36:50.209438 Epoch 15  	Train Loss = 2.84584 Val Loss = 2.82348
2023-04-12 20:38:41.197291 Epoch 16  	Train Loss = 2.83944 Val Loss = 2.82527
2023-04-12 20:40:30.693619 Epoch 17  	Train Loss = 2.83292 Val Loss = 2.81948
2023-04-12 20:42:19.304332 Epoch 18  	Train Loss = 2.82658 Val Loss = 2.81815
2023-04-12 20:44:10.822874 Epoch 19  	Train Loss = 2.82078 Val Loss = 2.80579
2023-04-12 20:46:05.679638 Epoch 20  	Train Loss = 2.81353 Val Loss = 2.81017
2023-04-12 20:48:02.346440 Epoch 21  	Train Loss = 2.80828 Val Loss = 2.79912
2023-04-12 20:50:00.488490 Epoch 22  	Train Loss = 2.80178 Val Loss = 2.79967
2023-04-12 20:51:56.451583 Epoch 23  	Train Loss = 2.79607 Val Loss = 2.80869
2023-04-12 20:53:49.844845 Epoch 24  	Train Loss = 2.79210 Val Loss = 2.80205
2023-04-12 20:55:43.532434 Epoch 25  	Train Loss = 2.78636 Val Loss = 2.79786
2023-04-12 20:57:37.998330 Epoch 26  	Train Loss = 2.78161 Val Loss = 2.79262
2023-04-12 20:59:33.780911 Epoch 27  	Train Loss = 2.77526 Val Loss = 2.79994
2023-04-12 21:01:30.310661 Epoch 28  	Train Loss = 2.77215 Val Loss = 2.79909
2023-04-12 21:03:28.025522 Epoch 29  	Train Loss = 2.76661 Val Loss = 2.79520
2023-04-12 21:05:25.661588 Epoch 30  	Train Loss = 2.76201 Val Loss = 2.79151
2023-04-12 21:07:19.371574 Epoch 31  	Train Loss = 2.74898 Val Loss = 2.78569
2023-04-12 21:09:12.838376 Epoch 32  	Train Loss = 2.74686 Val Loss = 2.78431
2023-04-12 21:11:06.952522 Epoch 33  	Train Loss = 2.74637 Val Loss = 2.78523
2023-04-12 21:13:02.115928 Epoch 34  	Train Loss = 2.74569 Val Loss = 2.78183
2023-04-12 21:14:57.867809 Epoch 35  	Train Loss = 2.74530 Val Loss = 2.78534
2023-04-12 21:16:54.369629 Epoch 36  	Train Loss = 2.74380 Val Loss = 2.78482
2023-04-12 21:18:51.898402 Epoch 37  	Train Loss = 2.74417 Val Loss = 2.78380
2023-04-12 21:20:47.769078 Epoch 38  	Train Loss = 2.74356 Val Loss = 2.78489
2023-04-12 21:22:40.995122 Epoch 39  	Train Loss = 2.74267 Val Loss = 2.78350
2023-04-12 21:24:34.519930 Epoch 40  	Train Loss = 2.74250 Val Loss = 2.78131
2023-04-12 21:26:28.773368 Epoch 41  	Train Loss = 2.74153 Val Loss = 2.78655
2023-04-12 21:28:23.915205 Epoch 42  	Train Loss = 2.74083 Val Loss = 2.78101
2023-04-12 21:30:19.680769 Epoch 43  	Train Loss = 2.74051 Val Loss = 2.78204
2023-04-12 21:32:16.221580 Epoch 44  	Train Loss = 2.74022 Val Loss = 2.78328
2023-04-12 21:34:11.386003 Epoch 45  	Train Loss = 2.73893 Val Loss = 2.78195
2023-04-12 21:36:04.090061 Epoch 46  	Train Loss = 2.73890 Val Loss = 2.78224
2023-04-12 21:37:56.715464 Epoch 47  	Train Loss = 2.73801 Val Loss = 2.78436
2023-04-12 21:39:50.130144 Epoch 48  	Train Loss = 2.73798 Val Loss = 2.78414
2023-04-12 21:41:45.309475 Epoch 49  	Train Loss = 2.73767 Val Loss = 2.78319
2023-04-12 21:43:42.514811 Epoch 50  	Train Loss = 2.73730 Val Loss = 2.78434
2023-04-12 21:45:39.794717 Epoch 51  	Train Loss = 2.73591 Val Loss = 2.78338
2023-04-12 21:47:36.550247 Epoch 52  	Train Loss = 2.73516 Val Loss = 2.78194
Early stopping at epoch: 52
Best at epoch 42:
Train Loss = 2.74083
Train RMSE = 5.48007, MAE = 2.71475, MAPE = 7.10377
Val Loss = 2.78101
Val RMSE = 5.91995, MAE = 2.81903, MAPE = 7.79580
--------- Test ---------
All Steps RMSE = 6.21347, MAE = 3.02387, MAPE = 8.32879
Step 1 RMSE = 4.10555, MAE = 2.31117, MAPE = 5.58585
Step 2 RMSE = 4.83784, MAE = 2.56275, MAPE = 6.45243
Step 3 RMSE = 5.31150, MAE = 2.74113, MAPE = 7.12039
Step 4 RMSE = 5.71068, MAE = 2.86662, MAPE = 7.64098
Step 5 RMSE = 6.02611, MAE = 2.97448, MAPE = 8.08052
Step 6 RMSE = 6.27600, MAE = 3.06408, MAPE = 8.45235
Step 7 RMSE = 6.49814, MAE = 3.15111, MAPE = 8.82877
Step 8 RMSE = 6.67145, MAE = 3.21316, MAPE = 9.09280
Step 9 RMSE = 6.86589, MAE = 3.27530, MAPE = 9.36520
Step 10 RMSE = 7.01160, MAE = 3.32678, MAPE = 9.57365
Step 11 RMSE = 7.12784, MAE = 3.37441, MAPE = 9.77967
Step 12 RMSE = 7.25667, MAE = 3.42553, MAPE = 9.97315
Inference time: 10.00 s
