METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.2
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 526,633
Trainable params: 526,633
Non-trainable params: 0
Total mult-adds (M): 33.70
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 7250.58
Params size (MB): 2.11
Estimated Total Size (MB): 7254.59
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 16:55:19.053267 Epoch 1  	Train Loss = 5.03358 Val Loss = 3.38774
2023-04-12 16:57:19.861063 Epoch 2  	Train Loss = 3.46841 Val Loss = 3.25322
2023-04-12 16:59:20.192920 Epoch 3  	Train Loss = 3.32715 Val Loss = 3.17222
2023-04-12 17:01:19.807841 Epoch 4  	Train Loss = 3.22501 Val Loss = 3.00003
2023-04-12 17:03:19.890077 Epoch 5  	Train Loss = 3.12541 Val Loss = 3.00886
2023-04-12 17:05:20.620525 Epoch 6  	Train Loss = 3.05479 Val Loss = 2.90073
2023-04-12 17:07:23.158501 Epoch 7  	Train Loss = 3.01596 Val Loss = 2.95760
2023-04-12 17:09:26.056612 Epoch 8  	Train Loss = 2.98278 Val Loss = 2.88048
2023-04-12 17:11:28.916836 Epoch 9  	Train Loss = 2.95200 Val Loss = 2.89028
2023-04-12 17:13:30.752584 Epoch 10  	Train Loss = 2.92312 Val Loss = 2.84413
2023-04-12 17:15:31.012650 Epoch 11  	Train Loss = 2.85110 Val Loss = 2.80794
2023-04-12 17:17:31.625837 Epoch 12  	Train Loss = 2.83508 Val Loss = 2.80679
2023-04-12 17:19:32.881106 Epoch 13  	Train Loss = 2.82837 Val Loss = 2.81275
2023-04-12 17:21:36.008466 Epoch 14  	Train Loss = 2.82106 Val Loss = 2.79927
2023-04-12 17:23:39.991038 Epoch 15  	Train Loss = 2.81388 Val Loss = 2.79111
2023-04-12 17:25:44.467460 Epoch 16  	Train Loss = 2.80790 Val Loss = 2.78459
2023-04-12 17:27:46.916755 Epoch 17  	Train Loss = 2.80064 Val Loss = 2.79866
2023-04-12 17:29:46.961823 Epoch 18  	Train Loss = 2.79594 Val Loss = 2.80173
2023-04-12 17:31:47.348762 Epoch 19  	Train Loss = 2.78931 Val Loss = 2.78046
2023-04-12 17:33:47.797454 Epoch 20  	Train Loss = 2.78307 Val Loss = 2.79969
2023-04-12 17:35:48.190041 Epoch 21  	Train Loss = 2.77716 Val Loss = 2.79428
2023-04-12 17:37:48.606709 Epoch 22  	Train Loss = 2.77139 Val Loss = 2.79370
2023-04-12 17:39:47.977341 Epoch 23  	Train Loss = 2.76650 Val Loss = 2.81151
2023-04-12 17:41:47.488944 Epoch 24  	Train Loss = 2.76047 Val Loss = 2.78447
2023-04-12 17:43:48.307607 Epoch 25  	Train Loss = 2.75587 Val Loss = 2.80196
2023-04-12 17:45:49.721525 Epoch 26  	Train Loss = 2.75001 Val Loss = 2.80295
2023-04-12 17:47:52.546207 Epoch 27  	Train Loss = 2.74614 Val Loss = 2.77622
2023-04-12 17:49:55.243429 Epoch 28  	Train Loss = 2.74109 Val Loss = 2.78956
2023-04-12 17:51:54.956313 Epoch 29  	Train Loss = 2.73623 Val Loss = 2.79589
2023-04-12 17:53:54.777846 Epoch 30  	Train Loss = 2.73195 Val Loss = 2.80107
2023-04-12 17:55:54.958206 Epoch 31  	Train Loss = 2.71702 Val Loss = 2.78394
2023-04-12 17:57:55.850503 Epoch 32  	Train Loss = 2.71514 Val Loss = 2.78354
2023-04-12 17:59:57.669300 Epoch 33  	Train Loss = 2.71385 Val Loss = 2.78493
2023-04-12 18:02:00.601273 Epoch 34  	Train Loss = 2.71326 Val Loss = 2.78312
2023-04-12 18:04:03.761044 Epoch 35  	Train Loss = 2.71322 Val Loss = 2.78278
2023-04-12 18:06:03.837056 Epoch 36  	Train Loss = 2.71307 Val Loss = 2.77929
2023-04-12 18:08:03.706539 Epoch 37  	Train Loss = 2.71229 Val Loss = 2.78104
Early stopping at epoch: 37
Best at epoch 27:
Train Loss = 2.74614
Train RMSE = 5.43140, MAE = 2.69854, MAPE = 6.99258
Val Loss = 2.77622
Val RMSE = 5.90659, MAE = 2.81523, MAPE = 7.65565
--------- Test ---------
All Steps RMSE = 6.17122, MAE = 3.00830, MAPE = 8.16486
Step 1 RMSE = 4.12043, MAE = 2.31594, MAPE = 5.55880
Step 2 RMSE = 4.80786, MAE = 2.55551, MAPE = 6.35556
Step 3 RMSE = 5.26044, MAE = 2.72043, MAPE = 6.97107
Step 4 RMSE = 5.64414, MAE = 2.84773, MAPE = 7.48552
Step 5 RMSE = 5.95187, MAE = 2.95561, MAPE = 7.91666
Step 6 RMSE = 6.21364, MAE = 3.04993, MAPE = 8.30634
Step 7 RMSE = 6.44181, MAE = 3.12734, MAPE = 8.62648
Step 8 RMSE = 6.61435, MAE = 3.19632, MAPE = 8.92133
Step 9 RMSE = 6.80203, MAE = 3.25670, MAPE = 9.16710
Step 10 RMSE = 6.95726, MAE = 3.30456, MAPE = 9.36964
Step 11 RMSE = 7.10943, MAE = 3.35342, MAPE = 9.54682
Step 12 RMSE = 7.27263, MAE = 3.41610, MAPE = 9.75332
Inference time: 10.91 s
