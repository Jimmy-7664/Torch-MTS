METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 48,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 84]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 84]         28,560
│    │    └─Dropout: 3-2                 [64, 207, 12, 84]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 84]         168
│    │    └─Sequential: 3-4              [64, 207, 12, 84]         43,348
│    │    └─Dropout: 3-5                 [64, 207, 12, 84]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 84]         168
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 84]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 84]         28,560
│    │    └─Dropout: 3-8                 [64, 207, 12, 84]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 84]         168
│    │    └─Sequential: 3-10             [64, 207, 12, 84]         43,348
│    │    └─Dropout: 3-11                [64, 207, 12, 84]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 84]         168
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 84]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 84]         28,560
│    │    └─Dropout: 3-14                [64, 207, 12, 84]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 84]         168
│    │    └─Sequential: 3-16             [64, 207, 12, 84]         43,348
│    │    └─Dropout: 3-17                [64, 207, 12, 84]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 84]         168
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 84]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 84]         28,560
│    │    └─Dropout: 3-20                [64, 12, 207, 84]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 84]         168
│    │    └─Sequential: 3-22             [64, 12, 207, 84]         43,348
│    │    └─Dropout: 3-23                [64, 12, 207, 84]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 84]         168
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 84]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 84]         28,560
│    │    └─Dropout: 3-26                [64, 12, 207, 84]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 84]         168
│    │    └─Sequential: 3-28             [64, 12, 207, 84]         43,348
│    │    └─Dropout: 3-29                [64, 12, 207, 84]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 84]         168
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 84]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 84]         28,560
│    │    └─Dropout: 3-32                [64, 12, 207, 84]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 84]         168
│    │    └─Sequential: 3-34             [64, 12, 207, 84]         43,348
│    │    └─Dropout: 3-35                [64, 12, 207, 84]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 84]         168
├─Linear: 1-6                            [64, 84, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          85
==========================================================================================
Total params: 437,269
Trainable params: 437,269
Non-trainable params: 0
Total mult-adds (M): 27.99
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 6594.32
Params size (MB): 1.75
Estimated Total Size (MB): 6597.98
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 20:10:59.331278 Epoch 1  	Train Loss = 5.09755 Val Loss = 3.41751
2023-04-12 20:12:52.075364 Epoch 2  	Train Loss = 3.48757 Val Loss = 3.26108
2023-04-12 20:14:45.211573 Epoch 3  	Train Loss = 3.37438 Val Loss = 3.14270
2023-04-12 20:16:38.043826 Epoch 4  	Train Loss = 3.28131 Val Loss = 3.03759
2023-04-12 20:18:30.889661 Epoch 5  	Train Loss = 3.17720 Val Loss = 2.99317
2023-04-12 20:20:23.626583 Epoch 6  	Train Loss = 3.09079 Val Loss = 2.93343
2023-04-12 20:22:16.415163 Epoch 7  	Train Loss = 3.03574 Val Loss = 2.92704
2023-04-12 20:24:09.364318 Epoch 8  	Train Loss = 2.99223 Val Loss = 2.88227
2023-04-12 20:26:02.317539 Epoch 9  	Train Loss = 2.96099 Val Loss = 2.87981
2023-04-12 20:27:55.536199 Epoch 10  	Train Loss = 2.93139 Val Loss = 2.87367
2023-04-12 20:29:48.116047 Epoch 11  	Train Loss = 2.85977 Val Loss = 2.81262
2023-04-12 20:31:40.732025 Epoch 12  	Train Loss = 2.84680 Val Loss = 2.80610
2023-04-12 20:33:33.368218 Epoch 13  	Train Loss = 2.83996 Val Loss = 2.80557
2023-04-12 20:35:25.944267 Epoch 14  	Train Loss = 2.83397 Val Loss = 2.80801
2023-04-12 20:37:18.639052 Epoch 15  	Train Loss = 2.82879 Val Loss = 2.80093
2023-04-12 20:39:11.182782 Epoch 16  	Train Loss = 2.82145 Val Loss = 2.80549
2023-04-12 20:41:03.376325 Epoch 17  	Train Loss = 2.81554 Val Loss = 2.79165
2023-04-12 20:42:55.397673 Epoch 18  	Train Loss = 2.80866 Val Loss = 2.79266
2023-04-12 20:44:46.929052 Epoch 19  	Train Loss = 2.80314 Val Loss = 2.79040
2023-04-12 20:46:38.807870 Epoch 20  	Train Loss = 2.79600 Val Loss = 2.78781
2023-04-12 20:48:30.999588 Epoch 21  	Train Loss = 2.79108 Val Loss = 2.78124
2023-04-12 20:50:23.767656 Epoch 22  	Train Loss = 2.78421 Val Loss = 2.77642
2023-04-12 20:52:16.066625 Epoch 23  	Train Loss = 2.77980 Val Loss = 2.78229
2023-04-12 20:54:07.869362 Epoch 24  	Train Loss = 2.77445 Val Loss = 2.76817
2023-04-12 20:55:59.737288 Epoch 25  	Train Loss = 2.76864 Val Loss = 2.77276
2023-04-12 20:57:51.621210 Epoch 26  	Train Loss = 2.76405 Val Loss = 2.76946
2023-04-12 20:59:43.811724 Epoch 27  	Train Loss = 2.75738 Val Loss = 2.76285
2023-04-12 21:01:36.361246 Epoch 28  	Train Loss = 2.75478 Val Loss = 2.76915
2023-04-12 21:03:28.896918 Epoch 29  	Train Loss = 2.75008 Val Loss = 2.76947
2023-04-12 21:05:21.713797 Epoch 30  	Train Loss = 2.74407 Val Loss = 2.76307
2023-04-12 21:07:13.946119 Epoch 31  	Train Loss = 2.73073 Val Loss = 2.76469
2023-04-12 21:09:05.618761 Epoch 32  	Train Loss = 2.72825 Val Loss = 2.76419
2023-04-12 21:10:57.318360 Epoch 33  	Train Loss = 2.72793 Val Loss = 2.76566
2023-04-12 21:12:49.304796 Epoch 34  	Train Loss = 2.72739 Val Loss = 2.76040
2023-04-12 21:14:41.449589 Epoch 35  	Train Loss = 2.72729 Val Loss = 2.76374
2023-04-12 21:16:33.590518 Epoch 36  	Train Loss = 2.72612 Val Loss = 2.76453
2023-04-12 21:18:25.935852 Epoch 37  	Train Loss = 2.72556 Val Loss = 2.76313
2023-04-12 21:20:18.336161 Epoch 38  	Train Loss = 2.72491 Val Loss = 2.76522
2023-04-12 21:22:09.922671 Epoch 39  	Train Loss = 2.72506 Val Loss = 2.75827
2023-04-12 21:24:01.347005 Epoch 40  	Train Loss = 2.72313 Val Loss = 2.76004
2023-04-12 21:25:52.773658 Epoch 41  	Train Loss = 2.72345 Val Loss = 2.76271
2023-04-12 21:27:44.495131 Epoch 42  	Train Loss = 2.72223 Val Loss = 2.75899
2023-04-12 21:29:36.418023 Epoch 43  	Train Loss = 2.72184 Val Loss = 2.76215
2023-04-12 21:31:28.548729 Epoch 44  	Train Loss = 2.72182 Val Loss = 2.75922
2023-04-12 21:33:20.042466 Epoch 45  	Train Loss = 2.72058 Val Loss = 2.76174
2023-04-12 21:35:11.587042 Epoch 46  	Train Loss = 2.72023 Val Loss = 2.76056
2023-04-12 21:37:02.631875 Epoch 47  	Train Loss = 2.71972 Val Loss = 2.76336
2023-04-12 21:38:53.678794 Epoch 48  	Train Loss = 2.71903 Val Loss = 2.76244
2023-04-12 21:40:44.915547 Epoch 49  	Train Loss = 2.71800 Val Loss = 2.75856
Early stopping at epoch: 49
Best at epoch 39:
Train Loss = 2.72506
Train RMSE = 5.42774, MAE = 2.69415, MAPE = 7.05123
Val Loss = 2.75827
Val RMSE = 5.85782, MAE = 2.79541, MAPE = 7.67487
--------- Test ---------
All Steps RMSE = 6.18203, MAE = 3.00972, MAPE = 8.26995
Step 1 RMSE = 4.08670, MAE = 2.30186, MAPE = 5.57361
Step 2 RMSE = 4.78701, MAE = 2.54594, MAPE = 6.39905
Step 3 RMSE = 5.27753, MAE = 2.71747, MAPE = 7.03821
Step 4 RMSE = 5.65412, MAE = 2.85061, MAPE = 7.58266
Step 5 RMSE = 5.99301, MAE = 2.96463, MAPE = 8.05936
Step 6 RMSE = 6.26855, MAE = 3.05131, MAPE = 8.41101
Step 7 RMSE = 6.46147, MAE = 3.13243, MAPE = 8.74127
Step 8 RMSE = 6.64427, MAE = 3.19805, MAPE = 9.03095
Step 9 RMSE = 6.82431, MAE = 3.26134, MAPE = 9.28746
Step 10 RMSE = 6.96532, MAE = 3.31323, MAPE = 9.51927
Step 11 RMSE = 7.09680, MAE = 3.36361, MAPE = 9.70974
Step 12 RMSE = 7.25358, MAE = 3.41620, MAPE = 9.88699
Inference time: 10.00 s
