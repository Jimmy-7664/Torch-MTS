PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- STAttention ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.002,
    "weight_decay": 0.0001,
    "milestones": [
        1,
        50,
        80
    ],
    "lr_decay_rate": 0.5,
    "batch_size": 64,
    "max_epochs": 300,
    "early_stop": 15,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 170, 12]         24
├─Embedding: 1-4                         [64, 12, 170, 12]         3,456
├─Embedding: 1-5                         [64, 12, 170, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 170, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 170, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 170, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 170, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 170, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 170, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 170, 12, 96]         49,504
│    │    └─Dropout: 3-11                [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 170, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 170, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 170, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 170, 12, 96]         49,504
│    │    └─Dropout: 3-17                [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 170, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 170, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 170, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 170, 96]         49,504
│    │    └─Dropout: 3-23                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 170, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 170, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 170, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 170, 96]         49,504
│    │    └─Dropout: 3-29                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 170, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 170, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 170, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 170, 96]         49,504
│    │    └─Dropout: 3-35                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 170, 96]         192
├─Linear: 1-6                            [64, 96, 170, 12]         156
├─Linear: 1-7                            [64, 12, 170, 1]          97
==========================================================================================
Total params: 526,633
Trainable params: 526,633
Non-trainable params: 0
Total mult-adds (M): 33.70
==========================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 5954.58
Params size (MB): 2.11
Estimated Total Size (MB): 5958.25
==========================================================================================

Loss: HuberLoss

2023-04-13 09:58:19.450944 Epoch 1  	Train Loss = 39.94598 Val Loss = 20.20770
2023-04-13 09:59:01.442626 Epoch 2  	Train Loss = 19.54042 Val Loss = 19.10164
2023-04-13 09:59:43.755875 Epoch 3  	Train Loss = 18.62260 Val Loss = 18.12508
2023-04-13 10:00:27.044729 Epoch 4  	Train Loss = 17.70191 Val Loss = 17.60924
2023-04-13 10:01:11.589245 Epoch 5  	Train Loss = 17.07856 Val Loss = 17.07951
2023-04-13 10:01:56.804374 Epoch 6  	Train Loss = 16.69362 Val Loss = 16.50328
2023-04-13 10:02:42.645560 Epoch 7  	Train Loss = 16.35789 Val Loss = 17.01570
2023-04-13 10:03:28.917056 Epoch 8  	Train Loss = 15.88816 Val Loss = 16.27508
2023-04-13 10:04:15.456676 Epoch 9  	Train Loss = 15.70294 Val Loss = 15.52501
2023-04-13 10:05:01.769678 Epoch 10  	Train Loss = 15.57166 Val Loss = 15.71863
2023-04-13 10:05:48.094734 Epoch 11  	Train Loss = 15.34379 Val Loss = 15.45270
2023-04-13 10:06:34.475551 Epoch 12  	Train Loss = 15.19090 Val Loss = 15.38080
2023-04-13 10:07:20.710597 Epoch 13  	Train Loss = 15.10999 Val Loss = 15.15094
2023-04-13 10:08:06.776666 Epoch 14  	Train Loss = 14.86525 Val Loss = 15.01605
2023-04-13 10:08:52.775654 Epoch 15  	Train Loss = 14.84461 Val Loss = 15.01742
2023-04-13 10:09:38.104518 Epoch 16  	Train Loss = 14.67488 Val Loss = 14.93759
2023-04-13 10:10:22.749534 Epoch 17  	Train Loss = 14.57084 Val Loss = 14.80755
2023-04-13 10:11:07.065004 Epoch 18  	Train Loss = 14.49237 Val Loss = 15.10832
2023-04-13 10:11:51.457830 Epoch 19  	Train Loss = 14.48556 Val Loss = 15.18920
2023-04-13 10:12:35.994574 Epoch 20  	Train Loss = 14.35891 Val Loss = 15.09187
2023-04-13 10:13:20.583797 Epoch 21  	Train Loss = 14.20188 Val Loss = 14.50447
2023-04-13 10:14:05.308885 Epoch 22  	Train Loss = 14.09650 Val Loss = 14.61727
2023-04-13 10:14:50.145908 Epoch 23  	Train Loss = 14.10266 Val Loss = 14.90091
2023-04-13 10:15:35.312521 Epoch 24  	Train Loss = 13.92422 Val Loss = 14.58546
2023-04-13 10:16:20.819492 Epoch 25  	Train Loss = 13.91094 Val Loss = 14.71531
2023-04-13 10:17:06.747357 Epoch 26  	Train Loss = 13.87188 Val Loss = 14.49125
2023-04-13 10:17:52.872656 Epoch 27  	Train Loss = 13.75939 Val Loss = 14.63448
2023-04-13 10:18:39.042676 Epoch 28  	Train Loss = 13.74986 Val Loss = 14.48461
2023-04-13 10:19:25.116434 Epoch 29  	Train Loss = 13.65019 Val Loss = 14.58290
2023-04-13 10:20:11.285639 Epoch 30  	Train Loss = 13.67898 Val Loss = 14.49785
2023-04-13 10:20:57.530175 Epoch 31  	Train Loss = 13.54729 Val Loss = 14.26822
2023-04-13 10:21:43.639251 Epoch 32  	Train Loss = 13.51295 Val Loss = 14.21249
2023-04-13 10:22:29.696439 Epoch 33  	Train Loss = 13.44115 Val Loss = 14.06172
2023-04-13 10:23:15.775851 Epoch 34  	Train Loss = 13.43344 Val Loss = 14.85084
2023-04-13 10:24:01.207177 Epoch 35  	Train Loss = 13.33045 Val Loss = 14.12357
2023-04-13 10:24:46.023441 Epoch 36  	Train Loss = 13.40651 Val Loss = 14.22201
2023-04-13 10:25:30.523408 Epoch 37  	Train Loss = 13.29729 Val Loss = 14.12849
2023-04-13 10:26:15.076210 Epoch 38  	Train Loss = 13.20990 Val Loss = 14.44788
2023-04-13 10:26:59.661406 Epoch 39  	Train Loss = 13.14817 Val Loss = 14.23522
2023-04-13 10:27:44.393853 Epoch 40  	Train Loss = 13.18082 Val Loss = 14.19423
2023-04-13 10:28:29.208878 Epoch 41  	Train Loss = 13.13615 Val Loss = 14.29584
2023-04-13 10:29:14.148164 Epoch 42  	Train Loss = 13.00621 Val Loss = 14.47091
2023-04-13 10:29:59.426141 Epoch 43  	Train Loss = 13.19175 Val Loss = 14.06073
2023-04-13 10:30:45.037527 Epoch 44  	Train Loss = 12.98480 Val Loss = 14.16479
2023-04-13 10:31:30.974693 Epoch 45  	Train Loss = 12.89842 Val Loss = 14.12465
2023-04-13 10:32:16.978582 Epoch 46  	Train Loss = 12.89319 Val Loss = 14.01018
2023-04-13 10:33:03.024020 Epoch 47  	Train Loss = 12.98900 Val Loss = 14.16424
2023-04-13 10:33:48.839642 Epoch 48  	Train Loss = 12.90341 Val Loss = 14.20892
2023-04-13 10:34:34.652497 Epoch 49  	Train Loss = 12.82360 Val Loss = 13.97223
2023-04-13 10:35:20.471683 Epoch 50  	Train Loss = 12.75897 Val Loss = 14.08378
2023-04-13 10:36:06.243269 Epoch 51  	Train Loss = 12.53768 Val Loss = 13.92739
2023-04-13 10:36:52.076850 Epoch 52  	Train Loss = 12.47752 Val Loss = 14.03699
2023-04-13 10:37:38.009610 Epoch 53  	Train Loss = 12.47396 Val Loss = 13.95403
2023-04-13 10:38:23.606115 Epoch 54  	Train Loss = 12.44425 Val Loss = 14.17347
2023-04-13 10:39:08.670084 Epoch 55  	Train Loss = 12.45434 Val Loss = 13.93520
2023-04-13 10:39:53.284100 Epoch 56  	Train Loss = 12.43267 Val Loss = 13.96052
2023-04-13 10:40:38.012881 Epoch 57  	Train Loss = 12.38718 Val Loss = 13.90605
2023-04-13 10:41:22.848283 Epoch 58  	Train Loss = 12.40504 Val Loss = 13.95851
2023-04-13 10:42:07.807215 Epoch 59  	Train Loss = 12.37337 Val Loss = 13.94634
2023-04-13 10:42:52.900981 Epoch 60  	Train Loss = 12.37601 Val Loss = 13.97348
2023-04-13 10:43:38.167786 Epoch 61  	Train Loss = 12.33501 Val Loss = 13.93784
2023-04-13 10:44:23.544671 Epoch 62  	Train Loss = 12.30356 Val Loss = 14.08409
2023-04-13 10:45:08.993157 Epoch 63  	Train Loss = 12.29870 Val Loss = 13.95328
2023-04-13 10:45:54.768117 Epoch 64  	Train Loss = 12.28474 Val Loss = 14.07958
2023-04-13 10:46:40.738988 Epoch 65  	Train Loss = 12.27479 Val Loss = 14.12152
2023-04-13 10:47:26.769590 Epoch 66  	Train Loss = 12.25072 Val Loss = 13.96240
2023-04-13 10:48:12.689965 Epoch 67  	Train Loss = 12.20260 Val Loss = 14.11022
2023-04-13 10:48:58.442574 Epoch 68  	Train Loss = 12.23323 Val Loss = 13.85092
2023-04-13 10:49:44.234289 Epoch 69  	Train Loss = 12.18847 Val Loss = 14.00231
2023-04-13 10:50:29.857102 Epoch 70  	Train Loss = 12.13541 Val Loss = 14.11635
2023-04-13 10:51:15.430245 Epoch 71  	Train Loss = 12.15076 Val Loss = 14.12972
2023-04-13 10:52:00.977667 Epoch 72  	Train Loss = 12.13241 Val Loss = 13.90161
2023-04-13 10:52:45.998003 Epoch 73  	Train Loss = 12.09648 Val Loss = 14.08071
2023-04-13 10:53:30.251384 Epoch 74  	Train Loss = 12.08269 Val Loss = 14.08859
2023-04-13 10:54:14.090616 Epoch 75  	Train Loss = 12.05459 Val Loss = 13.96474
2023-04-13 10:54:57.907736 Epoch 76  	Train Loss = 12.05583 Val Loss = 13.95458
2023-04-13 10:55:41.985722 Epoch 77  	Train Loss = 12.05756 Val Loss = 13.94815
2023-04-13 10:56:26.308770 Epoch 78  	Train Loss = 12.03825 Val Loss = 14.01728
2023-04-13 10:57:10.811178 Epoch 79  	Train Loss = 12.02556 Val Loss = 13.96850
2023-04-13 10:57:55.533040 Epoch 80  	Train Loss = 11.98124 Val Loss = 14.04774
2023-04-13 10:58:40.505638 Epoch 81  	Train Loss = 11.83675 Val Loss = 13.92342
2023-04-13 10:59:25.747745 Epoch 82  	Train Loss = 11.80281 Val Loss = 13.94567
2023-04-13 11:00:11.165466 Epoch 83  	Train Loss = 11.80224 Val Loss = 13.98050
Early stopping at epoch: 83
Best at epoch 68:
Train Loss = 12.23323
Train RMSE = 20.72610, MAE = 12.12544, MAPE = 8.45108
Val Loss = 13.85092
Val RMSE = 25.48594, MAE = 14.41064, MAPE = 10.86356
--------- Test ---------
All Steps RMSE = 25.10886, MAE = 14.50821, MAPE = 9.71877
Step 1 RMSE = 21.04572, MAE = 13.02693, MAPE = 8.80993
Step 2 RMSE = 21.90062, MAE = 13.19197, MAPE = 8.95501
Step 3 RMSE = 22.84197, MAE = 13.60061, MAPE = 9.20096
Step 4 RMSE = 23.69566, MAE = 13.92242, MAPE = 9.33753
Step 5 RMSE = 24.45643, MAE = 14.20643, MAPE = 9.44155
Step 6 RMSE = 25.20862, MAE = 14.69629, MAPE = 9.83303
Step 7 RMSE = 25.76521, MAE = 14.70798, MAPE = 9.81662
Step 8 RMSE = 26.40371, MAE = 14.94666, MAPE = 9.97860
Step 9 RMSE = 26.80689, MAE = 15.10564, MAPE = 10.08416
Step 10 RMSE = 27.15001, MAE = 15.30856, MAPE = 10.19120
Step 11 RMSE = 27.31127, MAE = 15.52787, MAPE = 10.46771
Step 12 RMSE = 27.63017, MAE = 15.85716, MAPE = 10.50900
Inference time: 4.50 s
