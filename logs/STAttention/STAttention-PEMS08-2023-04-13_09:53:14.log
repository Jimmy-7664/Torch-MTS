PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- STAttention ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        60
    ],
    "batch_size": 64,
    "max_epochs": 300,
    "early_stop": 15,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 170, 12]         24
├─Embedding: 1-4                         [64, 12, 170, 12]         3,456
├─Embedding: 1-5                         [64, 12, 170, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 170, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 170, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 170, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 170, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 170, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 170, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 170, 12, 96]         49,504
│    │    └─Dropout: 3-11                [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 170, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 170, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 170, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 170, 12, 96]         49,504
│    │    └─Dropout: 3-17                [64, 170, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 170, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 170, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 170, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 170, 96]         49,504
│    │    └─Dropout: 3-23                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 170, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 170, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 170, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 170, 96]         49,504
│    │    └─Dropout: 3-29                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 170, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 170, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 170, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 170, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 170, 96]         49,504
│    │    └─Dropout: 3-35                [64, 12, 170, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 170, 96]         192
├─Linear: 1-6                            [64, 96, 170, 12]         156
├─Linear: 1-7                            [64, 12, 170, 1]          97
==========================================================================================
Total params: 526,633
Trainable params: 526,633
Non-trainable params: 0
Total mult-adds (M): 33.70
==========================================================================================
Input size (MB): 1.57
Forward/backward pass size (MB): 5954.58
Params size (MB): 2.11
Estimated Total Size (MB): 5958.25
==========================================================================================

Loss: HuberLoss

2023-04-13 09:53:59.299589 Epoch 1  	Train Loss = 45.57818 Val Loss = 22.01403
2023-04-13 09:54:40.953923 Epoch 2  	Train Loss = 20.11544 Val Loss = 19.72481
2023-04-13 09:55:24.074495 Epoch 3  	Train Loss = 19.25329 Val Loss = 18.72031
2023-04-13 09:56:06.917493 Epoch 4  	Train Loss = 18.22728 Val Loss = 17.79933
2023-04-13 09:56:50.017374 Epoch 5  	Train Loss = 17.50028 Val Loss = 17.13705
2023-04-13 09:57:34.243337 Epoch 6  	Train Loss = 17.03835 Val Loss = 16.70813
2023-04-13 09:58:17.859892 Epoch 7  	Train Loss = 16.60022 Val Loss = 16.75549
2023-04-13 09:59:01.754766 Epoch 8  	Train Loss = 16.16906 Val Loss = 16.14885
2023-04-13 09:59:45.199520 Epoch 9  	Train Loss = 15.98519 Val Loss = 15.72815
2023-04-13 10:00:28.779148 Epoch 10  	Train Loss = 15.66575 Val Loss = 15.66203
2023-04-13 10:01:12.130720 Epoch 11  	Train Loss = 15.26251 Val Loss = 15.33669
2023-04-13 10:01:55.460631 Epoch 12  	Train Loss = 15.20718 Val Loss = 15.28499
2023-04-13 10:02:38.712698 Epoch 13  	Train Loss = 15.17601 Val Loss = 15.21429
2023-04-13 10:03:22.706895 Epoch 14  	Train Loss = 15.13803 Val Loss = 15.28606
2023-04-13 10:04:07.088961 Epoch 15  	Train Loss = 15.13702 Val Loss = 15.18899
2023-04-13 10:04:50.460243 Epoch 16  	Train Loss = 15.09818 Val Loss = 15.19693
2023-04-13 10:05:33.813433 Epoch 17  	Train Loss = 15.06173 Val Loss = 15.16781
2023-04-13 10:06:17.245500 Epoch 18  	Train Loss = 15.02122 Val Loss = 15.12980
2023-04-13 10:07:00.559144 Epoch 19  	Train Loss = 14.98630 Val Loss = 15.15590
2023-04-13 10:07:44.191751 Epoch 20  	Train Loss = 14.95950 Val Loss = 15.13537
2023-04-13 10:08:27.516424 Epoch 21  	Train Loss = 14.95034 Val Loss = 15.08032
2023-04-13 10:09:11.780221 Epoch 22  	Train Loss = 14.89390 Val Loss = 15.08088
2023-04-13 10:09:55.246638 Epoch 23  	Train Loss = 14.85919 Val Loss = 15.05824
2023-04-13 10:10:39.625509 Epoch 24  	Train Loss = 14.82400 Val Loss = 15.00975
2023-04-13 10:11:23.414165 Epoch 25  	Train Loss = 14.79427 Val Loss = 14.92984
2023-04-13 10:12:06.594171 Epoch 26  	Train Loss = 14.77831 Val Loss = 14.86454
2023-04-13 10:12:49.793082 Epoch 27  	Train Loss = 14.72965 Val Loss = 14.90545
2023-04-13 10:13:32.994968 Epoch 28  	Train Loss = 14.71457 Val Loss = 15.06361
2023-04-13 10:14:16.218192 Epoch 29  	Train Loss = 14.69452 Val Loss = 14.93191
2023-04-13 10:14:59.432507 Epoch 30  	Train Loss = 14.66143 Val Loss = 14.84571
2023-04-13 10:15:42.671764 Epoch 31  	Train Loss = 14.60252 Val Loss = 14.80016
2023-04-13 10:16:25.937276 Epoch 32  	Train Loss = 14.58942 Val Loss = 14.75330
2023-04-13 10:17:09.211083 Epoch 33  	Train Loss = 14.57096 Val Loss = 14.82517
2023-04-13 10:17:52.505963 Epoch 34  	Train Loss = 14.54127 Val Loss = 14.74675
2023-04-13 10:18:35.833115 Epoch 35  	Train Loss = 14.50806 Val Loss = 14.67312
2023-04-13 10:19:19.599486 Epoch 36  	Train Loss = 14.48314 Val Loss = 14.73064
2023-04-13 10:20:03.352497 Epoch 37  	Train Loss = 14.44969 Val Loss = 14.75892
2023-04-13 10:20:46.707560 Epoch 38  	Train Loss = 14.41049 Val Loss = 14.81155
2023-04-13 10:21:30.033527 Epoch 39  	Train Loss = 14.40322 Val Loss = 14.64181
2023-04-13 10:22:13.336491 Epoch 40  	Train Loss = 14.36123 Val Loss = 14.59324
2023-04-13 10:22:56.657082 Epoch 41  	Train Loss = 14.34796 Val Loss = 14.64441
2023-04-13 10:23:39.971377 Epoch 42  	Train Loss = 14.31328 Val Loss = 14.63942
2023-04-13 10:24:23.199652 Epoch 43  	Train Loss = 14.29623 Val Loss = 14.54286
2023-04-13 10:25:06.429284 Epoch 44  	Train Loss = 14.25501 Val Loss = 14.55824
2023-04-13 10:25:49.588254 Epoch 45  	Train Loss = 14.24020 Val Loss = 14.51409
2023-04-13 10:26:32.776829 Epoch 46  	Train Loss = 14.21255 Val Loss = 14.53039
2023-04-13 10:27:15.937299 Epoch 47  	Train Loss = 14.19706 Val Loss = 14.50862
2023-04-13 10:27:59.123301 Epoch 48  	Train Loss = 14.16983 Val Loss = 14.49202
2023-04-13 10:28:42.309597 Epoch 49  	Train Loss = 14.15666 Val Loss = 14.45825
2023-04-13 10:29:25.532262 Epoch 50  	Train Loss = 14.11198 Val Loss = 14.45536
2023-04-13 10:30:08.707523 Epoch 51  	Train Loss = 14.11032 Val Loss = 14.46037
2023-04-13 10:30:51.902125 Epoch 52  	Train Loss = 14.07626 Val Loss = 14.43783
2023-04-13 10:31:35.213142 Epoch 53  	Train Loss = 14.06730 Val Loss = 14.45713
2023-04-13 10:32:18.511206 Epoch 54  	Train Loss = 14.04317 Val Loss = 14.39940
2023-04-13 10:33:01.789865 Epoch 55  	Train Loss = 14.02050 Val Loss = 14.46759
2023-04-13 10:33:45.075804 Epoch 56  	Train Loss = 13.99599 Val Loss = 14.36536
2023-04-13 10:34:28.351936 Epoch 57  	Train Loss = 13.98704 Val Loss = 14.43012
2023-04-13 10:35:11.622254 Epoch 58  	Train Loss = 13.97133 Val Loss = 14.33125
2023-04-13 10:35:54.871735 Epoch 59  	Train Loss = 13.94918 Val Loss = 14.40065
2023-04-13 10:36:38.151384 Epoch 60  	Train Loss = 13.91897 Val Loss = 14.34356
2023-04-13 10:37:21.446007 Epoch 61  	Train Loss = 13.85235 Val Loss = 14.30721
2023-04-13 10:38:04.686689 Epoch 62  	Train Loss = 13.86091 Val Loss = 14.31424
2023-04-13 10:38:47.907260 Epoch 63  	Train Loss = 13.85013 Val Loss = 14.31653
2023-04-13 10:39:31.062130 Epoch 64  	Train Loss = 13.85861 Val Loss = 14.30224
2023-04-13 10:40:14.233445 Epoch 65  	Train Loss = 13.85504 Val Loss = 14.29948
2023-04-13 10:40:57.388229 Epoch 66  	Train Loss = 13.84571 Val Loss = 14.32426
2023-04-13 10:41:40.558695 Epoch 67  	Train Loss = 13.84138 Val Loss = 14.29607
2023-04-13 10:42:23.724409 Epoch 68  	Train Loss = 13.85334 Val Loss = 14.30432
2023-04-13 10:43:06.872519 Epoch 69  	Train Loss = 13.83699 Val Loss = 14.29691
2023-04-13 10:43:50.042128 Epoch 70  	Train Loss = 13.83922 Val Loss = 14.31120
2023-04-13 10:44:33.237791 Epoch 71  	Train Loss = 13.83496 Val Loss = 14.29798
2023-04-13 10:45:16.446596 Epoch 72  	Train Loss = 13.84107 Val Loss = 14.30667
2023-04-13 10:45:59.667023 Epoch 73  	Train Loss = 13.82801 Val Loss = 14.30537
2023-04-13 10:46:42.960884 Epoch 74  	Train Loss = 13.82256 Val Loss = 14.29461
2023-04-13 10:47:26.300660 Epoch 75  	Train Loss = 13.81779 Val Loss = 14.29406
2023-04-13 10:48:09.587008 Epoch 76  	Train Loss = 13.82836 Val Loss = 14.29703
2023-04-13 10:48:52.887871 Epoch 77  	Train Loss = 13.81794 Val Loss = 14.30745
2023-04-13 10:49:36.182912 Epoch 78  	Train Loss = 13.82485 Val Loss = 14.29695
2023-04-13 10:50:19.438956 Epoch 79  	Train Loss = 13.82089 Val Loss = 14.29245
2023-04-13 10:51:02.649901 Epoch 80  	Train Loss = 13.80708 Val Loss = 14.27124
2023-04-13 10:51:45.849531 Epoch 81  	Train Loss = 13.80572 Val Loss = 14.29742
2023-04-13 10:52:29.036973 Epoch 82  	Train Loss = 13.79584 Val Loss = 14.28019
2023-04-13 10:53:12.190181 Epoch 83  	Train Loss = 13.80396 Val Loss = 14.27769
2023-04-13 10:53:55.262286 Epoch 84  	Train Loss = 13.80900 Val Loss = 14.26528
2023-04-13 10:54:38.359896 Epoch 85  	Train Loss = 13.79903 Val Loss = 14.28547
2023-04-13 10:55:21.503917 Epoch 86  	Train Loss = 13.78927 Val Loss = 14.28003
2023-04-13 10:56:04.575503 Epoch 87  	Train Loss = 13.79078 Val Loss = 14.28646
2023-04-13 10:56:47.699143 Epoch 88  	Train Loss = 13.79140 Val Loss = 14.28245
2023-04-13 10:57:30.802879 Epoch 89  	Train Loss = 13.78206 Val Loss = 14.27825
2023-04-13 10:58:13.952315 Epoch 90  	Train Loss = 13.78837 Val Loss = 14.27031
2023-04-13 10:58:57.088976 Epoch 91  	Train Loss = 13.77597 Val Loss = 14.26257
2023-04-13 10:59:40.252469 Epoch 92  	Train Loss = 13.78094 Val Loss = 14.26376
2023-04-13 11:00:23.385071 Epoch 93  	Train Loss = 13.77031 Val Loss = 14.26507
2023-04-13 11:01:06.549736 Epoch 94  	Train Loss = 13.77480 Val Loss = 14.25863
2023-04-13 11:01:49.739067 Epoch 95  	Train Loss = 13.77467 Val Loss = 14.26440
2023-04-13 11:02:32.946280 Epoch 96  	Train Loss = 13.75779 Val Loss = 14.26909
2023-04-13 11:03:16.140530 Epoch 97  	Train Loss = 13.77525 Val Loss = 14.25752
2023-04-13 11:03:59.251299 Epoch 98  	Train Loss = 13.76058 Val Loss = 14.25605
2023-04-13 11:04:42.378014 Epoch 99  	Train Loss = 13.75555 Val Loss = 14.24835
2023-04-13 11:05:25.466075 Epoch 100  	Train Loss = 13.76211 Val Loss = 14.25282
2023-04-13 11:06:08.548553 Epoch 101  	Train Loss = 13.75395 Val Loss = 14.26809
2023-04-13 11:06:51.634890 Epoch 102  	Train Loss = 13.74319 Val Loss = 14.24287
2023-04-13 11:07:34.713337 Epoch 103  	Train Loss = 13.75311 Val Loss = 14.23967
2023-04-13 11:08:17.793680 Epoch 104  	Train Loss = 13.75263 Val Loss = 14.24097
2023-04-13 11:09:00.747885 Epoch 105  	Train Loss = 13.73320 Val Loss = 14.23170
2023-04-13 11:09:43.770443 Epoch 106  	Train Loss = 13.75938 Val Loss = 14.24720
2023-04-13 11:10:26.798681 Epoch 107  	Train Loss = 13.73626 Val Loss = 14.25219
2023-04-13 11:11:09.804585 Epoch 108  	Train Loss = 13.74589 Val Loss = 14.24291
2023-04-13 11:11:52.851949 Epoch 109  	Train Loss = 13.73635 Val Loss = 14.25603
2023-04-13 11:12:35.904764 Epoch 110  	Train Loss = 13.73269 Val Loss = 14.25551
2023-04-13 11:13:18.961116 Epoch 111  	Train Loss = 13.73054 Val Loss = 14.24199
2023-04-13 11:14:02.017608 Epoch 112  	Train Loss = 13.72510 Val Loss = 14.22576
2023-04-13 11:14:45.074589 Epoch 113  	Train Loss = 13.72842 Val Loss = 14.23283
2023-04-13 11:15:28.161623 Epoch 114  	Train Loss = 13.72316 Val Loss = 14.24113
2023-04-13 11:16:11.242895 Epoch 115  	Train Loss = 13.71629 Val Loss = 14.24440
2023-04-13 11:16:54.328273 Epoch 116  	Train Loss = 13.71168 Val Loss = 14.22563
2023-04-13 11:17:37.456741 Epoch 117  	Train Loss = 13.71848 Val Loss = 14.23703
2023-04-13 11:18:20.549602 Epoch 118  	Train Loss = 13.70807 Val Loss = 14.22879
2023-04-13 11:19:03.971340 Epoch 119  	Train Loss = 13.71149 Val Loss = 14.22653
2023-04-13 11:19:47.746927 Epoch 120  	Train Loss = 13.71000 Val Loss = 14.22361
2023-04-13 11:20:30.831579 Epoch 121  	Train Loss = 13.70381 Val Loss = 14.22234
2023-04-13 11:21:13.931181 Epoch 122  	Train Loss = 13.70466 Val Loss = 14.23378
2023-04-13 11:21:57.033793 Epoch 123  	Train Loss = 13.69656 Val Loss = 14.23037
2023-04-13 11:22:40.126988 Epoch 124  	Train Loss = 13.69559 Val Loss = 14.22801
2023-04-13 11:23:24.055494 Epoch 125  	Train Loss = 13.70181 Val Loss = 14.23901
2023-04-13 11:24:07.137565 Epoch 126  	Train Loss = 13.69237 Val Loss = 14.21847
2023-04-13 11:24:50.178363 Epoch 127  	Train Loss = 13.69412 Val Loss = 14.20973
2023-04-13 11:25:33.156617 Epoch 128  	Train Loss = 13.68973 Val Loss = 14.22759
2023-04-13 11:26:16.193386 Epoch 129  	Train Loss = 13.68473 Val Loss = 14.19339
2023-04-13 11:26:59.203294 Epoch 130  	Train Loss = 13.68471 Val Loss = 14.22097
2023-04-13 11:27:42.159273 Epoch 131  	Train Loss = 13.68315 Val Loss = 14.21937
2023-04-13 11:28:25.134154 Epoch 132  	Train Loss = 13.68106 Val Loss = 14.21710
2023-04-13 11:29:08.104519 Epoch 133  	Train Loss = 13.67674 Val Loss = 14.20633
2023-04-13 11:29:51.076099 Epoch 134  	Train Loss = 13.67551 Val Loss = 14.19888
2023-04-13 11:30:34.064834 Epoch 135  	Train Loss = 13.66631 Val Loss = 14.20460
2023-04-13 11:31:17.055222 Epoch 136  	Train Loss = 13.67596 Val Loss = 14.20961
2023-04-13 11:32:00.041851 Epoch 137  	Train Loss = 13.66721 Val Loss = 14.21277
2023-04-13 11:32:43.007613 Epoch 138  	Train Loss = 13.66215 Val Loss = 14.18090
2023-04-13 11:33:26.017521 Epoch 139  	Train Loss = 13.65800 Val Loss = 14.21142
2023-04-13 11:34:08.996352 Epoch 140  	Train Loss = 13.65825 Val Loss = 14.18723
2023-04-13 11:34:52.009707 Epoch 141  	Train Loss = 13.65844 Val Loss = 14.19571
2023-04-13 11:35:35.019806 Epoch 142  	Train Loss = 13.66041 Val Loss = 14.19957
2023-04-13 11:36:18.031031 Epoch 143  	Train Loss = 13.66077 Val Loss = 14.18321
2023-04-13 11:37:01.100659 Epoch 144  	Train Loss = 13.65063 Val Loss = 14.20122
2023-04-13 11:37:44.126702 Epoch 145  	Train Loss = 13.63365 Val Loss = 14.18314
2023-04-13 11:38:27.193174 Epoch 146  	Train Loss = 13.64722 Val Loss = 14.20646
2023-04-13 11:39:10.242015 Epoch 147  	Train Loss = 13.65430 Val Loss = 14.19446
2023-04-13 11:39:53.246431 Epoch 148  	Train Loss = 13.63510 Val Loss = 14.20076
2023-04-13 11:40:36.349301 Epoch 149  	Train Loss = 13.63655 Val Loss = 14.20526
2023-04-13 11:41:20.363514 Epoch 150  	Train Loss = 13.63516 Val Loss = 14.20110
2023-04-13 11:42:03.447398 Epoch 151  	Train Loss = 13.62795 Val Loss = 14.19606
2023-04-13 11:42:47.707992 Epoch 152  	Train Loss = 13.64219 Val Loss = 14.18662
2023-04-13 11:43:31.954045 Epoch 153  	Train Loss = 13.61333 Val Loss = 14.19937
Early stopping at epoch: 153
Best at epoch 138:
Train Loss = 13.66215
Train RMSE = 23.56646, MAE = 13.90455, MAPE = 9.22692
Val Loss = 14.18090
Val RMSE = 24.52796, MAE = 14.64482, MAPE = 10.69111
--------- Test ---------
All Steps RMSE = 23.52313, MAE = 14.47529, MAPE = 9.54654
Step 1 RMSE = 20.65495, MAE = 13.19580, MAPE = 8.78550
Step 2 RMSE = 21.42403, MAE = 13.44528, MAPE = 8.86102
Step 3 RMSE = 22.03502, MAE = 13.68475, MAPE = 8.98592
Step 4 RMSE = 22.61031, MAE = 13.99704, MAPE = 9.27465
Step 5 RMSE = 23.13735, MAE = 14.29300, MAPE = 9.40536
Step 6 RMSE = 23.51957, MAE = 14.46942, MAPE = 9.51128
Step 7 RMSE = 23.81037, MAE = 14.54265, MAPE = 9.58444
Step 8 RMSE = 24.23143, MAE = 14.85112, MAPE = 9.81876
Step 9 RMSE = 24.51187, MAE = 14.94472, MAPE = 9.92124
Step 10 RMSE = 24.86978, MAE = 15.19635, MAPE = 9.98975
Step 11 RMSE = 25.19078, MAE = 15.35579, MAPE = 10.13407
Step 12 RMSE = 25.71296, MAE = 15.72759, MAPE = 10.28659
Inference time: 4.23 s
