METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 2,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         24,800
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         24,800
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-14                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-15              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-16             [64, 12, 207, 96]         24,800
│    │    └─Dropout: 3-17                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-18              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         24,800
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 253,545
Trainable params: 253,545
Non-trainable params: 0
Total mult-adds (M): 16.23
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 4238.94
Params size (MB): 1.01
Estimated Total Size (MB): 4241.86
==========================================================================================

Loss: MaskedMAELoss

2023-04-13 16:00:19.572529 Epoch 1  	Train Loss = 4.73563 Val Loss = 3.48279
2023-04-13 16:01:30.342119 Epoch 2  	Train Loss = 3.50311 Val Loss = 3.25607
2023-04-13 16:02:41.334216 Epoch 3  	Train Loss = 3.33635 Val Loss = 3.15647
2023-04-13 16:03:51.886379 Epoch 4  	Train Loss = 3.22796 Val Loss = 3.03894
2023-04-13 16:05:02.821767 Epoch 5  	Train Loss = 3.12950 Val Loss = 2.96394
2023-04-13 16:06:13.882910 Epoch 6  	Train Loss = 3.06474 Val Loss = 2.97049
2023-04-13 16:07:24.895614 Epoch 7  	Train Loss = 3.02373 Val Loss = 2.89216
2023-04-13 16:08:35.935059 Epoch 8  	Train Loss = 2.98490 Val Loss = 2.90670
2023-04-13 16:09:46.804627 Epoch 9  	Train Loss = 2.96182 Val Loss = 2.87432
2023-04-13 16:10:57.724957 Epoch 10  	Train Loss = 2.94117 Val Loss = 2.91480
2023-04-13 16:12:08.750755 Epoch 11  	Train Loss = 2.87543 Val Loss = 2.84793
2023-04-13 16:13:19.779625 Epoch 12  	Train Loss = 2.86342 Val Loss = 2.84051
2023-04-13 16:14:30.614817 Epoch 13  	Train Loss = 2.85843 Val Loss = 2.84659
2023-04-13 16:15:41.579307 Epoch 14  	Train Loss = 2.85311 Val Loss = 2.84339
2023-04-13 16:16:52.416236 Epoch 15  	Train Loss = 2.84789 Val Loss = 2.84558
2023-04-13 16:18:03.324753 Epoch 16  	Train Loss = 2.84337 Val Loss = 2.83843
2023-04-13 16:19:14.464727 Epoch 17  	Train Loss = 2.83874 Val Loss = 2.83874
2023-04-13 16:20:25.430416 Epoch 18  	Train Loss = 2.83359 Val Loss = 2.82528
2023-04-13 16:21:36.324197 Epoch 19  	Train Loss = 2.82936 Val Loss = 2.83731
2023-04-13 16:22:47.407803 Epoch 20  	Train Loss = 2.82469 Val Loss = 2.84576
2023-04-13 16:23:58.340683 Epoch 21  	Train Loss = 2.82083 Val Loss = 2.82722
2023-04-13 16:25:09.440449 Epoch 22  	Train Loss = 2.81562 Val Loss = 2.83587
2023-04-13 16:26:20.362746 Epoch 23  	Train Loss = 2.81171 Val Loss = 2.82702
2023-04-13 16:27:31.332025 Epoch 24  	Train Loss = 2.80750 Val Loss = 2.82957
2023-04-13 16:28:42.352269 Epoch 25  	Train Loss = 2.80380 Val Loss = 2.82548
2023-04-13 16:29:53.386649 Epoch 26  	Train Loss = 2.79887 Val Loss = 2.84082
2023-04-13 16:31:04.401649 Epoch 27  	Train Loss = 2.79582 Val Loss = 2.82866
2023-04-13 16:32:15.500821 Epoch 28  	Train Loss = 2.79276 Val Loss = 2.83051
2023-04-13 16:33:25.649941 Epoch 29  	Train Loss = 2.78871 Val Loss = 2.82610
2023-04-13 16:34:36.691238 Epoch 30  	Train Loss = 2.78455 Val Loss = 2.83239
2023-04-13 16:35:47.631135 Epoch 31  	Train Loss = 2.78144 Val Loss = 2.83703
2023-04-13 16:36:58.730011 Epoch 32  	Train Loss = 2.77792 Val Loss = 2.83306
2023-04-13 16:38:09.809183 Epoch 33  	Train Loss = 2.77452 Val Loss = 2.82896
2023-04-13 16:39:20.836702 Epoch 34  	Train Loss = 2.77063 Val Loss = 2.84295
2023-04-13 16:40:31.882946 Epoch 35  	Train Loss = 2.76810 Val Loss = 2.83506
2023-04-13 16:41:42.887257 Epoch 36  	Train Loss = 2.76400 Val Loss = 2.82141
2023-04-13 16:42:53.875929 Epoch 37  	Train Loss = 2.76154 Val Loss = 2.82944
2023-04-13 16:44:04.908828 Epoch 38  	Train Loss = 2.75740 Val Loss = 2.83197
2023-04-13 16:45:16.000693 Epoch 39  	Train Loss = 2.75469 Val Loss = 2.82618
2023-04-13 16:46:27.063531 Epoch 40  	Train Loss = 2.75283 Val Loss = 2.84384
2023-04-13 16:47:38.147200 Epoch 41  	Train Loss = 2.74962 Val Loss = 2.83204
2023-04-13 16:48:49.177267 Epoch 42  	Train Loss = 2.74685 Val Loss = 2.84286
2023-04-13 16:50:00.192063 Epoch 43  	Train Loss = 2.74325 Val Loss = 2.81892
2023-04-13 16:51:11.017594 Epoch 44  	Train Loss = 2.74131 Val Loss = 2.83785
2023-04-13 16:52:22.241958 Epoch 45  	Train Loss = 2.73836 Val Loss = 2.83344
2023-04-13 16:53:33.343435 Epoch 46  	Train Loss = 2.73486 Val Loss = 2.83155
2023-04-13 16:54:44.067522 Epoch 47  	Train Loss = 2.73248 Val Loss = 2.81726
2023-04-13 16:55:55.208422 Epoch 48  	Train Loss = 2.72882 Val Loss = 2.83950
2023-04-13 16:57:06.167616 Epoch 49  	Train Loss = 2.72711 Val Loss = 2.84556
2023-04-13 16:58:17.348439 Epoch 50  	Train Loss = 2.72437 Val Loss = 2.81912
2023-04-13 16:59:28.282425 Epoch 51  	Train Loss = 2.71114 Val Loss = 2.82247
2023-04-13 17:00:39.235862 Epoch 52  	Train Loss = 2.71014 Val Loss = 2.82566
2023-04-13 17:01:50.226894 Epoch 53  	Train Loss = 2.70904 Val Loss = 2.82851
2023-04-13 17:03:00.633426 Epoch 54  	Train Loss = 2.70882 Val Loss = 2.82914
2023-04-13 17:04:11.669847 Epoch 55  	Train Loss = 2.70934 Val Loss = 2.83075
2023-04-13 17:05:22.776052 Epoch 56  	Train Loss = 2.70812 Val Loss = 2.83132
2023-04-13 17:06:33.801173 Epoch 57  	Train Loss = 2.70770 Val Loss = 2.82918
2023-04-13 17:07:44.968774 Epoch 58  	Train Loss = 2.70704 Val Loss = 2.82557
2023-04-13 17:08:55.884488 Epoch 59  	Train Loss = 2.70678 Val Loss = 2.82763
2023-04-13 17:10:06.926848 Epoch 60  	Train Loss = 2.70786 Val Loss = 2.82434
2023-04-13 17:11:17.940069 Epoch 61  	Train Loss = 2.70669 Val Loss = 2.82792
2023-04-13 17:12:29.087434 Epoch 62  	Train Loss = 2.70632 Val Loss = 2.82884
2023-04-13 17:13:40.160044 Epoch 63  	Train Loss = 2.70584 Val Loss = 2.82705
2023-04-13 17:14:51.376893 Epoch 64  	Train Loss = 2.70579 Val Loss = 2.82611
2023-04-13 17:16:02.422513 Epoch 65  	Train Loss = 2.70571 Val Loss = 2.82826
2023-04-13 17:17:13.443238 Epoch 66  	Train Loss = 2.70516 Val Loss = 2.82644
2023-04-13 17:18:24.551163 Epoch 67  	Train Loss = 2.70437 Val Loss = 2.82794
Early stopping at epoch: 67
Best at epoch 47:
Train Loss = 2.73248
Train RMSE = 5.41336, MAE = 2.69349, MAPE = 7.06026
Val Loss = 2.81726
Val RMSE = 6.02383, MAE = 2.86588, MAPE = 7.95628
--------- Test ---------
All Steps RMSE = 6.34587, MAE = 3.08146, MAPE = 8.55192
Step 1 RMSE = 4.16480, MAE = 2.34002, MAPE = 5.72625
Step 2 RMSE = 4.93152, MAE = 2.60285, MAPE = 6.63399
Step 3 RMSE = 5.42260, MAE = 2.78073, MAPE = 7.30955
Step 4 RMSE = 5.84762, MAE = 2.91997, MAPE = 7.86821
Step 5 RMSE = 6.15853, MAE = 3.03218, MAPE = 8.31964
Step 6 RMSE = 6.42525, MAE = 3.13049, MAPE = 8.72566
Step 7 RMSE = 6.65325, MAE = 3.21427, MAPE = 9.07728
Step 8 RMSE = 6.82558, MAE = 3.28278, MAPE = 9.36291
Step 9 RMSE = 6.99910, MAE = 3.34587, MAPE = 9.61396
Step 10 RMSE = 7.14716, MAE = 3.39672, MAPE = 9.82397
Step 11 RMSE = 7.28219, MAE = 3.44270, MAPE = 10.00034
Step 12 RMSE = 7.40355, MAE = 3.48897, MAPE = 10.16143
Inference time: 6.47 s
