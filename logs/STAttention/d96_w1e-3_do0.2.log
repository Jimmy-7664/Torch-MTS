METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.001,
    "milestones": [
        10,
        30
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.2
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         49,504
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         49,504
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 526,633
Trainable params: 526,633
Non-trainable params: 0
Total mult-adds (M): 33.70
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 7250.58
Params size (MB): 2.11
Estimated Total Size (MB): 7254.59
==========================================================================================

Loss: MaskedMAELoss

2023-04-12 14:52:53.728863 Epoch 1  	Train Loss = 5.07167 Val Loss = 3.41428
2023-04-12 14:54:47.456568 Epoch 2  	Train Loss = 3.52423 Val Loss = 3.32453
2023-04-12 14:56:42.220492 Epoch 3  	Train Loss = 3.41987 Val Loss = 3.27669
2023-04-12 14:58:37.058585 Epoch 4  	Train Loss = 3.34471 Val Loss = 3.18003
2023-04-12 15:00:32.104990 Epoch 5  	Train Loss = 3.28256 Val Loss = 3.08705
2023-04-12 15:02:27.548283 Epoch 6  	Train Loss = 3.20585 Val Loss = 3.06282
2023-04-12 15:04:20.398409 Epoch 7  	Train Loss = 3.15698 Val Loss = 3.00553
2023-04-12 15:06:10.638944 Epoch 8  	Train Loss = 3.12498 Val Loss = 3.00634
2023-04-12 15:08:00.912230 Epoch 9  	Train Loss = 3.09293 Val Loss = 2.95598
2023-04-12 15:09:54.020474 Epoch 10  	Train Loss = 3.06131 Val Loss = 2.91521
2023-04-12 15:11:49.084533 Epoch 11  	Train Loss = 2.99545 Val Loss = 2.87318
2023-04-12 15:13:44.259623 Epoch 12  	Train Loss = 2.98414 Val Loss = 2.87152
2023-04-12 15:15:39.672268 Epoch 13  	Train Loss = 2.97994 Val Loss = 2.87724
2023-04-12 15:17:35.335241 Epoch 14  	Train Loss = 2.97548 Val Loss = 2.87056
2023-04-12 15:19:30.383101 Epoch 15  	Train Loss = 2.97272 Val Loss = 2.86491
2023-04-12 15:21:25.121937 Epoch 16  	Train Loss = 2.96906 Val Loss = 2.86888
2023-04-12 15:23:19.930558 Epoch 17  	Train Loss = 2.96480 Val Loss = 2.86578
2023-04-12 15:25:14.712457 Epoch 18  	Train Loss = 2.96244 Val Loss = 2.87124
2023-04-12 15:27:09.536015 Epoch 19  	Train Loss = 2.95988 Val Loss = 2.86088
2023-04-12 15:29:04.625588 Epoch 20  	Train Loss = 2.95696 Val Loss = 2.87629
2023-04-12 15:30:59.557009 Epoch 21  	Train Loss = 2.95337 Val Loss = 2.86518
2023-04-12 15:32:54.062082 Epoch 22  	Train Loss = 2.94965 Val Loss = 2.85468
2023-04-12 15:34:48.560203 Epoch 23  	Train Loss = 2.94631 Val Loss = 2.87476
2023-04-12 15:36:42.993441 Epoch 24  	Train Loss = 2.94476 Val Loss = 2.86249
2023-04-12 15:38:37.586089 Epoch 25  	Train Loss = 2.94338 Val Loss = 2.88816
2023-04-12 15:40:32.941858 Epoch 26  	Train Loss = 2.93983 Val Loss = 2.85988
2023-04-12 15:42:27.986931 Epoch 27  	Train Loss = 2.93692 Val Loss = 2.85985
2023-04-12 15:44:23.099967 Epoch 28  	Train Loss = 2.93412 Val Loss = 2.85939
2023-04-12 15:46:18.297563 Epoch 29  	Train Loss = 2.93323 Val Loss = 2.84989
2023-04-12 15:48:12.903276 Epoch 30  	Train Loss = 2.93146 Val Loss = 2.85400
2023-04-12 15:50:08.423523 Epoch 31  	Train Loss = 2.91800 Val Loss = 2.84498
2023-04-12 15:52:03.023714 Epoch 32  	Train Loss = 2.91664 Val Loss = 2.84523
2023-04-12 15:53:58.315428 Epoch 33  	Train Loss = 2.91618 Val Loss = 2.84866
2023-04-12 15:55:53.917172 Epoch 34  	Train Loss = 2.91523 Val Loss = 2.84624
2023-04-12 15:57:49.015532 Epoch 35  	Train Loss = 2.91591 Val Loss = 2.84453
2023-04-12 15:59:44.348824 Epoch 36  	Train Loss = 2.91504 Val Loss = 2.84417
2023-04-12 16:01:39.007368 Epoch 37  	Train Loss = 2.91472 Val Loss = 2.84623
2023-04-12 16:03:33.560603 Epoch 38  	Train Loss = 2.91429 Val Loss = 2.84660
2023-04-12 16:05:28.816492 Epoch 39  	Train Loss = 2.91357 Val Loss = 2.84580
2023-04-12 16:07:23.452524 Epoch 40  	Train Loss = 2.91352 Val Loss = 2.84380
2023-04-12 16:09:18.686899 Epoch 41  	Train Loss = 2.91294 Val Loss = 2.84413
2023-04-12 16:11:14.428348 Epoch 42  	Train Loss = 2.91277 Val Loss = 2.84421
2023-04-12 16:13:09.306133 Epoch 43  	Train Loss = 2.91216 Val Loss = 2.85191
2023-04-12 16:15:04.069692 Epoch 44  	Train Loss = 2.91199 Val Loss = 2.84707
2023-04-12 16:16:58.422516 Epoch 45  	Train Loss = 2.91170 Val Loss = 2.84692
2023-04-12 16:18:52.805577 Epoch 46  	Train Loss = 2.91151 Val Loss = 2.84476
2023-04-12 16:20:47.257728 Epoch 47  	Train Loss = 2.91131 Val Loss = 2.84829
2023-04-12 16:22:41.748807 Epoch 48  	Train Loss = 2.91089 Val Loss = 2.84664
2023-04-12 16:24:36.249515 Epoch 49  	Train Loss = 2.91018 Val Loss = 2.84859
2023-04-12 16:26:30.718807 Epoch 50  	Train Loss = 2.90982 Val Loss = 2.84543
Early stopping at epoch: 50
Best at epoch 40:
Train Loss = 2.91352
Train RMSE = 5.90324, MAE = 2.89347, MAPE = 7.73789
Val Loss = 2.84380
Val RMSE = 6.04786, MAE = 2.88303, MAPE = 8.03414
--------- Test ---------
All Steps RMSE = 6.40473, MAE = 3.12330, MAPE = 8.68150
Step 1 RMSE = 4.23173, MAE = 2.36754, MAPE = 5.81396
Step 2 RMSE = 5.03988, MAE = 2.63724, MAPE = 6.73173
Step 3 RMSE = 5.53834, MAE = 2.82165, MAPE = 7.43266
Step 4 RMSE = 5.93984, MAE = 2.96215, MAPE = 8.01362
Step 5 RMSE = 6.22490, MAE = 3.07800, MAPE = 8.47093
Step 6 RMSE = 6.47557, MAE = 3.17769, MAPE = 8.87410
Step 7 RMSE = 6.70685, MAE = 3.25890, MAPE = 9.21971
Step 8 RMSE = 6.86900, MAE = 3.32786, MAPE = 9.51149
Step 9 RMSE = 7.04300, MAE = 3.38706, MAPE = 9.75159
Step 10 RMSE = 7.17459, MAE = 3.43207, MAPE = 9.92428
Step 11 RMSE = 7.29764, MAE = 3.48296, MAPE = 10.11797
Step 12 RMSE = 7.47023, MAE = 3.54661, MAPE = 10.31607
Inference time: 10.35 s
