METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─SelfAttentionLayer: 2-1           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-2                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-3               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-4              [64, 207, 12, 96]         24,800
│    │    └─Dropout: 3-5                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-6               [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-2           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-7          [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-8                 [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-9               [64, 207, 12, 96]         192
│    │    └─Sequential: 3-10             [64, 207, 12, 96]         24,800
│    │    └─Dropout: 3-11                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-12              [64, 207, 12, 96]         192
│    └─SelfAttentionLayer: 2-3           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-13         [64, 207, 12, 96]         37,248
│    │    └─Dropout: 3-14                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-15              [64, 207, 12, 96]         192
│    │    └─Sequential: 3-16             [64, 207, 12, 96]         24,800
│    │    └─Dropout: 3-17                [64, 207, 12, 96]         --
│    │    └─LayerNorm: 3-18              [64, 207, 12, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─SelfAttentionLayer: 2-4           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-19         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-20                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-21              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-22             [64, 12, 207, 96]         24,800
│    │    └─Dropout: 3-23                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-24              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-5           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-25         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-26                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-27              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-28             [64, 12, 207, 96]         24,800
│    │    └─Dropout: 3-29                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-30              [64, 12, 207, 96]         192
│    └─SelfAttentionLayer: 2-6           [64, 12, 207, 96]         --
│    │    └─AttentionLayer: 3-31         [64, 12, 207, 96]         37,248
│    │    └─Dropout: 3-32                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-33              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-34             [64, 12, 207, 96]         24,800
│    │    └─Dropout: 3-35                [64, 12, 207, 96]         --
│    │    └─LayerNorm: 3-36              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 378,409
Trainable params: 378,409
Non-trainable params: 0
Total mult-adds (M): 24.22
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 6273.83
Params size (MB): 1.51
Estimated Total Size (MB): 6277.25
==========================================================================================

Loss: MaskedMAELoss

2023-04-13 15:55:31.335882 Epoch 1  	Train Loss = 4.85401 Val Loss = 3.40433
2023-04-13 15:57:17.256188 Epoch 2  	Train Loss = 3.46290 Val Loss = 3.22456
2023-04-13 15:59:03.840914 Epoch 3  	Train Loss = 3.30580 Val Loss = 3.10251
2023-04-13 16:00:50.639094 Epoch 4  	Train Loss = 3.18012 Val Loss = 2.96109
2023-04-13 16:02:37.600903 Epoch 5  	Train Loss = 3.08196 Val Loss = 2.96191
2023-04-13 16:04:24.123131 Epoch 6  	Train Loss = 3.02760 Val Loss = 2.90945
2023-04-13 16:06:11.189810 Epoch 7  	Train Loss = 2.99399 Val Loss = 2.91175
2023-04-13 16:07:58.317883 Epoch 8  	Train Loss = 2.95704 Val Loss = 2.86057
2023-04-13 16:09:45.197054 Epoch 9  	Train Loss = 2.92641 Val Loss = 2.90257
2023-04-13 16:11:32.381585 Epoch 10  	Train Loss = 2.89515 Val Loss = 2.84074
2023-04-13 16:13:19.254959 Epoch 11  	Train Loss = 2.81340 Val Loss = 2.77396
2023-04-13 16:15:05.751968 Epoch 12  	Train Loss = 2.79923 Val Loss = 2.77351
2023-04-13 16:16:52.387527 Epoch 13  	Train Loss = 2.79283 Val Loss = 2.77370
2023-04-13 16:18:39.213266 Epoch 14  	Train Loss = 2.78724 Val Loss = 2.76924
2023-04-13 16:20:26.232526 Epoch 15  	Train Loss = 2.78047 Val Loss = 2.77546
2023-04-13 16:22:13.777687 Epoch 16  	Train Loss = 2.77525 Val Loss = 2.76515
2023-04-13 16:24:01.535924 Epoch 17  	Train Loss = 2.76922 Val Loss = 2.76963
2023-04-13 16:25:49.119309 Epoch 18  	Train Loss = 2.76301 Val Loss = 2.76201
2023-04-13 16:27:36.372137 Epoch 19  	Train Loss = 2.75743 Val Loss = 2.75955
2023-04-13 16:29:23.152435 Epoch 20  	Train Loss = 2.75309 Val Loss = 2.76041
2023-04-13 16:31:09.827767 Epoch 21  	Train Loss = 2.74889 Val Loss = 2.76018
2023-04-13 16:32:56.437635 Epoch 22  	Train Loss = 2.74350 Val Loss = 2.75805
2023-04-13 16:34:43.315336 Epoch 23  	Train Loss = 2.73809 Val Loss = 2.75854
2023-04-13 16:36:30.801042 Epoch 24  	Train Loss = 2.73335 Val Loss = 2.75225
2023-04-13 16:38:18.193684 Epoch 25  	Train Loss = 2.72867 Val Loss = 2.77458
2023-04-13 16:40:05.569440 Epoch 26  	Train Loss = 2.72534 Val Loss = 2.75381
2023-04-13 16:41:53.320703 Epoch 27  	Train Loss = 2.72004 Val Loss = 2.75710
2023-04-13 16:43:40.933584 Epoch 28  	Train Loss = 2.71584 Val Loss = 2.75384
2023-04-13 16:45:28.296049 Epoch 29  	Train Loss = 2.71197 Val Loss = 2.75387
2023-04-13 16:47:15.374158 Epoch 30  	Train Loss = 2.70733 Val Loss = 2.75327
2023-04-13 16:49:02.860991 Epoch 31  	Train Loss = 2.70542 Val Loss = 2.75957
2023-04-13 16:50:50.238764 Epoch 32  	Train Loss = 2.69949 Val Loss = 2.74990
2023-04-13 16:52:37.825619 Epoch 33  	Train Loss = 2.69555 Val Loss = 2.75329
2023-04-13 16:54:25.546095 Epoch 34  	Train Loss = 2.69260 Val Loss = 2.76269
2023-04-13 16:56:12.820063 Epoch 35  	Train Loss = 2.68817 Val Loss = 2.75601
2023-04-13 16:57:59.536298 Epoch 36  	Train Loss = 2.68629 Val Loss = 2.75235
2023-04-13 16:59:46.228915 Epoch 37  	Train Loss = 2.68192 Val Loss = 2.75205
2023-04-13 17:01:33.040041 Epoch 38  	Train Loss = 2.67845 Val Loss = 2.74457
2023-04-13 17:03:20.347169 Epoch 39  	Train Loss = 2.67433 Val Loss = 2.75999
2023-04-13 17:05:07.802240 Epoch 40  	Train Loss = 2.67166 Val Loss = 2.75394
2023-04-13 17:06:55.516736 Epoch 41  	Train Loss = 2.66885 Val Loss = 2.75949
2023-04-13 17:08:43.099080 Epoch 42  	Train Loss = 2.66535 Val Loss = 2.76035
2023-04-13 17:10:30.337136 Epoch 43  	Train Loss = 2.66260 Val Loss = 2.75166
2023-04-13 17:12:16.905025 Epoch 44  	Train Loss = 2.65912 Val Loss = 2.74541
2023-04-13 17:14:03.409444 Epoch 45  	Train Loss = 2.65655 Val Loss = 2.75340
2023-04-13 17:15:50.095262 Epoch 46  	Train Loss = 2.65437 Val Loss = 2.75908
2023-04-13 17:17:37.245602 Epoch 47  	Train Loss = 2.64964 Val Loss = 2.75974
2023-04-13 17:19:24.105859 Epoch 48  	Train Loss = 2.64779 Val Loss = 2.75504
2023-04-13 17:21:11.150949 Epoch 49  	Train Loss = 2.64518 Val Loss = 2.74952
2023-04-13 17:22:58.034257 Epoch 50  	Train Loss = 2.64219 Val Loss = 2.75061
2023-04-13 17:24:44.446397 Epoch 51  	Train Loss = 2.62711 Val Loss = 2.74877
2023-04-13 17:26:30.322140 Epoch 52  	Train Loss = 2.62584 Val Loss = 2.74909
2023-04-13 17:28:16.283560 Epoch 53  	Train Loss = 2.62541 Val Loss = 2.74796
2023-04-13 17:30:02.135095 Epoch 54  	Train Loss = 2.62473 Val Loss = 2.74876
2023-04-13 17:31:48.075470 Epoch 55  	Train Loss = 2.62514 Val Loss = 2.75090
2023-04-13 17:33:34.270365 Epoch 56  	Train Loss = 2.62437 Val Loss = 2.74910
2023-04-13 17:35:20.537922 Epoch 57  	Train Loss = 2.62373 Val Loss = 2.74777
2023-04-13 17:37:06.975373 Epoch 58  	Train Loss = 2.62350 Val Loss = 2.75105
Early stopping at epoch: 58
Best at epoch 38:
Train Loss = 2.67845
Train RMSE = 5.16964, MAE = 2.59803, MAPE = 6.70755
Val Loss = 2.74457
Val RMSE = 5.82036, MAE = 2.78659, MAPE = 7.60708
--------- Test ---------
All Steps RMSE = 6.13257, MAE = 2.98146, MAPE = 8.15049
Step 1 RMSE = 4.02383, MAE = 2.27575, MAPE = 5.46261
Step 2 RMSE = 4.69001, MAE = 2.50947, MAPE = 6.25864
Step 3 RMSE = 5.16786, MAE = 2.67772, MAPE = 6.90658
Step 4 RMSE = 5.58055, MAE = 2.81255, MAPE = 7.44998
Step 5 RMSE = 5.89769, MAE = 2.92276, MAPE = 7.87475
Step 6 RMSE = 6.17579, MAE = 3.02109, MAPE = 8.26907
Step 7 RMSE = 6.41419, MAE = 3.10514, MAPE = 8.61625
Step 8 RMSE = 6.60239, MAE = 3.17592, MAPE = 8.91527
Step 9 RMSE = 6.79575, MAE = 3.24245, MAPE = 9.19684
Step 10 RMSE = 6.95585, MAE = 3.29358, MAPE = 9.42103
Step 11 RMSE = 7.10495, MAE = 3.34259, MAPE = 9.62065
Step 12 RMSE = 7.25007, MAE = 3.39855, MAPE = 9.81453
Inference time: 9.58 s
