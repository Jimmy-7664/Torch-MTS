ELECTRICITY
Trainset:	x-(18212, 96, 321, 3)	y-(18212, 192, 321, 1)
Valset:  	x-(2602, 96, 321, 3)  	y-(2602, 192, 321, 1)
Testset:	x-(5203, 96, 321, 3)	y-(5203, 192, 321, 1)
INFO: Using scaled X and Y, only for LTSF!

Random seed = 233
--------- STAEformer ---------
{
    "num_nodes": 321,
    "in_steps": 96,
    "out_steps": 192,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.0001,
    "weight_decay": 0,
    "milestones": [
        10,
        30
    ],
    "lr_decay_rate": 0.1,
    "clip_grad": 0,
    "batch_size": 16,
    "max_epochs": 200,
    "dataloader": "ltsf",
    "runner": "ltsf",
    "loss": "mse",
    "model_args": {
        "num_nodes": 321,
        "in_steps": 96,
        "out_steps": 192,
        "steps_per_day": 24,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 192, 321, 1]         2,465,280
├─Linear: 1-1                            [16, 96, 321, 24]         96
├─Embedding: 1-2                         [16, 96, 321, 24]         576
├─Embedding: 1-3                         [16, 96, 321, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 96, 321, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 321, 96, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 321, 96, 152]        --
│    │    └─LayerNorm: 3-3               [16, 321, 96, 152]        304
│    │    └─Sequential: 3-4              [16, 321, 96, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 321, 96, 152]        --
│    │    └─LayerNorm: 3-6               [16, 321, 96, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-2           [16, 96, 321, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 96, 321, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 96, 321, 152]        --
│    │    └─LayerNorm: 3-9               [16, 96, 321, 152]        304
│    │    └─Sequential: 3-10             [16, 96, 321, 152]        78,232
│    │    └─Dropout: 3-11                [16, 96, 321, 152]        --
│    │    └─LayerNorm: 3-12              [16, 96, 321, 152]        304
├─Linear: 1-6                            [16, 321, 192]            2,801,856
==========================================================================================
Total params: 5,611,704
Trainable params: 5,611,704
Non-trainable params: 0
Total mult-adds (M): 50.34
==========================================================================================
Input size (MB): 5.92
Forward/backward pass size (MB): 10705.23
Params size (MB): 12.59
Estimated Total Size (MB): 10723.73
==========================================================================================

Loss: MSELoss

2024-04-14 10:57:51.290308 Epoch 1  	Train Loss = 0.22646 Val Loss = 0.15373
2024-04-14 11:03:37.958388 Epoch 2  	Train Loss = 0.17345 Val Loss = 0.14759
2024-04-14 11:09:23.873749 Epoch 3  	Train Loss = 0.16476 Val Loss = 0.13767
2024-04-14 11:15:09.923858 Epoch 4  	Train Loss = 0.15898 Val Loss = 0.13490
2024-04-14 11:20:55.863793 Epoch 5  	Train Loss = 0.15403 Val Loss = 0.13910
2024-04-14 11:26:42.946873 Epoch 6  	Train Loss = 0.14951 Val Loss = 0.13607
2024-04-14 11:32:29.077931 Epoch 7  	Train Loss = 0.14603 Val Loss = 0.12923
2024-04-14 11:38:16.267064 Epoch 8  	Train Loss = 0.14248 Val Loss = 0.12874
2024-04-14 11:44:02.526800 Epoch 9  	Train Loss = 0.13996 Val Loss = 0.14003
2024-04-14 11:49:50.578237 Epoch 10  	Train Loss = 0.13758 Val Loss = 0.14248
2024-04-14 11:55:37.137423 Epoch 11  	Train Loss = 0.13349 Val Loss = 0.13707
2024-04-14 12:01:24.543175 Epoch 12  	Train Loss = 0.13298 Val Loss = 0.13647
2024-04-14 12:07:10.889458 Epoch 13  	Train Loss = 0.13268 Val Loss = 0.13782
2024-04-14 12:12:57.306951 Epoch 14  	Train Loss = 0.13239 Val Loss = 0.13585
2024-04-14 12:18:44.185943 Epoch 15  	Train Loss = 0.13211 Val Loss = 0.13739
2024-04-14 12:24:43.026229 Epoch 16  	Train Loss = 0.13179 Val Loss = 0.13693
2024-04-14 12:30:30.657548 Epoch 17  	Train Loss = 0.13155 Val Loss = 0.13791
2024-04-14 12:36:17.926739 Epoch 18  	Train Loss = 0.13125 Val Loss = 0.13775
Early stopping at epoch: 18
Best at epoch 8:
Train Loss = 0.14248
Train MSE = 0.14261, MAE = 0.24758
Val Loss = 0.12874
Val MSE = 0.12877, MAE = 0.23142
Model checkpoint saved to: ../saved_models/STAEformer/STAEformer-ELECTRICITY-2024-04-14-10-51-50.pt
--------- Test ---------
All Steps (1-192) MSE = 0.15176, MAE = 0.25546
Inference time: 34.44 s
