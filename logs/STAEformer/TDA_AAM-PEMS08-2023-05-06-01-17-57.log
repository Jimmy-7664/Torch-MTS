PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- TDA_AAM ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TDA_AAM                                  [16, 12, 170, 1]          163,200
├─Linear: 1-1                            [16, 12, 170, 24]         96
├─Embedding: 1-2                         [16, 12, 170, 24]         6,912
├─Embedding: 1-3                         [16, 12, 170, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 170, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 170, 152]        304
├─Linear: 1-6                            [16, 170, 12]             21,900
==========================================================================================
Total params: 1,223,460
Trainable params: 1,223,460
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2087.13
Params size (MB): 4.24
Estimated Total Size (MB): 2091.76
==========================================================================================

Loss: HuberLoss

2023-05-06 01:19:26.230850 Epoch 1  	Train Loss = 26.98568 Val Loss = 18.33571
2023-05-06 01:20:49.864276 Epoch 2  	Train Loss = 18.65351 Val Loss = 17.95529
2023-05-06 01:22:14.561796 Epoch 3  	Train Loss = 17.61373 Val Loss = 17.24548
2023-05-06 01:23:09.734942 Epoch 4  	Train Loss = 17.09619 Val Loss = 17.67510
2023-05-06 01:23:51.399320 Epoch 5  	Train Loss = 16.16737 Val Loss = 16.00483
2023-05-06 01:24:33.227468 Epoch 6  	Train Loss = 15.86976 Val Loss = 15.54388
2023-05-06 01:25:14.856452 Epoch 7  	Train Loss = 15.47371 Val Loss = 15.78229
2023-05-06 01:25:56.121350 Epoch 8  	Train Loss = 15.21417 Val Loss = 15.00521
2023-05-06 01:26:37.047204 Epoch 9  	Train Loss = 14.92306 Val Loss = 15.31990
2023-05-06 01:27:17.945000 Epoch 10  	Train Loss = 15.01017 Val Loss = 14.74563
2023-05-06 01:27:59.117152 Epoch 11  	Train Loss = 14.58237 Val Loss = 14.39007
2023-05-06 01:28:40.383144 Epoch 12  	Train Loss = 14.51742 Val Loss = 14.39481
2023-05-06 01:29:21.427936 Epoch 13  	Train Loss = 14.36192 Val Loss = 14.46048
2023-05-06 01:30:02.489756 Epoch 14  	Train Loss = 14.21718 Val Loss = 15.14519
2023-05-06 01:30:43.973484 Epoch 15  	Train Loss = 14.15819 Val Loss = 14.71335
2023-05-06 01:31:25.393515 Epoch 16  	Train Loss = 13.99122 Val Loss = 13.96601
2023-05-06 01:32:06.912959 Epoch 17  	Train Loss = 13.80849 Val Loss = 14.09545
2023-05-06 01:32:48.456165 Epoch 18  	Train Loss = 13.73714 Val Loss = 14.03218
2023-05-06 01:33:29.932897 Epoch 19  	Train Loss = 13.64311 Val Loss = 13.79417
2023-05-06 01:34:11.855818 Epoch 20  	Train Loss = 13.56435 Val Loss = 13.87970
2023-05-06 01:34:53.342246 Epoch 21  	Train Loss = 13.47103 Val Loss = 14.23029
2023-05-06 01:35:34.425147 Epoch 22  	Train Loss = 13.49635 Val Loss = 14.25959
2023-05-06 01:36:15.302977 Epoch 23  	Train Loss = 13.33710 Val Loss = 13.86126
2023-05-06 01:36:56.142981 Epoch 24  	Train Loss = 13.28663 Val Loss = 14.20393
2023-05-06 01:37:36.924078 Epoch 25  	Train Loss = 13.26305 Val Loss = 13.95465
2023-05-06 01:38:17.952252 Epoch 26  	Train Loss = 12.62410 Val Loss = 13.20084
2023-05-06 01:38:58.885403 Epoch 27  	Train Loss = 12.52783 Val Loss = 13.19967
2023-05-06 01:39:40.491268 Epoch 28  	Train Loss = 12.50108 Val Loss = 13.19556
2023-05-06 01:40:22.043048 Epoch 29  	Train Loss = 12.47523 Val Loss = 13.19943
2023-05-06 01:41:03.718483 Epoch 30  	Train Loss = 12.45436 Val Loss = 13.17372
2023-05-06 01:41:45.393076 Epoch 31  	Train Loss = 12.43589 Val Loss = 13.23496
2023-05-06 01:42:27.074221 Epoch 32  	Train Loss = 12.41925 Val Loss = 13.21625
2023-05-06 01:43:08.905004 Epoch 33  	Train Loss = 12.40294 Val Loss = 13.16098
2023-05-06 01:43:50.480569 Epoch 34  	Train Loss = 12.38715 Val Loss = 13.15103
2023-05-06 01:44:31.742177 Epoch 35  	Train Loss = 12.37439 Val Loss = 13.21539
2023-05-06 01:45:12.679933 Epoch 36  	Train Loss = 12.36243 Val Loss = 13.16208
2023-05-06 01:45:53.747553 Epoch 37  	Train Loss = 12.34788 Val Loss = 13.17825
2023-05-06 01:46:34.522309 Epoch 38  	Train Loss = 12.34536 Val Loss = 13.20783
2023-05-06 01:47:15.633738 Epoch 39  	Train Loss = 12.32441 Val Loss = 13.15213
2023-05-06 01:47:56.677093 Epoch 40  	Train Loss = 12.31488 Val Loss = 13.15711
2023-05-06 01:48:38.151294 Epoch 41  	Train Loss = 12.30385 Val Loss = 13.27613
2023-05-06 01:49:19.682411 Epoch 42  	Train Loss = 12.29595 Val Loss = 13.20831
2023-05-06 01:50:01.344651 Epoch 43  	Train Loss = 12.28969 Val Loss = 13.16976
2023-05-06 01:50:42.900040 Epoch 44  	Train Loss = 12.28207 Val Loss = 13.16283
2023-05-06 01:51:24.409973 Epoch 45  	Train Loss = 12.26652 Val Loss = 13.16922
2023-05-06 01:52:06.102356 Epoch 46  	Train Loss = 12.19827 Val Loss = 13.15262
2023-05-06 01:52:48.553182 Epoch 47  	Train Loss = 12.19037 Val Loss = 13.13139
2023-05-06 01:53:31.247870 Epoch 48  	Train Loss = 12.18755 Val Loss = 13.13302
2023-05-06 01:54:12.801395 Epoch 49  	Train Loss = 12.18657 Val Loss = 13.14372
2023-05-06 01:54:53.827286 Epoch 50  	Train Loss = 12.18341 Val Loss = 13.13161
2023-05-06 01:55:34.772053 Epoch 51  	Train Loss = 12.18299 Val Loss = 13.14140
2023-05-06 01:56:15.898555 Epoch 52  	Train Loss = 12.18129 Val Loss = 13.14310
2023-05-06 01:56:57.042544 Epoch 53  	Train Loss = 12.17908 Val Loss = 13.15027
2023-05-06 01:57:38.572358 Epoch 54  	Train Loss = 12.17588 Val Loss = 13.14187
2023-05-06 01:58:20.184604 Epoch 55  	Train Loss = 12.17619 Val Loss = 13.14217
2023-05-06 01:59:01.700845 Epoch 56  	Train Loss = 12.17289 Val Loss = 13.15361
2023-05-06 01:59:43.363175 Epoch 57  	Train Loss = 12.17142 Val Loss = 13.14626
2023-05-06 02:00:24.706585 Epoch 58  	Train Loss = 12.17041 Val Loss = 13.14580
2023-05-06 02:01:05.950789 Epoch 59  	Train Loss = 12.16981 Val Loss = 13.14280
2023-05-06 02:01:47.031038 Epoch 60  	Train Loss = 12.16698 Val Loss = 13.15173
2023-05-06 02:02:27.887845 Epoch 61  	Train Loss = 12.16500 Val Loss = 13.13919
2023-05-06 02:03:08.757578 Epoch 62  	Train Loss = 12.16458 Val Loss = 13.13797
2023-05-06 02:03:49.787023 Epoch 63  	Train Loss = 12.16209 Val Loss = 13.14783
2023-05-06 02:04:30.839943 Epoch 64  	Train Loss = 12.16185 Val Loss = 13.16073
2023-05-06 02:05:12.053096 Epoch 65  	Train Loss = 12.16038 Val Loss = 13.15496
2023-05-06 02:05:53.237878 Epoch 66  	Train Loss = 12.15322 Val Loss = 13.14625
2023-05-06 02:06:34.593017 Epoch 67  	Train Loss = 12.15217 Val Loss = 13.14603
2023-05-06 02:07:15.639263 Epoch 68  	Train Loss = 12.15097 Val Loss = 13.14461
2023-05-06 02:07:56.639852 Epoch 69  	Train Loss = 12.15057 Val Loss = 13.14559
2023-05-06 02:08:37.643313 Epoch 70  	Train Loss = 12.15185 Val Loss = 13.14405
2023-05-06 02:09:18.835650 Epoch 71  	Train Loss = 12.15146 Val Loss = 13.14379
2023-05-06 02:09:59.966783 Epoch 72  	Train Loss = 12.15022 Val Loss = 13.14725
2023-05-06 02:10:41.108141 Epoch 73  	Train Loss = 12.15123 Val Loss = 13.14321
2023-05-06 02:11:21.893894 Epoch 74  	Train Loss = 12.15114 Val Loss = 13.14438
2023-05-06 02:12:02.752845 Epoch 75  	Train Loss = 12.14960 Val Loss = 13.14478
2023-05-06 02:12:43.499470 Epoch 76  	Train Loss = 12.15078 Val Loss = 13.14630
2023-05-06 02:13:24.360424 Epoch 77  	Train Loss = 12.14927 Val Loss = 13.14473
Early stopping at epoch: 77
Best at epoch 47:
Train Loss = 12.19037
Train RMSE = 22.00926, MAE = 12.39613, MAPE = 8.16276
Val Loss = 13.13139
Val RMSE = 24.24892, MAE = 13.57395, MAPE = 10.19171
Saved Model: ../saved_models/TDA_AAM/TDA_AAM-PEMS08-2023-05-06-01-17-57.pt
--------- Test ---------
All Steps RMSE = 23.25029, MAE = 13.46090, MAPE = 8.88157
Step 1 RMSE = 19.59203, MAE = 11.78246, MAPE = 7.80906
Step 2 RMSE = 20.67820, MAE = 12.23071, MAPE = 8.08937
Step 3 RMSE = 21.53540, MAE = 12.62261, MAPE = 8.32108
Step 4 RMSE = 22.23623, MAE = 12.93862, MAPE = 8.51277
Step 5 RMSE = 22.78652, MAE = 13.20619, MAPE = 8.68757
Step 6 RMSE = 23.29770, MAE = 13.46272, MAPE = 8.85555
Step 7 RMSE = 23.75888, MAE = 13.69754, MAPE = 9.01623
Step 8 RMSE = 24.16530, MAE = 13.91153, MAPE = 9.15897
Step 9 RMSE = 24.51938, MAE = 14.11049, MAPE = 9.30350
Step 10 RMSE = 24.86098, MAE = 14.29612, MAPE = 9.45377
Step 11 RMSE = 25.18764, MAE = 14.50133, MAPE = 9.58401
Step 12 RMSE = 25.55614, MAE = 14.77052, MAPE = 9.78696
Inference time: 3.97 s
