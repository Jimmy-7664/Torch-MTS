PEMSD7L
Trainset:	x-(7589, 12, 1026, 3)	y-(7589, 12, 1026, 1)
Valset:  	x-(2530, 12, 1026, 3)  	y-(2530, 12, 1026, 1)
Testset:	x-(2530, 12, 1026, 3)	y-(2530, 12, 1026, 1)

Random seed = 233
--------- STAEformer ---------
{
    "num_nodes": 1026,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0003,
    "milestones": [
        20,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 200,
    "model_args": {
        "num_nodes": 1026,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "days_per_week": 5,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 12, 1026, 1]         984,960
├─Linear: 1-1                            [16, 12, 1026, 24]        96
├─Embedding: 1-2                         [16, 12, 1026, 24]        6,912
├─Embedding: 1-3                         [16, 12, 1026, 24]        120
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 1026, 152]       --
│    │    └─AttentionLayer: 3-1          [16, 1026, 12, 152]       93,024
│    │    └─Dropout: 3-2                 [16, 1026, 12, 152]       --
│    │    └─LayerNorm: 3-3               [16, 1026, 12, 152]       304
│    │    └─Sequential: 3-4              [16, 1026, 12, 152]       78,232
│    │    └─Dropout: 3-5                 [16, 1026, 12, 152]       --
│    │    └─LayerNorm: 3-6               [16, 1026, 12, 152]       304
│    └─SelfAttentionLayer: 2-2           [16, 12, 1026, 152]       --
│    │    └─AttentionLayer: 3-7          [16, 1026, 12, 152]       93,024
│    │    └─Dropout: 3-8                 [16, 1026, 12, 152]       --
│    │    └─LayerNorm: 3-9               [16, 1026, 12, 152]       304
│    │    └─Sequential: 3-10             [16, 1026, 12, 152]       78,232
│    │    └─Dropout: 3-11                [16, 1026, 12, 152]       --
│    │    └─LayerNorm: 3-12              [16, 1026, 12, 152]       304
│    └─SelfAttentionLayer: 2-3           [16, 12, 1026, 152]       --
│    │    └─AttentionLayer: 3-13         [16, 1026, 12, 152]       93,024
│    │    └─Dropout: 3-14                [16, 1026, 12, 152]       --
│    │    └─LayerNorm: 3-15              [16, 1026, 12, 152]       304
│    │    └─Sequential: 3-16             [16, 1026, 12, 152]       78,232
│    │    └─Dropout: 3-17                [16, 1026, 12, 152]       --
│    │    └─LayerNorm: 3-18              [16, 1026, 12, 152]       304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 1026, 152]       --
│    │    └─AttentionLayer: 3-19         [16, 12, 1026, 152]       93,024
│    │    └─Dropout: 3-20                [16, 12, 1026, 152]       --
│    │    └─LayerNorm: 3-21              [16, 12, 1026, 152]       304
│    │    └─Sequential: 3-22             [16, 12, 1026, 152]       78,232
│    │    └─Dropout: 3-23                [16, 12, 1026, 152]       --
│    │    └─LayerNorm: 3-24              [16, 12, 1026, 152]       304
│    └─SelfAttentionLayer: 2-5           [16, 12, 1026, 152]       --
│    │    └─AttentionLayer: 3-25         [16, 12, 1026, 152]       93,024
│    │    └─Dropout: 3-26                [16, 12, 1026, 152]       --
│    │    └─LayerNorm: 3-27              [16, 12, 1026, 152]       304
│    │    └─Sequential: 3-28             [16, 12, 1026, 152]       78,232
│    │    └─Dropout: 3-29                [16, 12, 1026, 152]       --
│    │    └─LayerNorm: 3-30              [16, 12, 1026, 152]       304
│    └─SelfAttentionLayer: 2-6           [16, 12, 1026, 152]       --
│    │    └─AttentionLayer: 3-31         [16, 12, 1026, 152]       93,024
│    │    └─Dropout: 3-32                [16, 12, 1026, 152]       --
│    │    └─LayerNorm: 3-33              [16, 12, 1026, 152]       304
│    │    └─Sequential: 3-34             [16, 12, 1026, 152]       78,232
│    │    └─Dropout: 3-35                [16, 12, 1026, 152]       --
│    │    └─LayerNorm: 3-36              [16, 12, 1026, 152]       304
├─Linear: 1-6                            [16, 1026, 12]            21,900
==========================================================================================
Total params: 2,045,172
Trainable params: 2,045,172
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 2.36
Forward/backward pass size (MB): 12596.46
Params size (MB): 4.24
Estimated Total Size (MB): 12603.06
==========================================================================================

Loss: MaskedMAELoss

2024-05-13 17:23:37.005590 Epoch 1  	Train Loss = 4.18962 Val Loss = 3.56923
2024-05-13 17:29:01.134760 Epoch 2  	Train Loss = 3.24337 Val Loss = 3.24678
2024-05-13 17:34:25.858005 Epoch 3  	Train Loss = 3.03731 Val Loss = 3.21918
2024-05-13 17:39:49.587156 Epoch 4  	Train Loss = 2.88341 Val Loss = 3.03363
2024-05-13 17:45:13.890990 Epoch 5  	Train Loss = 2.82094 Val Loss = 2.96146
2024-05-13 17:50:38.359850 Epoch 6  	Train Loss = 2.75961 Val Loss = 2.96221
2024-05-13 17:56:02.827958 Epoch 7  	Train Loss = 2.71166 Val Loss = 3.01095
2024-05-13 18:01:27.371012 Epoch 8  	Train Loss = 2.67765 Val Loss = 2.84599
2024-05-13 18:06:51.851411 Epoch 9  	Train Loss = 2.62614 Val Loss = 2.83579
2024-05-13 18:12:15.718189 Epoch 10  	Train Loss = 2.59784 Val Loss = 2.83364
2024-05-13 18:17:40.392664 Epoch 11  	Train Loss = 2.57731 Val Loss = 2.81357
2024-05-13 18:23:04.685638 Epoch 12  	Train Loss = 2.54566 Val Loss = 2.79947
2024-05-13 18:28:28.687639 Epoch 13  	Train Loss = 2.52692 Val Loss = 2.78040
2024-05-13 18:33:52.620634 Epoch 14  	Train Loss = 2.50455 Val Loss = 2.82950
2024-05-13 18:39:15.781468 Epoch 15  	Train Loss = 2.48308 Val Loss = 2.85246
2024-05-13 18:44:39.196594 Epoch 16  	Train Loss = 2.46949 Val Loss = 2.79302
2024-05-13 18:50:03.194542 Epoch 17  	Train Loss = 2.45007 Val Loss = 2.80198
2024-05-13 18:55:25.634792 Epoch 18  	Train Loss = 2.43974 Val Loss = 2.80652
2024-05-13 19:00:48.223178 Epoch 19  	Train Loss = 2.41313 Val Loss = 2.78568
2024-05-13 19:06:12.229433 Epoch 20  	Train Loss = 2.39377 Val Loss = 2.80515
2024-05-13 19:11:36.721864 Epoch 21  	Train Loss = 2.30236 Val Loss = 2.76343
2024-05-13 19:17:01.351986 Epoch 22  	Train Loss = 2.27849 Val Loss = 2.76553
2024-05-13 19:22:25.778366 Epoch 23  	Train Loss = 2.26666 Val Loss = 2.77314
2024-05-13 19:27:50.260890 Epoch 24  	Train Loss = 2.26002 Val Loss = 2.77444
2024-05-13 19:33:13.650074 Epoch 25  	Train Loss = 2.25403 Val Loss = 2.78114
2024-05-13 19:38:37.938227 Epoch 26  	Train Loss = 2.24621 Val Loss = 2.78217
2024-05-13 19:44:01.807171 Epoch 27  	Train Loss = 2.23944 Val Loss = 2.78595
2024-05-13 19:49:26.121782 Epoch 28  	Train Loss = 2.23434 Val Loss = 2.79029
2024-05-13 19:54:50.591802 Epoch 29  	Train Loss = 2.22778 Val Loss = 2.78583
2024-05-13 20:00:14.921547 Epoch 30  	Train Loss = 2.22249 Val Loss = 2.79195
2024-05-13 20:05:39.213920 Epoch 31  	Train Loss = 2.20567 Val Loss = 2.79219
Early stopping at epoch: 31
Best at epoch 21:
Train Loss = 2.30236
Train MAE = 2.24956, RMSE = 4.66164, MAPE = 5.48164
Val Loss = 2.76343
Val MAE = 2.77010, RMSE = 5.76305, MAPE = 7.25128
Model checkpoint saved to: ../saved_models/STAEformer/STAEformer-PEMSD7L-2024-05-13-17-18-07.pt
--------- Test ---------
All Steps (1-12) MAE = 2.80008, RMSE = 5.80362, MAPE = 7.07283
Step 1 MAE = 1.38984, RMSE = 2.42840, MAPE = 3.08774
Step 2 MAE = 1.91948, RMSE = 3.54681, MAPE = 4.42802
Step 3 MAE = 2.28314, RMSE = 4.38179, MAPE = 5.44864
Step 4 MAE = 2.55526, RMSE = 5.03824, MAPE = 6.26314
Step 5 MAE = 2.76680, RMSE = 5.55413, MAPE = 6.91836
Step 6 MAE = 2.93380, RMSE = 5.95684, MAPE = 7.44470
Step 7 MAE = 3.06778, RMSE = 6.27509, MAPE = 7.87204
Step 8 MAE = 3.17508, RMSE = 6.52211, MAPE = 8.20836
Step 9 MAE = 3.26506, RMSE = 6.71786, MAPE = 8.48209
Step 10 MAE = 3.34428, RMSE = 6.88170, MAPE = 8.71261
Step 11 MAE = 3.41568, RMSE = 7.02250, MAPE = 8.91227
Step 12 MAE = 3.48481, RMSE = 7.14653, MAPE = 9.09605
Inference time: 33.00 s
