PEMSBAY
Trainset:	x-(36465, 12, 325, 3)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 3)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 3)	y-(10419, 12, 325, 1)

--------- TDA_AAM ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "model_args": {
        "num_nodes": 325,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TDA_AAM                                  [16, 12, 325, 1]          312,000
├─Linear: 1-1                            [16, 12, 325, 24]         96
├─Embedding: 1-2                         [16, 12, 325, 24]         6,912
├─Embedding: 1-3                         [16, 12, 325, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 325, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 325, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 325, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 325, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 325, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 325, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 325, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 325, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 325, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 325, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 325, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 325, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 325, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 325, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 325, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 325, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 325, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 325, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 325, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 325, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 325, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 325, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 325, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 325, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 325, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 325, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 325, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 325, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 325, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 325, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 325, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 325, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 325, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 325, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 325, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 325, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 325, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 325, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 325, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 325, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 325, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 325, 152]        304
├─Linear: 1-6                            [16, 325, 12]             21,900
==========================================================================================
Total params: 1,372,260
Trainable params: 1,372,260
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.75
Forward/backward pass size (MB): 3990.11
Params size (MB): 4.24
Estimated Total Size (MB): 3995.10
==========================================================================================

Loss: MaskedMAELoss

2023-05-15 20:29:43.808263 Epoch 1  	Train Loss = 1.95382 Val Loss = 1.86063
2023-05-15 20:34:17.433488 Epoch 2  	Train Loss = 1.64858 Val Loss = 1.76352
2023-05-15 20:38:50.767369 Epoch 3  	Train Loss = 1.59032 Val Loss = 1.78452
2023-05-15 20:43:23.625902 Epoch 4  	Train Loss = 1.54854 Val Loss = 1.70071
2023-05-15 20:47:56.939915 Epoch 5  	Train Loss = 1.51869 Val Loss = 1.64094
2023-05-15 20:52:30.035240 Epoch 6  	Train Loss = 1.49515 Val Loss = 1.63758
2023-05-15 20:57:02.298997 Epoch 7  	Train Loss = 1.47847 Val Loss = 1.66015
2023-05-15 21:01:34.467441 Epoch 8  	Train Loss = 1.46470 Val Loss = 1.60665
2023-05-15 21:06:07.874525 Epoch 9  	Train Loss = 1.45046 Val Loss = 1.62839
2023-05-15 21:10:40.592130 Epoch 10  	Train Loss = 1.44121 Val Loss = 1.59413
2023-05-15 21:15:13.300742 Epoch 11  	Train Loss = 1.38238 Val Loss = 1.54750
2023-05-15 21:19:46.267562 Epoch 12  	Train Loss = 1.37133 Val Loss = 1.54463
2023-05-15 21:24:18.712334 Epoch 13  	Train Loss = 1.36594 Val Loss = 1.54205
2023-05-15 21:28:51.474046 Epoch 14  	Train Loss = 1.36054 Val Loss = 1.54016
2023-05-15 21:33:24.117146 Epoch 15  	Train Loss = 1.35635 Val Loss = 1.53985
2023-05-15 21:37:57.125175 Epoch 16  	Train Loss = 1.35241 Val Loss = 1.54452
2023-05-15 21:42:30.873766 Epoch 17  	Train Loss = 1.34927 Val Loss = 1.53792
2023-05-15 21:47:04.566465 Epoch 18  	Train Loss = 1.34560 Val Loss = 1.53626
2023-05-15 21:51:37.379351 Epoch 19  	Train Loss = 1.34263 Val Loss = 1.54082
2023-05-15 21:56:10.419703 Epoch 20  	Train Loss = 1.33999 Val Loss = 1.54085
2023-05-15 22:00:43.985109 Epoch 21  	Train Loss = 1.33673 Val Loss = 1.53985
2023-05-15 22:05:17.046834 Epoch 22  	Train Loss = 1.33422 Val Loss = 1.53543
2023-05-15 22:09:50.847823 Epoch 23  	Train Loss = 1.33138 Val Loss = 1.53941
2023-05-15 22:14:24.350371 Epoch 24  	Train Loss = 1.32835 Val Loss = 1.53872
2023-05-15 22:18:56.812038 Epoch 25  	Train Loss = 1.32597 Val Loss = 1.53695
2023-05-15 22:23:30.667650 Epoch 26  	Train Loss = 1.32346 Val Loss = 1.54076
2023-05-15 22:28:03.769918 Epoch 27  	Train Loss = 1.32115 Val Loss = 1.53977
2023-05-15 22:32:37.298751 Epoch 28  	Train Loss = 1.31910 Val Loss = 1.54012
2023-05-15 22:37:10.239653 Epoch 29  	Train Loss = 1.31621 Val Loss = 1.54221
2023-05-15 22:41:42.939971 Epoch 30  	Train Loss = 1.31428 Val Loss = 1.53994
2023-05-15 22:46:14.989046 Epoch 31  	Train Loss = 1.30372 Val Loss = 1.53563
2023-05-15 22:50:47.750402 Epoch 32  	Train Loss = 1.30259 Val Loss = 1.53478
2023-05-15 22:55:20.434794 Epoch 33  	Train Loss = 1.30215 Val Loss = 1.53449
2023-05-15 22:59:52.934090 Epoch 34  	Train Loss = 1.30104 Val Loss = 1.53456
2023-05-15 23:04:25.571164 Epoch 35  	Train Loss = 1.30042 Val Loss = 1.53545
2023-05-15 23:08:58.458243 Epoch 36  	Train Loss = 1.30065 Val Loss = 1.53477
2023-05-15 23:13:31.482332 Epoch 37  	Train Loss = 1.29982 Val Loss = 1.53634
2023-05-15 23:18:04.133624 Epoch 38  	Train Loss = 1.29948 Val Loss = 1.53609
2023-05-15 23:22:36.717069 Epoch 39  	Train Loss = 1.29980 Val Loss = 1.53551
2023-05-15 23:27:10.116406 Epoch 40  	Train Loss = 1.29865 Val Loss = 1.53666
2023-05-15 23:31:42.954287 Epoch 41  	Train Loss = 1.29851 Val Loss = 1.53575
2023-05-15 23:36:15.910199 Epoch 42  	Train Loss = 1.29842 Val Loss = 1.53468
2023-05-15 23:40:49.120147 Epoch 43  	Train Loss = 1.29777 Val Loss = 1.53418
2023-05-15 23:45:22.202947 Epoch 44  	Train Loss = 1.29746 Val Loss = 1.53632
2023-05-15 23:49:55.055657 Epoch 45  	Train Loss = 1.29715 Val Loss = 1.53597
2023-05-15 23:54:28.069471 Epoch 46  	Train Loss = 1.29677 Val Loss = 1.53627
2023-05-15 23:59:00.832075 Epoch 47  	Train Loss = 1.29687 Val Loss = 1.53603
2023-05-16 00:03:34.150884 Epoch 48  	Train Loss = 1.29643 Val Loss = 1.53612
2023-05-16 00:08:06.183594 Epoch 49  	Train Loss = 1.29603 Val Loss = 1.53687
2023-05-16 00:12:38.577691 Epoch 50  	Train Loss = 1.29512 Val Loss = 1.53635
2023-05-16 00:17:11.926919 Epoch 51  	Train Loss = 1.29555 Val Loss = 1.53672
2023-05-16 00:21:45.586534 Epoch 52  	Train Loss = 1.29452 Val Loss = 1.53532
2023-05-16 00:26:18.678175 Epoch 53  	Train Loss = 1.29443 Val Loss = 1.53492
2023-05-16 00:30:51.473897 Epoch 54  	Train Loss = 1.29407 Val Loss = 1.53541
2023-05-16 00:35:22.486660 Epoch 55  	Train Loss = 1.29353 Val Loss = 1.53595
2023-05-16 00:39:52.355324 Epoch 56  	Train Loss = 1.29367 Val Loss = 1.53682
2023-05-16 00:44:22.047196 Epoch 57  	Train Loss = 1.29297 Val Loss = 1.53630
2023-05-16 00:48:51.394609 Epoch 58  	Train Loss = 1.29250 Val Loss = 1.53708
2023-05-16 00:53:21.125436 Epoch 59  	Train Loss = 1.29241 Val Loss = 1.53591
2023-05-16 00:57:52.098755 Epoch 60  	Train Loss = 1.29235 Val Loss = 1.53704
2023-05-16 01:02:21.371736 Epoch 61  	Train Loss = 1.29161 Val Loss = 1.53606
2023-05-16 01:06:50.794997 Epoch 62  	Train Loss = 1.29124 Val Loss = 1.53618
2023-05-16 01:11:20.724199 Epoch 63  	Train Loss = 1.29200 Val Loss = 1.53638
Early stopping at epoch: 63
Best at epoch 43:
Train Loss = 1.29777
Train RMSE = 2.81578, MAE = 1.26876, MAPE = 2.68660
Val Loss = 1.53418
Val RMSE = 3.57426, MAE = 1.53459, MAPE = 3.47642
Saved Model: ../saved_models/TDA_AAM/TDA_AAM-PEMSBAY-2023-05-15-20-23-46.pt
--------- Test ---------
All Steps RMSE = 3.56530, MAE = 1.55334, MAPE = 3.47270
Step 1 RMSE = 1.57504, MAE = 0.86235, MAPE = 1.67241
Step 2 RMSE = 2.25963, MAE = 1.12928, MAPE = 2.29121
Step 3 RMSE = 2.77597, MAE = 1.30986, MAPE = 2.75634
Step 4 RMSE = 3.16912, MAE = 1.44118, MAPE = 3.11457
Step 5 RMSE = 3.45965, MAE = 1.53960, MAPE = 3.39281
Step 6 RMSE = 3.67647, MAE = 1.61830, MAPE = 3.62163
Step 7 RMSE = 3.84737, MAE = 1.68129, MAPE = 3.80969
Step 8 RMSE = 3.98152, MAE = 1.73304, MAPE = 3.96472
Step 9 RMSE = 4.08515, MAE = 1.77736, MAPE = 4.10121
Step 10 RMSE = 4.17524, MAE = 1.81558, MAPE = 4.22020
Step 11 RMSE = 4.25857, MAE = 1.84881, MAPE = 4.31743
Step 12 RMSE = 4.33635, MAE = 1.88339, MAPE = 4.41007
Inference time: 24.36 s
