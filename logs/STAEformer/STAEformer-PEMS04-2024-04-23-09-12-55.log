PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

Random seed = 233
--------- STAEformer ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 12, 307, 1]          294,720
├─Linear: 1-1                            [16, 12, 307, 24]         96
├─Embedding: 1-2                         [16, 12, 307, 24]         6,912
├─Embedding: 1-3                         [16, 12, 307, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 307, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 307, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 307, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 307, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 307, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 307, 152]        304
├─Linear: 1-6                            [16, 307, 12]             21,900
==========================================================================================
Total params: 1,354,980
Trainable params: 1,354,980
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 3769.12
Params size (MB): 4.24
Estimated Total Size (MB): 3774.06
==========================================================================================

Loss: HuberLoss

2024-04-23 09:14:13.771650 Epoch 1  	Train Loss = 32.78431 Val Loss = 24.36371
2024-04-23 09:15:30.402503 Epoch 2  	Train Loss = 23.17881 Val Loss = 22.98657
2024-04-23 09:16:47.925106 Epoch 3  	Train Loss = 21.92247 Val Loss = 20.98590
2024-04-23 09:18:05.987237 Epoch 4  	Train Loss = 20.78276 Val Loss = 20.54316
2024-04-23 09:19:22.751131 Epoch 5  	Train Loss = 20.12144 Val Loss = 20.92410
2024-04-23 09:20:38.284915 Epoch 6  	Train Loss = 19.61459 Val Loss = 21.09052
2024-04-23 09:21:56.754272 Epoch 7  	Train Loss = 19.21295 Val Loss = 19.24794
2024-04-23 09:23:14.338387 Epoch 8  	Train Loss = 18.74609 Val Loss = 21.26904
2024-04-23 09:24:31.904389 Epoch 9  	Train Loss = 18.56067 Val Loss = 20.77651
2024-04-23 09:25:48.082880 Epoch 10  	Train Loss = 18.37982 Val Loss = 20.27845
2024-04-23 09:27:04.853078 Epoch 11  	Train Loss = 18.23190 Val Loss = 18.63373
2024-04-23 09:28:20.528891 Epoch 12  	Train Loss = 17.93238 Val Loss = 18.45165
2024-04-23 09:29:35.555616 Epoch 13  	Train Loss = 17.83065 Val Loss = 18.49097
2024-04-23 09:30:50.858275 Epoch 14  	Train Loss = 17.68905 Val Loss = 18.64801
2024-04-23 09:32:06.750064 Epoch 15  	Train Loss = 17.65750 Val Loss = 18.45373
2024-04-23 09:33:22.926388 Epoch 16  	Train Loss = 16.70961 Val Loss = 17.70533
2024-04-23 09:34:39.896385 Epoch 17  	Train Loss = 16.61700 Val Loss = 17.63214
2024-04-23 09:35:56.239072 Epoch 18  	Train Loss = 16.56574 Val Loss = 17.77500
2024-04-23 09:37:12.581581 Epoch 19  	Train Loss = 16.52348 Val Loss = 17.67070
2024-04-23 09:38:29.391392 Epoch 20  	Train Loss = 16.48703 Val Loss = 17.69830
2024-04-23 09:39:47.505369 Epoch 21  	Train Loss = 16.46240 Val Loss = 17.68734
2024-04-23 09:41:05.562725 Epoch 22  	Train Loss = 16.42615 Val Loss = 17.61754
2024-04-23 09:42:22.474559 Epoch 23  	Train Loss = 16.40790 Val Loss = 17.60501
2024-04-23 09:43:37.778945 Epoch 24  	Train Loss = 16.36917 Val Loss = 17.63087
2024-04-23 09:44:52.864567 Epoch 25  	Train Loss = 16.33485 Val Loss = 17.61285
2024-04-23 09:46:08.973844 Epoch 26  	Train Loss = 16.31606 Val Loss = 17.58984
2024-04-23 09:47:25.327524 Epoch 27  	Train Loss = 16.28813 Val Loss = 17.59090
2024-04-23 09:48:43.023787 Epoch 28  	Train Loss = 16.25351 Val Loss = 17.70817
2024-04-23 09:50:00.518660 Epoch 29  	Train Loss = 16.23826 Val Loss = 17.59832
2024-04-23 09:51:16.120459 Epoch 30  	Train Loss = 16.22176 Val Loss = 17.63942
2024-04-23 09:52:34.246521 Epoch 31  	Train Loss = 16.11333 Val Loss = 17.51850
2024-04-23 09:53:51.590475 Epoch 32  	Train Loss = 16.09576 Val Loss = 17.52415
2024-04-23 09:55:08.059489 Epoch 33  	Train Loss = 16.09638 Val Loss = 17.52189
2024-04-23 09:56:23.450075 Epoch 34  	Train Loss = 16.09044 Val Loss = 17.53502
2024-04-23 09:57:38.849469 Epoch 35  	Train Loss = 16.08449 Val Loss = 17.53542
2024-04-23 09:58:55.732639 Epoch 36  	Train Loss = 16.08091 Val Loss = 17.52389
2024-04-23 10:00:11.159060 Epoch 37  	Train Loss = 16.08108 Val Loss = 17.53477
2024-04-23 10:01:26.074793 Epoch 38  	Train Loss = 16.07317 Val Loss = 17.52371
2024-04-23 10:02:41.286921 Epoch 39  	Train Loss = 16.06558 Val Loss = 17.52506
2024-04-23 10:03:57.034473 Epoch 40  	Train Loss = 16.07416 Val Loss = 17.52177
2024-04-23 10:05:12.071685 Epoch 41  	Train Loss = 16.06395 Val Loss = 17.52605
Early stopping at epoch: 41
Best at epoch 31:
Train Loss = 16.11333
Train MAE = 16.58819, RMSE = 27.75922, MAPE = 11.85110
Val Loss = 17.51850
Val MAE = 18.21686, RMSE = 30.54996, MAPE = 11.68422
Model checkpoint saved to: ../saved_models/STAEformer/STAEformer-PEMS04-2024-04-23-09-12-55.pt
--------- Test ---------
All Steps (1-12) MAE = 18.13896, RMSE = 30.00151, MAPE = 12.01845
Step 1 MAE = 16.63028, RMSE = 27.02667, MAPE = 11.06373
Step 2 MAE = 17.04876, RMSE = 27.94237, MAPE = 11.38145
Step 3 MAE = 17.42364, RMSE = 28.66539, MAPE = 11.61435
Step 4 MAE = 17.69790, RMSE = 29.20258, MAPE = 11.76737
Step 5 MAE = 17.95579, RMSE = 29.67033, MAPE = 11.92062
Step 6 MAE = 18.16145, RMSE = 30.06331, MAPE = 12.01215
Step 7 MAE = 18.36205, RMSE = 30.44337, MAPE = 12.17412
Step 8 MAE = 18.54127, RMSE = 30.75924, MAPE = 12.25146
Step 9 MAE = 18.69772, RMSE = 31.04134, MAPE = 12.34436
Step 10 MAE = 18.85791, RMSE = 31.29523, MAPE = 12.43804
Step 11 MAE = 19.04803, RMSE = 31.58779, MAPE = 12.56754
Step 12 MAE = 19.24249, RMSE = 31.89905, MAPE = 12.68603
Inference time: 7.02 s
