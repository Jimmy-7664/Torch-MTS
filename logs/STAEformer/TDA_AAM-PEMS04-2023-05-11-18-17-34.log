PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

--------- TDA_AAM ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TDA_AAM                                  [16, 12, 307, 1]          294,720
├─Linear: 1-1                            [16, 12, 307, 24]         96
├─Embedding: 1-2                         [16, 12, 307, 24]         6,912
├─Embedding: 1-3                         [16, 12, 307, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 307, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 307, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 307, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 307, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 307, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 307, 152]        304
├─Linear: 1-6                            [16, 307, 12]             21,900
==========================================================================================
Total params: 1,354,980
Trainable params: 1,354,980
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 3769.12
Params size (MB): 4.24
Estimated Total Size (MB): 3774.06
==========================================================================================

Loss: HuberLoss

2023-05-11 18:18:53.881672 Epoch 1  	Train Loss = 32.03259 Val Loss = 23.58530
2023-05-11 18:20:09.350290 Epoch 2  	Train Loss = 22.91390 Val Loss = 22.66335
2023-05-11 18:21:25.263476 Epoch 3  	Train Loss = 21.87493 Val Loss = 21.36711
2023-05-11 18:22:41.652449 Epoch 4  	Train Loss = 20.86729 Val Loss = 23.14608
2023-05-11 18:23:58.076761 Epoch 5  	Train Loss = 20.14493 Val Loss = 19.97993
2023-05-11 18:25:14.076886 Epoch 6  	Train Loss = 19.57135 Val Loss = 19.77667
2023-05-11 18:26:30.266017 Epoch 7  	Train Loss = 19.12988 Val Loss = 19.29750
2023-05-11 18:27:46.683805 Epoch 8  	Train Loss = 18.94132 Val Loss = 19.56422
2023-05-11 18:29:02.858655 Epoch 9  	Train Loss = 18.43522 Val Loss = 19.18202
2023-05-11 18:30:19.161810 Epoch 10  	Train Loss = 18.43759 Val Loss = 19.99333
2023-05-11 18:31:35.547570 Epoch 11  	Train Loss = 18.22695 Val Loss = 19.02657
2023-05-11 18:32:51.612752 Epoch 12  	Train Loss = 17.98798 Val Loss = 18.66523
2023-05-11 18:34:07.743342 Epoch 13  	Train Loss = 17.84497 Val Loss = 20.08482
2023-05-11 18:35:23.980677 Epoch 14  	Train Loss = 17.76673 Val Loss = 19.42864
2023-05-11 18:36:40.123837 Epoch 15  	Train Loss = 17.65976 Val Loss = 18.23904
2023-05-11 18:37:56.351840 Epoch 16  	Train Loss = 16.72818 Val Loss = 17.70376
2023-05-11 18:39:12.810286 Epoch 17  	Train Loss = 16.62258 Val Loss = 17.67369
2023-05-11 18:40:28.959111 Epoch 18  	Train Loss = 16.58237 Val Loss = 17.71960
2023-05-11 18:41:45.271784 Epoch 19  	Train Loss = 16.54294 Val Loss = 17.68121
2023-05-11 18:43:01.515256 Epoch 20  	Train Loss = 16.51858 Val Loss = 17.63820
2023-05-11 18:44:17.579945 Epoch 21  	Train Loss = 16.47739 Val Loss = 17.69343
2023-05-11 18:45:33.839218 Epoch 22  	Train Loss = 16.44515 Val Loss = 17.67033
2023-05-11 18:46:50.457178 Epoch 23  	Train Loss = 16.41477 Val Loss = 17.69093
2023-05-11 18:48:06.953750 Epoch 24  	Train Loss = 16.39127 Val Loss = 17.64013
2023-05-11 18:49:23.491838 Epoch 25  	Train Loss = 16.36697 Val Loss = 17.70597
2023-05-11 18:50:40.436821 Epoch 26  	Train Loss = 16.33551 Val Loss = 17.65299
2023-05-11 18:51:56.498793 Epoch 27  	Train Loss = 16.31268 Val Loss = 17.70497
2023-05-11 18:53:12.871742 Epoch 28  	Train Loss = 16.29203 Val Loss = 17.66898
2023-05-11 18:54:29.059394 Epoch 29  	Train Loss = 16.25758 Val Loss = 17.66046
2023-05-11 18:55:45.251174 Epoch 30  	Train Loss = 16.23035 Val Loss = 17.74144
2023-05-11 18:57:01.551293 Epoch 31  	Train Loss = 16.12580 Val Loss = 17.60912
2023-05-11 18:58:17.651200 Epoch 32  	Train Loss = 16.11318 Val Loss = 17.61438
2023-05-11 18:59:33.686262 Epoch 33  	Train Loss = 16.11135 Val Loss = 17.61570
2023-05-11 19:00:49.876378 Epoch 34  	Train Loss = 16.11094 Val Loss = 17.59729
2023-05-11 19:02:06.128198 Epoch 35  	Train Loss = 16.10337 Val Loss = 17.59732
2023-05-11 19:03:22.464379 Epoch 36  	Train Loss = 16.09409 Val Loss = 17.61796
2023-05-11 19:04:38.670346 Epoch 37  	Train Loss = 16.10402 Val Loss = 17.59573
2023-05-11 19:05:55.003416 Epoch 38  	Train Loss = 16.10074 Val Loss = 17.60304
2023-05-11 19:07:11.185209 Epoch 39  	Train Loss = 16.09119 Val Loss = 17.60863
2023-05-11 19:08:27.527151 Epoch 40  	Train Loss = 16.09088 Val Loss = 17.60276
2023-05-11 19:09:43.660065 Epoch 41  	Train Loss = 16.08300 Val Loss = 17.60719
2023-05-11 19:10:59.413264 Epoch 42  	Train Loss = 16.08604 Val Loss = 17.59986
2023-05-11 19:12:15.344815 Epoch 43  	Train Loss = 16.08110 Val Loss = 17.62808
2023-05-11 19:13:31.347268 Epoch 44  	Train Loss = 16.07798 Val Loss = 17.60338
2023-05-11 19:14:46.956251 Epoch 45  	Train Loss = 16.07362 Val Loss = 17.60155
2023-05-11 19:16:02.682099 Epoch 46  	Train Loss = 16.06777 Val Loss = 17.60365
2023-05-11 19:17:18.426892 Epoch 47  	Train Loss = 16.06435 Val Loss = 17.60656
2023-05-11 19:18:34.292264 Epoch 48  	Train Loss = 16.06351 Val Loss = 17.60388
2023-05-11 19:19:49.922851 Epoch 49  	Train Loss = 16.06203 Val Loss = 17.61735
2023-05-11 19:21:06.158202 Epoch 50  	Train Loss = 16.06135 Val Loss = 17.60687
2023-05-11 19:22:21.909605 Epoch 51  	Train Loss = 16.04324 Val Loss = 17.60074
2023-05-11 19:23:37.622070 Epoch 52  	Train Loss = 16.04684 Val Loss = 17.60445
2023-05-11 19:24:53.864392 Epoch 53  	Train Loss = 16.04482 Val Loss = 17.60389
2023-05-11 19:26:10.079102 Epoch 54  	Train Loss = 16.04568 Val Loss = 17.60100
2023-05-11 19:27:26.012469 Epoch 55  	Train Loss = 16.03974 Val Loss = 17.60157
2023-05-11 19:28:42.042774 Epoch 56  	Train Loss = 16.04812 Val Loss = 17.60465
2023-05-11 19:29:58.014199 Epoch 57  	Train Loss = 16.03792 Val Loss = 17.60525
Early stopping at epoch: 57
Best at epoch 37:
Train Loss = 16.10402
Train RMSE = 27.62525, MAE = 16.53051, MAPE = 11.78737
Val Loss = 17.59573
Val RMSE = 30.61679, MAE = 18.25525, MAPE = 11.69958
Saved Model: ../saved_models/TDA_AAM/TDA_AAM-PEMS04-2023-05-11-18-17-34.pt
--------- Test ---------
All Steps RMSE = 30.28502, MAE = 18.22351, MAPE = 12.00915
Step 1 RMSE = 27.03087, MAE = 16.63256, MAPE = 11.03233
Step 2 RMSE = 28.02020, MAE = 17.09026, MAPE = 11.35480
Step 3 RMSE = 28.78655, MAE = 17.47624, MAPE = 11.58603
Step 4 RMSE = 29.39523, MAE = 17.77102, MAPE = 11.75606
Step 5 RMSE = 29.91030, MAE = 18.02919, MAPE = 11.89408
Step 6 RMSE = 30.34926, MAE = 18.24592, MAPE = 12.01177
Step 7 RMSE = 30.75811, MAE = 18.45436, MAPE = 12.14339
Step 8 RMSE = 31.10659, MAE = 18.63553, MAPE = 12.24315
Step 9 RMSE = 31.43435, MAE = 18.81602, MAPE = 12.34810
Step 10 RMSE = 31.73158, MAE = 18.98943, MAPE = 12.45193
Step 11 RMSE = 32.02095, MAE = 19.16020, MAPE = 12.56860
Step 12 RMSE = 32.37096, MAE = 19.38139, MAPE = 12.71927
Inference time: 7.51 s
