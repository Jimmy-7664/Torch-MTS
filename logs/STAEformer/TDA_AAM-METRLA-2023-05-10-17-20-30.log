METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- TDA_AAM ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0003,
    "milestones": [
        20,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 200,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TDA_AAM                                  [16, 12, 207, 1]          198,720
├─Linear: 1-1                            [16, 12, 207, 24]         96
├─Embedding: 1-2                         [16, 12, 207, 24]         6,912
├─Embedding: 1-3                         [16, 12, 207, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 207, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 207, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 207, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 207, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 207, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 207, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 207, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 207, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 207, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 207, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 207, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 207, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 207, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 207, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 207, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 207, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 207, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 207, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 207, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 207, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 207, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 207, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 207, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 207, 152]        304
├─Linear: 1-6                            [16, 207, 12]             21,900
==========================================================================================
Total params: 1,258,980
Trainable params: 1,258,980
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.48
Forward/backward pass size (MB): 2541.39
Params size (MB): 4.24
Estimated Total Size (MB): 2546.11
==========================================================================================

Loss: MaskedMAELoss

2023-05-10 17:24:15.721797 Epoch 1  	Train Loss = 3.88642 Val Loss = 3.26538
2023-05-10 17:27:52.333027 Epoch 2  	Train Loss = 3.30199 Val Loss = 3.07455
2023-05-10 17:31:29.001711 Epoch 3  	Train Loss = 3.16690 Val Loss = 3.08368
2023-05-10 17:35:06.002662 Epoch 4  	Train Loss = 3.08907 Val Loss = 2.92256
2023-05-10 17:38:44.126669 Epoch 5  	Train Loss = 3.04771 Val Loss = 2.91325
2023-05-10 17:42:19.834675 Epoch 6  	Train Loss = 3.00948 Val Loss = 2.98997
2023-05-10 17:45:55.911018 Epoch 7  	Train Loss = 2.98984 Val Loss = 2.87918
2023-05-10 17:49:31.610759 Epoch 8  	Train Loss = 2.96672 Val Loss = 2.86048
2023-05-10 17:53:06.786733 Epoch 9  	Train Loss = 2.94249 Val Loss = 2.95512
2023-05-10 17:56:41.925703 Epoch 10  	Train Loss = 2.92083 Val Loss = 2.87747
2023-05-10 18:00:18.064060 Epoch 11  	Train Loss = 2.89869 Val Loss = 2.80978
2023-05-10 18:03:53.660775 Epoch 12  	Train Loss = 2.88401 Val Loss = 2.84602
2023-05-10 18:07:29.616441 Epoch 13  	Train Loss = 2.86149 Val Loss = 2.81260
2023-05-10 18:11:04.578366 Epoch 14  	Train Loss = 2.85024 Val Loss = 2.77325
2023-05-10 18:14:39.866645 Epoch 15  	Train Loss = 2.83345 Val Loss = 2.75907
2023-05-10 18:18:14.601966 Epoch 16  	Train Loss = 2.81655 Val Loss = 2.81254
2023-05-10 18:21:49.901842 Epoch 17  	Train Loss = 2.80446 Val Loss = 2.76605
2023-05-10 18:25:24.787490 Epoch 18  	Train Loss = 2.79424 Val Loss = 2.75100
2023-05-10 18:28:59.256272 Epoch 19  	Train Loss = 2.78276 Val Loss = 2.75188
2023-05-10 18:32:34.164086 Epoch 20  	Train Loss = 2.77774 Val Loss = 2.75871
2023-05-10 18:36:09.219286 Epoch 21  	Train Loss = 2.69608 Val Loss = 2.70790
2023-05-10 18:39:44.192296 Epoch 22  	Train Loss = 2.68098 Val Loss = 2.70922
2023-05-10 18:43:18.462078 Epoch 23  	Train Loss = 2.67508 Val Loss = 2.71247
2023-05-10 18:46:53.539788 Epoch 24  	Train Loss = 2.67056 Val Loss = 2.71123
2023-05-10 18:50:27.986220 Epoch 25  	Train Loss = 2.66653 Val Loss = 2.70785
2023-05-10 18:54:02.745978 Epoch 26  	Train Loss = 2.66230 Val Loss = 2.71565
2023-05-10 18:57:38.176865 Epoch 27  	Train Loss = 2.65755 Val Loss = 2.71530
2023-05-10 19:01:13.244632 Epoch 28  	Train Loss = 2.65620 Val Loss = 2.71765
2023-05-10 19:04:47.684753 Epoch 29  	Train Loss = 2.65237 Val Loss = 2.71164
2023-05-10 19:08:21.111165 Epoch 30  	Train Loss = 2.64927 Val Loss = 2.72499
2023-05-10 19:11:56.362687 Epoch 31  	Train Loss = 2.63767 Val Loss = 2.71249
2023-05-10 19:15:31.629296 Epoch 32  	Train Loss = 2.63583 Val Loss = 2.71257
2023-05-10 19:19:05.945564 Epoch 33  	Train Loss = 2.63514 Val Loss = 2.71312
2023-05-10 19:22:40.433029 Epoch 34  	Train Loss = 2.63460 Val Loss = 2.71545
2023-05-10 19:26:14.833152 Epoch 35  	Train Loss = 2.63449 Val Loss = 2.71351
2023-05-10 19:29:48.750979 Epoch 36  	Train Loss = 2.63337 Val Loss = 2.71461
2023-05-10 19:33:24.296861 Epoch 37  	Train Loss = 2.63329 Val Loss = 2.71313
2023-05-10 19:36:59.356654 Epoch 38  	Train Loss = 2.63338 Val Loss = 2.71373
2023-05-10 19:40:33.945843 Epoch 39  	Train Loss = 2.63274 Val Loss = 2.71331
2023-05-10 19:44:08.860950 Epoch 40  	Train Loss = 2.63198 Val Loss = 2.71368
2023-05-10 19:47:43.530118 Epoch 41  	Train Loss = 2.63229 Val Loss = 2.71394
2023-05-10 19:51:18.481195 Epoch 42  	Train Loss = 2.63142 Val Loss = 2.71520
2023-05-10 19:54:52.654734 Epoch 43  	Train Loss = 2.63073 Val Loss = 2.71410
2023-05-10 19:58:27.226129 Epoch 44  	Train Loss = 2.63015 Val Loss = 2.71381
2023-05-10 20:02:03.017178 Epoch 45  	Train Loss = 2.63023 Val Loss = 2.71422
2023-05-10 20:05:37.835707 Epoch 46  	Train Loss = 2.62937 Val Loss = 2.71459
2023-05-10 20:09:13.393624 Epoch 47  	Train Loss = 2.62869 Val Loss = 2.71571
2023-05-10 20:12:48.141953 Epoch 48  	Train Loss = 2.62942 Val Loss = 2.71450
2023-05-10 20:16:22.883925 Epoch 49  	Train Loss = 2.62893 Val Loss = 2.71646
2023-05-10 20:19:58.422813 Epoch 50  	Train Loss = 2.62923 Val Loss = 2.71561
2023-05-10 20:23:33.933992 Epoch 51  	Train Loss = 2.62784 Val Loss = 2.71621
2023-05-10 20:27:07.820718 Epoch 52  	Train Loss = 2.62771 Val Loss = 2.71619
2023-05-10 20:30:42.718215 Epoch 53  	Train Loss = 2.62741 Val Loss = 2.71632
2023-05-10 20:34:18.098823 Epoch 54  	Train Loss = 2.62696 Val Loss = 2.71549
2023-05-10 20:37:53.667088 Epoch 55  	Train Loss = 2.62581 Val Loss = 2.71606
Early stopping at epoch: 55
Best at epoch 25:
Train Loss = 2.66653
Train RMSE = 5.16606, MAE = 2.59631, MAPE = 6.76809
Val Loss = 2.70785
Val RMSE = 5.72548, MAE = 2.76471, MAPE = 7.61366
Saved Model: ../saved_models/TDA_AAM/TDA_AAM-METRLA-2023-05-10-17-20-30.pt
--------- Test ---------
All Steps RMSE = 5.97378, MAE = 2.93669, MAPE = 8.04621
Step 1 RMSE = 3.94951, MAE = 2.26157, MAPE = 5.49226
Step 2 RMSE = 4.64409, MAE = 2.49841, MAPE = 6.27883
Step 3 RMSE = 5.10526, MAE = 2.65226, MAPE = 6.84949
Step 4 RMSE = 5.45360, MAE = 2.77122, MAPE = 7.32319
Step 5 RMSE = 5.73472, MAE = 2.87406, MAPE = 7.74940
Step 6 RMSE = 5.99701, MAE = 2.96585, MAPE = 8.12808
Step 7 RMSE = 6.23462, MAE = 3.04547, MAPE = 8.45532
Step 8 RMSE = 6.43296, MAE = 3.11735, MAPE = 8.76886
Step 9 RMSE = 6.60441, MAE = 3.18332, MAPE = 9.04112
Step 10 RMSE = 6.75830, MAE = 3.23975, MAPE = 9.27852
Step 11 RMSE = 6.89559, MAE = 3.28893, MAPE = 9.49385
Step 12 RMSE = 7.02248, MAE = 3.34218, MAPE = 9.69587
Inference time: 18.44 s
