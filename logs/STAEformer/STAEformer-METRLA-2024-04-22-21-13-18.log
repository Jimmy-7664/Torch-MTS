METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

Random seed = 233
--------- STAEformer ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0003,
    "milestones": [
        20,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 200,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 12, 207, 1]          198,720
├─Linear: 1-1                            [16, 12, 207, 24]         96
├─Embedding: 1-2                         [16, 12, 207, 24]         6,912
├─Embedding: 1-3                         [16, 12, 207, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 207, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 207, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 207, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 207, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 207, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 207, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 207, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 207, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 207, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 207, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 207, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 207, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 207, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 207, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 207, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 207, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 207, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 207, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 207, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 207, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 207, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 207, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 207, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 207, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 207, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 207, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 207, 152]        304
├─Linear: 1-6                            [16, 207, 12]             21,900
==========================================================================================
Total params: 1,258,980
Trainable params: 1,258,980
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.48
Forward/backward pass size (MB): 2541.39
Params size (MB): 4.24
Estimated Total Size (MB): 2546.11
==========================================================================================

Loss: MaskedMAELoss

2024-04-22 21:14:59.807997 Epoch 1  	Train Loss = 3.87975 Val Loss = 3.19129
2024-04-22 21:16:39.777548 Epoch 2  	Train Loss = 3.31459 Val Loss = 3.05784
2024-04-22 21:18:19.084551 Epoch 3  	Train Loss = 3.16949 Val Loss = 2.96489
2024-04-22 21:19:58.099538 Epoch 4  	Train Loss = 3.09845 Val Loss = 2.94908
2024-04-22 21:21:37.214385 Epoch 5  	Train Loss = 3.05386 Val Loss = 2.91948
2024-04-22 21:23:16.475208 Epoch 6  	Train Loss = 3.02696 Val Loss = 2.88421
2024-04-22 21:24:56.543187 Epoch 7  	Train Loss = 2.99445 Val Loss = 2.98051
2024-04-22 21:26:35.727395 Epoch 8  	Train Loss = 2.96581 Val Loss = 2.85075
2024-04-22 21:28:15.385312 Epoch 9  	Train Loss = 2.94113 Val Loss = 2.87191
2024-04-22 21:29:54.528260 Epoch 10  	Train Loss = 2.91890 Val Loss = 2.79999
2024-04-22 21:31:33.292120 Epoch 11  	Train Loss = 2.88959 Val Loss = 2.79682
2024-04-22 21:33:12.478895 Epoch 12  	Train Loss = 2.86364 Val Loss = 2.81038
2024-04-22 21:34:51.367575 Epoch 13  	Train Loss = 2.84452 Val Loss = 2.77654
2024-04-22 21:36:30.713201 Epoch 14  	Train Loss = 2.82792 Val Loss = 2.77200
2024-04-22 21:38:09.310588 Epoch 15  	Train Loss = 2.81117 Val Loss = 2.77798
2024-04-22 21:39:48.218224 Epoch 16  	Train Loss = 2.79900 Val Loss = 2.75342
2024-04-22 21:41:26.781195 Epoch 17  	Train Loss = 2.78770 Val Loss = 2.78076
2024-04-22 21:43:05.722544 Epoch 18  	Train Loss = 2.77825 Val Loss = 2.75341
2024-04-22 21:44:44.597582 Epoch 19  	Train Loss = 2.77151 Val Loss = 2.73962
2024-04-22 21:46:23.022625 Epoch 20  	Train Loss = 2.76322 Val Loss = 2.72952
2024-04-22 21:48:01.411546 Epoch 21  	Train Loss = 2.67824 Val Loss = 2.70453
2024-04-22 21:49:39.932427 Epoch 22  	Train Loss = 2.66269 Val Loss = 2.71068
2024-04-22 21:51:18.926278 Epoch 23  	Train Loss = 2.65480 Val Loss = 2.70386
2024-04-22 21:52:58.144242 Epoch 24  	Train Loss = 2.64993 Val Loss = 2.71191
2024-04-22 21:54:36.894114 Epoch 25  	Train Loss = 2.64627 Val Loss = 2.71528
2024-04-22 21:56:16.416906 Epoch 26  	Train Loss = 2.64134 Val Loss = 2.71289
2024-04-22 21:57:55.263772 Epoch 27  	Train Loss = 2.63715 Val Loss = 2.71647
2024-04-22 21:59:34.246192 Epoch 28  	Train Loss = 2.63410 Val Loss = 2.72761
2024-04-22 22:01:12.710972 Epoch 29  	Train Loss = 2.63059 Val Loss = 2.71648
2024-04-22 22:02:51.108872 Epoch 30  	Train Loss = 2.62777 Val Loss = 2.71799
2024-04-22 22:04:29.508000 Epoch 31  	Train Loss = 2.61563 Val Loss = 2.71678
2024-04-22 22:06:08.252094 Epoch 32  	Train Loss = 2.61300 Val Loss = 2.71930
2024-04-22 22:07:46.966290 Epoch 33  	Train Loss = 2.61152 Val Loss = 2.71808
Early stopping at epoch: 33
Best at epoch 23:
Train Loss = 2.65480
Train MAE = 2.62358, RMSE = 5.24140, MAPE = 6.84225
Val Loss = 2.70386
Val MAE = 2.75380, RMSE = 5.69381, MAPE = 7.57859
Model checkpoint saved to: ../saved_models/STAEformer/STAEformer-METRLA-2024-04-22-21-13-18.pt
--------- Test ---------
All Steps (1-12) MAE = 2.93405, RMSE = 5.97559, MAPE = 8.09450
Step 1 MAE = 2.27380, RMSE = 3.98875, MAPE = 5.57772
Step 2 MAE = 2.49969, RMSE = 4.65225, MAPE = 6.33886
Step 3 MAE = 2.65383, RMSE = 5.10288, MAPE = 6.91635
Step 4 MAE = 2.76929, RMSE = 5.45793, MAPE = 7.39175
Step 5 MAE = 2.86803, RMSE = 5.73519, MAPE = 7.79212
Step 6 MAE = 2.96008, RMSE = 6.00099, MAPE = 8.17503
Step 7 MAE = 3.03550, RMSE = 6.23064, MAPE = 8.47086
Step 8 MAE = 3.10683, RMSE = 6.41788, MAPE = 8.76328
Step 9 MAE = 3.17473, RMSE = 6.58415, MAPE = 9.05469
Step 10 MAE = 3.23348, RMSE = 6.75266, MAPE = 9.31892
Step 11 MAE = 3.28771, RMSE = 6.90438, MAPE = 9.55355
Step 12 MAE = 3.34571, RMSE = 7.04116, MAPE = 9.78125
Inference time: 8.03 s
