PEMSD7M
Trainset:	x-(7589, 12, 228, 3)	y-(7589, 12, 228, 1)
Valset:  	x-(2530, 12, 228, 3)  	y-(2530, 12, 228, 1)
Testset:	x-(2530, 12, 228, 3)	y-(2530, 12, 228, 1)

Random seed = 233
--------- STAEformer ---------
{
    "num_nodes": 228,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        20,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 200,
    "loss": "mask_mae",
    "model_args": {
        "num_nodes": 228,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "days_per_week": 5,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 12, 228, 1]          218,880
├─Linear: 1-1                            [16, 12, 228, 24]         96
├─Embedding: 1-2                         [16, 12, 228, 24]         6,912
├─Embedding: 1-3                         [16, 12, 228, 24]         120
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 228, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 228, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 228, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 228, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 228, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 228, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 228, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 228, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 228, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 228, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 228, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 228, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 228, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 228, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 228, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 228, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 228, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 228, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 228, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 228, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 228, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 228, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 228, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 228, 152]        304
├─Linear: 1-6                            [16, 228, 12]             21,900
==========================================================================================
Total params: 1,279,092
Trainable params: 1,279,092
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.53
Forward/backward pass size (MB): 2799.21
Params size (MB): 4.24
Estimated Total Size (MB): 2803.98
==========================================================================================

Loss: MaskedMAELoss

2024-05-10 17:23:54.566227 Epoch 1  	Train Loss = 4.00124 Val Loss = 3.13316
2024-05-10 17:24:32.107937 Epoch 2  	Train Loss = 2.92006 Val Loss = 2.87362
2024-05-10 17:25:10.295856 Epoch 3  	Train Loss = 2.68170 Val Loss = 2.76694
2024-05-10 17:25:48.222855 Epoch 4  	Train Loss = 2.54000 Val Loss = 2.77622
2024-05-10 17:26:26.188980 Epoch 5  	Train Loss = 2.48122 Val Loss = 2.68555
2024-05-10 17:27:04.620048 Epoch 6  	Train Loss = 2.43401 Val Loss = 2.84377
2024-05-10 17:27:42.402197 Epoch 7  	Train Loss = 2.39387 Val Loss = 2.70519
2024-05-10 17:28:20.411600 Epoch 8  	Train Loss = 2.37395 Val Loss = 2.62881
2024-05-10 17:28:58.442526 Epoch 9  	Train Loss = 2.32837 Val Loss = 2.63899
2024-05-10 17:29:36.484476 Epoch 10  	Train Loss = 2.30301 Val Loss = 2.66639
2024-05-10 17:30:14.425995 Epoch 11  	Train Loss = 2.27486 Val Loss = 2.64454
2024-05-10 17:30:52.255076 Epoch 12  	Train Loss = 2.24817 Val Loss = 2.63917
2024-05-10 17:31:30.082853 Epoch 13  	Train Loss = 2.23094 Val Loss = 2.64897
2024-05-10 17:32:07.720294 Epoch 14  	Train Loss = 2.21038 Val Loss = 2.60310
2024-05-10 17:32:45.309587 Epoch 15  	Train Loss = 2.17984 Val Loss = 2.64433
2024-05-10 17:33:22.991120 Epoch 16  	Train Loss = 2.17064 Val Loss = 2.61770
2024-05-10 17:34:00.865415 Epoch 17  	Train Loss = 2.13935 Val Loss = 2.63211
2024-05-10 17:34:38.685798 Epoch 18  	Train Loss = 2.12986 Val Loss = 2.62039
2024-05-10 17:35:16.439792 Epoch 19  	Train Loss = 2.10363 Val Loss = 2.59936
2024-05-10 17:35:54.077405 Epoch 20  	Train Loss = 2.08169 Val Loss = 2.62590
2024-05-10 17:36:31.640804 Epoch 21  	Train Loss = 1.95748 Val Loss = 2.56925
2024-05-10 17:37:09.627966 Epoch 22  	Train Loss = 1.92453 Val Loss = 2.56632
2024-05-10 17:37:47.561388 Epoch 23  	Train Loss = 1.91241 Val Loss = 2.57682
2024-05-10 17:38:25.360822 Epoch 24  	Train Loss = 1.90273 Val Loss = 2.57676
2024-05-10 17:39:03.149542 Epoch 25  	Train Loss = 1.89305 Val Loss = 2.58975
2024-05-10 17:39:41.028273 Epoch 26  	Train Loss = 1.88485 Val Loss = 2.58962
2024-05-10 17:40:18.868573 Epoch 27  	Train Loss = 1.87645 Val Loss = 2.59323
2024-05-10 17:40:56.633529 Epoch 28  	Train Loss = 1.87045 Val Loss = 2.59642
2024-05-10 17:41:34.289526 Epoch 29  	Train Loss = 1.86211 Val Loss = 2.59219
2024-05-10 17:42:11.888806 Epoch 30  	Train Loss = 1.85594 Val Loss = 2.60206
2024-05-10 17:42:49.466251 Epoch 31  	Train Loss = 1.83601 Val Loss = 2.60115
2024-05-10 17:43:27.422362 Epoch 32  	Train Loss = 1.83334 Val Loss = 2.59836
Early stopping at epoch: 32
Best at epoch 22:
Train Loss = 1.92453
Train MAE = 1.88026, RMSE = 3.82595, MAPE = 4.41740
Val Loss = 2.56632
Val MAE = 2.57386, RMSE = 5.39358, MAPE = 6.74721
Model checkpoint saved to: ../saved_models/STAEformer/STAEformer-PEMSD7M-2024-05-10-17-23-15.pt
--------- Test ---------
All Steps (1-12) MAE = 2.57383, RMSE = 5.36367, MAPE = 6.47547
Step 1 MAE = 1.31474, RMSE = 2.28200, MAPE = 2.98748
Step 2 MAE = 1.79463, RMSE = 3.32133, MAPE = 4.20481
Step 3 MAE = 2.12561, RMSE = 4.10615, MAPE = 5.11351
Step 4 MAE = 2.36800, RMSE = 4.70525, MAPE = 5.81656
Step 5 MAE = 2.55283, RMSE = 5.16606, MAPE = 6.36733
Step 6 MAE = 2.69912, RMSE = 5.52847, MAPE = 6.81161
Step 7 MAE = 2.81785, RMSE = 5.81532, MAPE = 7.17071
Step 8 MAE = 2.90961, RMSE = 6.02937, MAPE = 7.45841
Step 9 MAE = 2.98468, RMSE = 6.19382, MAPE = 7.68697
Step 10 MAE = 3.04684, RMSE = 6.31975, MAPE = 7.86129
Step 11 MAE = 3.10563, RMSE = 6.43708, MAPE = 8.03222
Step 12 MAE = 3.16642, RMSE = 6.54424, MAPE = 8.19473
Inference time: 3.46 s
