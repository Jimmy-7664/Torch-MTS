PEMS07
Trainset:	x-(16921, 12, 883, 3)	y-(16921, 12, 883, 1)
Valset:  	x-(5640, 12, 883, 3)  	y-(5640, 12, 883, 1)
Testset:	x-(5640, 12, 883, 3)	y-(5640, 12, 883, 1)

--------- TDA_AAM ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.001,
    "milestones": [
        15,
        35,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 883,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TDA_AAM                                  [16, 12, 883, 1]          847,680
├─Linear: 1-1                            [16, 12, 883, 24]         96
├─Embedding: 1-2                         [16, 12, 883, 24]         6,912
├─Embedding: 1-3                         [16, 12, 883, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 883, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 883, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 883, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 883, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 883, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 883, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 883, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 883, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 883, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 883, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 883, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 883, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 883, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 883, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 883, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 883, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 883, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 883, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 883, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 883, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 883, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 883, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 883, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 883, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 883, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 883, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 883, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 883, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 883, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 883, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 883, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 883, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 883, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 883, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 883, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 883, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 883, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 883, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 883, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 883, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 883, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 883, 152]        304
├─Linear: 1-6                            [16, 883, 12]             21,900
==========================================================================================
Total params: 1,907,940
Trainable params: 1,907,940
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 2.03
Forward/backward pass size (MB): 10840.81
Params size (MB): 4.24
Estimated Total Size (MB): 10847.09
==========================================================================================

Loss: HuberLoss

2023-05-16 01:33:00.255082 Epoch 1  	Train Loss = 33.47577 Val Loss = 23.26331
2023-05-16 01:41:24.590704 Epoch 2  	Train Loss = 24.31409 Val Loss = 23.32408
2023-05-16 01:49:48.533613 Epoch 3  	Train Loss = 22.86663 Val Loss = 24.81424
2023-05-16 01:58:12.549311 Epoch 4  	Train Loss = 21.82562 Val Loss = 21.47680
2023-05-16 02:06:36.522813 Epoch 5  	Train Loss = 21.25302 Val Loss = 21.57807
2023-05-16 02:15:00.795617 Epoch 6  	Train Loss = 20.91313 Val Loss = 21.25037
2023-05-16 02:23:24.657647 Epoch 7  	Train Loss = 20.53983 Val Loss = 20.53379
2023-05-16 02:31:48.422923 Epoch 8  	Train Loss = 20.22042 Val Loss = 20.35374
2023-05-16 02:40:12.512172 Epoch 9  	Train Loss = 20.08455 Val Loss = 20.38699
2023-05-16 02:48:36.181352 Epoch 10  	Train Loss = 19.73156 Val Loss = 21.12303
2023-05-16 02:57:00.063903 Epoch 11  	Train Loss = 19.60032 Val Loss = 19.68903
2023-05-16 03:05:23.961369 Epoch 12  	Train Loss = 19.38440 Val Loss = 19.90623
2023-05-16 03:13:47.303607 Epoch 13  	Train Loss = 19.32078 Val Loss = 21.11348
2023-05-16 03:22:10.168593 Epoch 14  	Train Loss = 19.15334 Val Loss = 19.77570
2023-05-16 03:30:33.099518 Epoch 15  	Train Loss = 19.00476 Val Loss = 19.38562
2023-05-16 03:38:56.386010 Epoch 16  	Train Loss = 18.03308 Val Loss = 18.83816
2023-05-16 03:47:20.128205 Epoch 17  	Train Loss = 17.92205 Val Loss = 18.80788
2023-05-16 03:55:43.275349 Epoch 18  	Train Loss = 17.87548 Val Loss = 18.66773
2023-05-16 04:04:06.329642 Epoch 19  	Train Loss = 17.83524 Val Loss = 18.68178
2023-05-16 04:12:29.590392 Epoch 20  	Train Loss = 17.79720 Val Loss = 18.68057
2023-05-16 04:20:52.500753 Epoch 21  	Train Loss = 17.77000 Val Loss = 18.62573
2023-05-16 04:29:15.490172 Epoch 22  	Train Loss = 17.73734 Val Loss = 18.69959
2023-05-16 04:37:38.404026 Epoch 23  	Train Loss = 17.70881 Val Loss = 18.56632
2023-05-16 04:46:01.241280 Epoch 24  	Train Loss = 17.68158 Val Loss = 18.66962
2023-05-16 04:54:23.964812 Epoch 25  	Train Loss = 17.65584 Val Loss = 18.65182
2023-05-16 05:02:46.968686 Epoch 26  	Train Loss = 17.63328 Val Loss = 18.57795
2023-05-16 05:11:10.009228 Epoch 27  	Train Loss = 17.61062 Val Loss = 18.60140
2023-05-16 05:19:33.157834 Epoch 28  	Train Loss = 17.59017 Val Loss = 18.59199
2023-05-16 05:27:55.869228 Epoch 29  	Train Loss = 17.56822 Val Loss = 18.58380
2023-05-16 05:36:18.496470 Epoch 30  	Train Loss = 17.55148 Val Loss = 18.54358
2023-05-16 05:44:41.284525 Epoch 31  	Train Loss = 17.53566 Val Loss = 18.58460
2023-05-16 05:53:04.029355 Epoch 32  	Train Loss = 17.51806 Val Loss = 18.56431
2023-05-16 06:01:26.770317 Epoch 33  	Train Loss = 17.49873 Val Loss = 18.60088
2023-05-16 06:09:49.593950 Epoch 34  	Train Loss = 17.48369 Val Loss = 18.51723
2023-05-16 06:18:12.568821 Epoch 35  	Train Loss = 17.46946 Val Loss = 18.64452
2023-05-16 06:26:35.515094 Epoch 36  	Train Loss = 17.34356 Val Loss = 18.40955
2023-05-16 06:34:57.813196 Epoch 37  	Train Loss = 17.33048 Val Loss = 18.42490
2023-05-16 06:43:20.320361 Epoch 38  	Train Loss = 17.32671 Val Loss = 18.44416
2023-05-16 06:51:42.934818 Epoch 39  	Train Loss = 17.32161 Val Loss = 18.42023
2023-05-16 07:00:05.729844 Epoch 40  	Train Loss = 17.31916 Val Loss = 18.42730
2023-05-16 07:08:28.918877 Epoch 41  	Train Loss = 17.31497 Val Loss = 18.41420
2023-05-16 07:16:51.889577 Epoch 42  	Train Loss = 17.31300 Val Loss = 18.42183
2023-05-16 07:25:14.751367 Epoch 43  	Train Loss = 17.30813 Val Loss = 18.42156
2023-05-16 07:33:37.753106 Epoch 44  	Train Loss = 17.30343 Val Loss = 18.40711
2023-05-16 07:42:00.504004 Epoch 45  	Train Loss = 17.30274 Val Loss = 18.40196
2023-05-16 07:50:23.297101 Epoch 46  	Train Loss = 17.29807 Val Loss = 18.40524
2023-05-16 07:58:46.121774 Epoch 47  	Train Loss = 17.29589 Val Loss = 18.40686
2023-05-16 08:07:09.166200 Epoch 48  	Train Loss = 17.29302 Val Loss = 18.39333
2023-05-16 08:15:32.273538 Epoch 49  	Train Loss = 17.28846 Val Loss = 18.40587
2023-05-16 08:23:55.533749 Epoch 50  	Train Loss = 17.28595 Val Loss = 18.39845
2023-05-16 08:32:18.263614 Epoch 51  	Train Loss = 17.27278 Val Loss = 18.40980
2023-05-16 08:40:40.992879 Epoch 52  	Train Loss = 17.27303 Val Loss = 18.40492
2023-05-16 08:49:03.881647 Epoch 53  	Train Loss = 17.27217 Val Loss = 18.40255
2023-05-16 08:57:26.565268 Epoch 54  	Train Loss = 17.27109 Val Loss = 18.40549
2023-05-16 09:05:49.011633 Epoch 55  	Train Loss = 17.27029 Val Loss = 18.40334
2023-05-16 09:14:11.506103 Epoch 56  	Train Loss = 17.26960 Val Loss = 18.40776
2023-05-16 09:22:34.266125 Epoch 57  	Train Loss = 17.26931 Val Loss = 18.40823
2023-05-16 09:30:56.713582 Epoch 58  	Train Loss = 17.26907 Val Loss = 18.41106
2023-05-16 09:39:19.243506 Epoch 59  	Train Loss = 17.26889 Val Loss = 18.40815
2023-05-16 09:47:38.007223 Epoch 60  	Train Loss = 17.26820 Val Loss = 18.40839
2023-05-16 09:55:55.887450 Epoch 61  	Train Loss = 17.26858 Val Loss = 18.40805
2023-05-16 10:04:14.220485 Epoch 62  	Train Loss = 17.26788 Val Loss = 18.40416
2023-05-16 10:12:32.263035 Epoch 63  	Train Loss = 17.26778 Val Loss = 18.40691
2023-05-16 10:20:49.977710 Epoch 64  	Train Loss = 17.26831 Val Loss = 18.41238
2023-05-16 10:29:07.601181 Epoch 65  	Train Loss = 17.26707 Val Loss = 18.40841
2023-05-16 10:37:25.734437 Epoch 66  	Train Loss = 17.26663 Val Loss = 18.40204
2023-05-16 10:45:43.725296 Epoch 67  	Train Loss = 17.26646 Val Loss = 18.40863
2023-05-16 10:54:01.748840 Epoch 68  	Train Loss = 17.26758 Val Loss = 18.39909
Early stopping at epoch: 68
Best at epoch 48:
Train Loss = 17.29302
Train RMSE = 30.34041, MAE = 17.58912, MAPE = 7.68930
Val Loss = 18.39333
Val RMSE = 32.39851, MAE = 18.92163, MAPE = 8.27034
Saved Model: ../saved_models/TDA_AAM/TDA_AAM-PEMS07-2023-05-16-01-24-27.pt
--------- Test ---------
All Steps RMSE = 32.59617, MAE = 19.14017, MAPE = 8.01024
Step 1 RMSE = 26.94828, MAE = 16.46571, MAPE = 6.95166
Step 2 RMSE = 28.86285, MAE = 17.29953, MAPE = 7.27224
Step 3 RMSE = 30.14170, MAE = 17.91116, MAPE = 7.50060
Step 4 RMSE = 31.13663, MAE = 18.40183, MAPE = 7.69330
Step 5 RMSE = 31.96861, MAE = 18.81717, MAPE = 7.85853
Step 6 RMSE = 32.69526, MAE = 19.19000, MAPE = 8.01109
Step 7 RMSE = 33.35000, MAE = 19.53370, MAPE = 8.15378
Step 8 RMSE = 33.92455, MAE = 19.84074, MAPE = 8.28578
Step 9 RMSE = 34.45315, MAE = 20.12489, MAPE = 8.40139
Step 10 RMSE = 34.94487, MAE = 20.39726, MAPE = 8.52766
Step 11 RMSE = 35.44984, MAE = 20.68550, MAPE = 8.65728
Step 12 RMSE = 35.96887, MAE = 21.01154, MAPE = 8.80860
Inference time: 50.26 s
