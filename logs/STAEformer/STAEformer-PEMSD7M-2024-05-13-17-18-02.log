PEMSD7M
Trainset:	x-(7589, 12, 228, 3)	y-(7589, 12, 228, 1)
Valset:  	x-(2530, 12, 228, 3)  	y-(2530, 12, 228, 1)
Testset:	x-(2530, 12, 228, 3)	y-(2530, 12, 228, 1)

Random seed = 233
--------- STAEformer ---------
{
    "num_nodes": 228,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0003,
    "milestones": [
        20,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 200,
    "model_args": {
        "num_nodes": 228,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "days_per_week": 5,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 12, 228, 1]          218,880
├─Linear: 1-1                            [16, 12, 228, 24]         96
├─Embedding: 1-2                         [16, 12, 228, 24]         6,912
├─Embedding: 1-3                         [16, 12, 228, 24]         120
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 228, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 228, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 228, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 228, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 228, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 228, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 228, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 228, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 228, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 228, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 228, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 228, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 228, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 228, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 228, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 228, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 228, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 228, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 228, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 228, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 228, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 228, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 228, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 228, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 228, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 228, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 228, 152]        304
├─Linear: 1-6                            [16, 228, 12]             21,900
==========================================================================================
Total params: 1,279,092
Trainable params: 1,279,092
Non-trainable params: 0
Total mult-adds (M): 16.96
==========================================================================================
Input size (MB): 0.53
Forward/backward pass size (MB): 2799.21
Params size (MB): 4.24
Estimated Total Size (MB): 2803.98
==========================================================================================

Loss: MaskedMAELoss

2024-05-13 17:18:41.586635 Epoch 1  	Train Loss = 3.99969 Val Loss = 3.14765
2024-05-13 17:19:18.852976 Epoch 2  	Train Loss = 2.97264 Val Loss = 2.97530
2024-05-13 17:19:56.097946 Epoch 3  	Train Loss = 2.73813 Val Loss = 2.82109
2024-05-13 17:20:33.431300 Epoch 4  	Train Loss = 2.59974 Val Loss = 2.82479
2024-05-13 17:21:10.733302 Epoch 5  	Train Loss = 2.54586 Val Loss = 2.86843
2024-05-13 17:21:48.043184 Epoch 6  	Train Loss = 2.52929 Val Loss = 2.78709
2024-05-13 17:22:26.165519 Epoch 7  	Train Loss = 2.45160 Val Loss = 2.67087
2024-05-13 17:23:03.431289 Epoch 8  	Train Loss = 2.43644 Val Loss = 2.66611
2024-05-13 17:23:41.149898 Epoch 9  	Train Loss = 2.40302 Val Loss = 2.63866
2024-05-13 17:24:18.608043 Epoch 10  	Train Loss = 2.38342 Val Loss = 2.67302
2024-05-13 17:24:56.043841 Epoch 11  	Train Loss = 2.35486 Val Loss = 2.64913
2024-05-13 17:25:33.351353 Epoch 12  	Train Loss = 2.34270 Val Loss = 2.64246
2024-05-13 17:26:10.598557 Epoch 13  	Train Loss = 2.31741 Val Loss = 2.63812
2024-05-13 17:26:47.993091 Epoch 14  	Train Loss = 2.30917 Val Loss = 2.61923
2024-05-13 17:27:25.356089 Epoch 15  	Train Loss = 2.28167 Val Loss = 2.63678
2024-05-13 17:28:02.743701 Epoch 16  	Train Loss = 2.27090 Val Loss = 2.59611
2024-05-13 17:28:40.470674 Epoch 17  	Train Loss = 2.25277 Val Loss = 2.64877
2024-05-13 17:29:17.814616 Epoch 18  	Train Loss = 2.24205 Val Loss = 2.61808
2024-05-13 17:29:55.069337 Epoch 19  	Train Loss = 2.22224 Val Loss = 2.62473
2024-05-13 17:30:33.426209 Epoch 20  	Train Loss = 2.21903 Val Loss = 2.61487
2024-05-13 17:31:10.713172 Epoch 21  	Train Loss = 2.11523 Val Loss = 2.55972
2024-05-13 17:31:47.973304 Epoch 22  	Train Loss = 2.09298 Val Loss = 2.56075
2024-05-13 17:32:25.191977 Epoch 23  	Train Loss = 2.08610 Val Loss = 2.56148
2024-05-13 17:33:02.755589 Epoch 24  	Train Loss = 2.07882 Val Loss = 2.55819
2024-05-13 17:33:39.972977 Epoch 25  	Train Loss = 2.07303 Val Loss = 2.56632
2024-05-13 17:34:17.202229 Epoch 26  	Train Loss = 2.06806 Val Loss = 2.56388
2024-05-13 17:34:54.474318 Epoch 27  	Train Loss = 2.06194 Val Loss = 2.58052
2024-05-13 17:35:31.817833 Epoch 28  	Train Loss = 2.05841 Val Loss = 2.57519
2024-05-13 17:36:09.142704 Epoch 29  	Train Loss = 2.05368 Val Loss = 2.57458
2024-05-13 17:36:46.693273 Epoch 30  	Train Loss = 2.04923 Val Loss = 2.57509
2024-05-13 17:37:24.003778 Epoch 31  	Train Loss = 2.03344 Val Loss = 2.57308
2024-05-13 17:38:01.277588 Epoch 32  	Train Loss = 2.03207 Val Loss = 2.57140
2024-05-13 17:38:38.471392 Epoch 33  	Train Loss = 2.03164 Val Loss = 2.57142
2024-05-13 17:39:15.655756 Epoch 34  	Train Loss = 2.02952 Val Loss = 2.57327
Early stopping at epoch: 34
Best at epoch 24:
Train Loss = 2.07882
Train MAE = 2.04096, RMSE = 4.21861, MAPE = 4.89481
Val Loss = 2.55819
Val MAE = 2.56568, RMSE = 5.37466, MAPE = 6.73050
Model checkpoint saved to: ../saved_models/STAEformer/STAEformer-PEMSD7M-2024-05-13-17-18-02.pt
--------- Test ---------
All Steps (1-12) MAE = 2.55692, RMSE = 5.30389, MAPE = 6.42856
Step 1 MAE = 1.31758, RMSE = 2.28506, MAPE = 2.97839
Step 2 MAE = 1.79753, RMSE = 3.31364, MAPE = 4.18933
Step 3 MAE = 2.12198, RMSE = 4.07452, MAPE = 5.08215
Step 4 MAE = 2.35663, RMSE = 4.65243, MAPE = 5.76765
Step 5 MAE = 2.53378, RMSE = 5.09249, MAPE = 6.31148
Step 6 MAE = 2.67313, RMSE = 5.44353, MAPE = 6.73956
Step 7 MAE = 2.78633, RMSE = 5.72695, MAPE = 7.09065
Step 8 MAE = 2.88177, RMSE = 5.94842, MAPE = 7.37614
Step 9 MAE = 2.96013, RMSE = 6.12107, MAPE = 7.62158
Step 10 MAE = 3.02759, RMSE = 6.26322, MAPE = 7.81991
Step 11 MAE = 3.08465, RMSE = 6.37905, MAPE = 7.99657
Step 12 MAE = 3.14193, RMSE = 6.48010, MAPE = 8.16928
Inference time: 3.43 s
