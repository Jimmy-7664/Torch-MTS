PEMSD7L
Trainset:	x-(7589, 12, 1026, 2)	y-(7589, 12, 1026, 1)
Valset:  	x-(2530, 12, 1026, 2)  	y-(2530, 12, 1026, 1)
Testset:	x-(2530, 12, 1026, 2)	y-(2530, 12, 1026, 1)

Random seed = 233
--------- Transformer ---------
{
    "num_nodes": 1026,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        40
    ],
    "batch_size": 16,
    "max_epochs": 200,
    "model_args": {
        "num_nodes": 1026,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "with_spatial": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Transformer                              [16, 12, 1026, 1]         --
├─Linear: 1-1                            [16, 12, 1026, 64]        128
├─Linear: 1-2                            [16, 12, 1026, 64]        128
├─ModuleList: 1-3                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 1026, 64]        --
│    │    └─AttentionLayer: 3-1          [16, 1026, 12, 64]        16,640
│    │    └─Dropout: 3-2                 [16, 1026, 12, 64]        --
│    │    └─LayerNorm: 3-3               [16, 1026, 12, 64]        128
│    │    └─Sequential: 3-4              [16, 1026, 12, 64]        33,088
│    │    └─Dropout: 3-5                 [16, 1026, 12, 64]        --
│    │    └─LayerNorm: 3-6               [16, 1026, 12, 64]        128
│    └─SelfAttentionLayer: 2-2           [16, 12, 1026, 64]        --
│    │    └─AttentionLayer: 3-7          [16, 1026, 12, 64]        16,640
│    │    └─Dropout: 3-8                 [16, 1026, 12, 64]        --
│    │    └─LayerNorm: 3-9               [16, 1026, 12, 64]        128
│    │    └─Sequential: 3-10             [16, 1026, 12, 64]        33,088
│    │    └─Dropout: 3-11                [16, 1026, 12, 64]        --
│    │    └─LayerNorm: 3-12              [16, 1026, 12, 64]        128
│    └─SelfAttentionLayer: 2-3           [16, 12, 1026, 64]        --
│    │    └─AttentionLayer: 3-13         [16, 1026, 12, 64]        16,640
│    │    └─Dropout: 3-14                [16, 1026, 12, 64]        --
│    │    └─LayerNorm: 3-15              [16, 1026, 12, 64]        128
│    │    └─Sequential: 3-16             [16, 1026, 12, 64]        33,088
│    │    └─Dropout: 3-17                [16, 1026, 12, 64]        --
│    │    └─LayerNorm: 3-18              [16, 1026, 12, 64]        128
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 1026, 64]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 1026, 64]        16,640
│    │    └─Dropout: 3-20                [16, 12, 1026, 64]        --
│    │    └─LayerNorm: 3-21              [16, 12, 1026, 64]        128
│    │    └─Sequential: 3-22             [16, 12, 1026, 64]        33,088
│    │    └─Dropout: 3-23                [16, 12, 1026, 64]        --
│    │    └─LayerNorm: 3-24              [16, 12, 1026, 64]        128
│    └─SelfAttentionLayer: 2-5           [16, 12, 1026, 64]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 1026, 64]        16,640
│    │    └─Dropout: 3-26                [16, 12, 1026, 64]        --
│    │    └─LayerNorm: 3-27              [16, 12, 1026, 64]        128
│    │    └─Sequential: 3-28             [16, 12, 1026, 64]        33,088
│    │    └─Dropout: 3-29                [16, 12, 1026, 64]        --
│    │    └─LayerNorm: 3-30              [16, 12, 1026, 64]        128
│    └─SelfAttentionLayer: 2-6           [16, 12, 1026, 64]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 1026, 64]        16,640
│    │    └─Dropout: 3-32                [16, 12, 1026, 64]        --
│    │    └─LayerNorm: 3-33              [16, 12, 1026, 64]        128
│    │    └─Sequential: 3-34             [16, 12, 1026, 64]        33,088
│    │    └─Dropout: 3-35                [16, 12, 1026, 64]        --
│    │    └─LayerNorm: 3-36              [16, 12, 1026, 64]        128
├─Linear: 1-5                            [16, 64, 1026, 12]        156
├─Linear: 1-6                            [16, 12, 1026, 1]         65
==========================================================================================
Total params: 300,381
Trainable params: 300,381
Non-trainable params: 0
Total mult-adds (M): 4.81
==========================================================================================
Input size (MB): 1.58
Forward/backward pass size (MB): 6960.91
Params size (MB): 1.20
Estimated Total Size (MB): 6963.69
==========================================================================================

Loss: MaskedMAELoss

2024-05-10 19:58:01.752099 Epoch 1  	Train Loss = 4.71796 Val Loss = 4.24221
2024-05-10 20:02:16.403457 Epoch 2  	Train Loss = 3.71653 Val Loss = 4.04186
2024-05-10 20:06:31.160556 Epoch 3  	Train Loss = 3.44843 Val Loss = 3.46244
2024-05-10 20:10:45.785547 Epoch 4  	Train Loss = 3.32042 Val Loss = 3.41156
2024-05-10 20:15:00.367681 Epoch 5  	Train Loss = 3.24082 Val Loss = 3.42443
2024-05-10 20:19:14.963363 Epoch 6  	Train Loss = 3.39570 Val Loss = 3.32292
2024-05-10 20:23:29.559047 Epoch 7  	Train Loss = 3.21657 Val Loss = 3.28198
2024-05-10 20:27:44.079536 Epoch 8  	Train Loss = 3.17824 Val Loss = 3.28190
2024-05-10 20:31:58.760421 Epoch 9  	Train Loss = 3.15699 Val Loss = 3.30895
2024-05-10 20:36:13.364222 Epoch 10  	Train Loss = 3.13611 Val Loss = 3.24728
2024-05-10 20:40:28.050880 Epoch 11  	Train Loss = 3.07920 Val Loss = 3.20565
2024-05-10 20:44:42.774857 Epoch 12  	Train Loss = 3.07328 Val Loss = 3.20374
2024-05-10 20:48:57.544589 Epoch 13  	Train Loss = 3.07089 Val Loss = 3.19681
2024-05-10 20:53:12.237858 Epoch 14  	Train Loss = 3.06714 Val Loss = 3.19748
2024-05-10 20:57:26.972645 Epoch 15  	Train Loss = 3.06420 Val Loss = 3.19568
2024-05-10 21:01:41.607467 Epoch 16  	Train Loss = 3.06062 Val Loss = 3.19854
2024-05-10 21:05:56.212538 Epoch 17  	Train Loss = 3.05803 Val Loss = 3.19353
2024-05-10 21:10:10.901894 Epoch 18  	Train Loss = 3.05681 Val Loss = 3.19590
2024-05-10 21:14:25.659009 Epoch 19  	Train Loss = 3.05281 Val Loss = 3.18110
2024-05-10 21:18:40.246000 Epoch 20  	Train Loss = 3.05110 Val Loss = 3.17794
2024-05-10 21:22:54.868422 Epoch 21  	Train Loss = 3.04794 Val Loss = 3.17983
2024-05-10 21:27:09.498639 Epoch 22  	Train Loss = 3.04533 Val Loss = 3.17510
2024-05-10 21:31:24.202299 Epoch 23  	Train Loss = 3.04062 Val Loss = 3.17769
2024-05-10 21:35:38.880761 Epoch 24  	Train Loss = 3.04226 Val Loss = 3.17575
2024-05-10 21:39:53.561599 Epoch 25  	Train Loss = 3.03899 Val Loss = 3.17756
2024-05-10 21:44:08.348691 Epoch 26  	Train Loss = 3.03597 Val Loss = 3.17117
2024-05-10 21:48:23.194821 Epoch 27  	Train Loss = 3.03374 Val Loss = 3.18123
2024-05-10 21:52:37.925268 Epoch 28  	Train Loss = 3.03481 Val Loss = 3.16531
2024-05-10 21:56:52.704179 Epoch 29  	Train Loss = 3.03169 Val Loss = 3.16626
2024-05-10 22:01:07.455472 Epoch 30  	Train Loss = 3.03020 Val Loss = 3.16446
2024-05-10 22:05:22.214371 Epoch 31  	Train Loss = 3.02866 Val Loss = 3.17165
2024-05-10 22:09:37.015312 Epoch 32  	Train Loss = 3.02608 Val Loss = 3.16315
2024-05-10 22:13:51.775322 Epoch 33  	Train Loss = 3.02697 Val Loss = 3.16043
2024-05-10 22:18:06.483667 Epoch 34  	Train Loss = 3.02587 Val Loss = 3.16260
2024-05-10 22:22:21.122155 Epoch 35  	Train Loss = 3.02261 Val Loss = 3.15955
2024-05-10 22:26:35.885787 Epoch 36  	Train Loss = 3.02166 Val Loss = 3.15966
2024-05-10 22:30:50.647100 Epoch 37  	Train Loss = 3.02192 Val Loss = 3.15845
2024-05-10 22:35:05.409345 Epoch 38  	Train Loss = 3.01835 Val Loss = 3.16413
2024-05-10 22:39:20.063718 Epoch 39  	Train Loss = 3.01771 Val Loss = 3.16344
2024-05-10 22:43:34.462851 Epoch 40  	Train Loss = 3.01721 Val Loss = 3.15793
2024-05-10 22:47:48.930068 Epoch 41  	Train Loss = 3.00733 Val Loss = 3.15135
2024-05-10 22:52:03.680306 Epoch 42  	Train Loss = 3.00765 Val Loss = 3.15135
2024-05-10 22:56:18.541387 Epoch 43  	Train Loss = 3.00804 Val Loss = 3.15115
2024-05-10 23:00:32.961644 Epoch 44  	Train Loss = 3.00794 Val Loss = 3.15003
2024-05-10 23:04:47.372564 Epoch 45  	Train Loss = 3.00494 Val Loss = 3.15188
2024-05-10 23:09:01.754442 Epoch 46  	Train Loss = 3.00599 Val Loss = 3.15030
2024-05-10 23:13:16.336536 Epoch 47  	Train Loss = 3.00524 Val Loss = 3.15221
2024-05-10 23:17:30.878644 Epoch 48  	Train Loss = 3.00434 Val Loss = 3.15018
2024-05-10 23:21:45.357024 Epoch 49  	Train Loss = 3.00699 Val Loss = 3.15070
2024-05-10 23:25:59.822668 Epoch 50  	Train Loss = 3.00559 Val Loss = 3.15118
2024-05-10 23:30:14.314899 Epoch 51  	Train Loss = 3.00545 Val Loss = 3.15071
2024-05-10 23:34:28.749843 Epoch 52  	Train Loss = 3.00443 Val Loss = 3.15094
2024-05-10 23:38:43.186352 Epoch 53  	Train Loss = 3.00331 Val Loss = 3.15016
2024-05-10 23:42:57.997110 Epoch 54  	Train Loss = 3.00429 Val Loss = 3.14996
2024-05-10 23:47:12.863079 Epoch 55  	Train Loss = 3.00316 Val Loss = 3.14989
2024-05-10 23:51:27.349923 Epoch 56  	Train Loss = 3.00396 Val Loss = 3.14924
2024-05-10 23:55:41.837692 Epoch 57  	Train Loss = 3.00556 Val Loss = 3.15045
2024-05-10 23:59:56.472320 Epoch 58  	Train Loss = 3.00361 Val Loss = 3.15038
2024-05-11 00:04:10.916456 Epoch 59  	Train Loss = 3.00216 Val Loss = 3.15013
2024-05-11 00:08:25.416668 Epoch 60  	Train Loss = 3.00332 Val Loss = 3.15007
2024-05-11 00:12:39.818746 Epoch 61  	Train Loss = 3.00320 Val Loss = 3.15035
2024-05-11 00:16:54.181959 Epoch 62  	Train Loss = 3.00410 Val Loss = 3.14950
2024-05-11 00:21:08.595774 Epoch 63  	Train Loss = 3.00434 Val Loss = 3.14943
2024-05-11 00:25:23.107978 Epoch 64  	Train Loss = 3.00325 Val Loss = 3.14997
2024-05-11 00:29:37.877910 Epoch 65  	Train Loss = 3.00410 Val Loss = 3.14947
2024-05-11 00:33:52.305033 Epoch 66  	Train Loss = 3.00283 Val Loss = 3.15008
Early stopping at epoch: 66
Best at epoch 56:
Train Loss = 3.00396
Train MAE = 3.00364, RMSE = 6.12656, MAPE = 7.54370
Val Loss = 3.14924
Val MAE = 3.15740, RMSE = 6.38711, MAPE = 8.26072
Model checkpoint saved to: ../saved_models/Transformer/Transformer-PEMSD7L-2024-05-10-19-53-44.pt
--------- Test ---------
All Steps (1-12) MAE = 3.13975, RMSE = 6.33805, MAPE = 8.00411
Step 1 MAE = 1.40423, RMSE = 2.48537, MAPE = 3.08650
Step 2 MAE = 1.96305, RMSE = 3.64578, MAPE = 4.46252
Step 3 MAE = 2.36952, RMSE = 4.54595, MAPE = 5.55159
Step 4 MAE = 2.69336, RMSE = 5.25109, MAPE = 6.48104
Step 5 MAE = 2.96435, RMSE = 5.82622, MAPE = 7.30482
Step 6 MAE = 3.20298, RMSE = 6.30004, MAPE = 8.05936
Step 7 MAE = 3.41532, RMSE = 6.71934, MAPE = 8.74731
Step 8 MAE = 3.61183, RMSE = 7.07795, MAPE = 9.39925
Step 9 MAE = 3.78206, RMSE = 7.40506, MAPE = 9.97692
Step 10 MAE = 3.94204, RMSE = 7.69703, MAPE = 10.50698
Step 11 MAE = 4.09104, RMSE = 7.97306, MAPE = 11.00373
Step 12 MAE = 4.23728, RMSE = 8.23479, MAPE = 11.46909
Inference time: 26.29 s
