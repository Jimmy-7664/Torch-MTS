ETTH2
Trainset:	x-(8381, 336, 7, 1)	y-(8381, 96, 7, 1)
Valset:  	x-(2794, 336, 7, 1)  	y-(2794, 96, 7, 1)
Testset:	x-(2794, 336, 7, 1)	y-(2794, 96, 7, 1)
INFO: Using scaled X and Y, only for LTSF!

Random seed = 233
--------- PatchTST ---------
{
    "num_nodes": 7,
    "in_steps": 336,
    "out_steps": 96,
    "lr": 0.0001,
    "weight_decay": 0,
    "milestones": [
        10
    ],
    "lr_decay_rate": 0.1,
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "dataloader": "ltsf",
    "runner": "ltsf",
    "loss": "mse",
    "model_args": {
        "enc_in": 7,
        "seq_len": 336,
        "pred_len": 96,
        "e_layers": 3,
        "n_heads": 4,
        "d_model": 16,
        "d_ff": 128,
        "dropout": 0.3,
        "fc_dropout": 0.3,
        "head_dropout": 0,
        "patch_len": 16,
        "stride": 8,
        "individual": 0,
        "padding_patch": "end",
        "revin": 1,
        "affine": 0,
        "subtract_last": 0,
        "decomposition": 0,
        "kernel_size": 25
    }
}
========================================================================================================================
Layer (type:depth-idx)                                                 Output Shape              Param #
========================================================================================================================
PatchTST                                                               [64, 96, 7, 1]            --
├─PatchTST_backbone: 1-1                                               [64, 7, 96]               --
│    └─RevIN: 2-1                                                      [64, 336, 7]              --
│    └─ReplicationPad1d: 2-2                                           [64, 7, 344]              --
│    └─TSTiEncoder: 2-3                                                [64, 7, 16, 42]           672
│    │    └─Linear: 3-1                                                [64, 7, 42, 16]           272
│    │    └─Dropout: 3-2                                               [448, 42, 16]             --
│    │    └─TSTEncoder: 3-3                                            [448, 42, 16]             16,179
│    └─Flatten_Head: 2-4                                               [64, 7, 96]               --
│    │    └─Flatten: 3-4                                               [64, 7, 672]              --
│    │    └─Linear: 3-5                                                [64, 7, 96]               64,608
│    │    └─Dropout: 3-6                                               [64, 7, 96]               --
│    └─RevIN: 2-5                                                      [64, 96, 7]               --
========================================================================================================================
Total params: 81,731
Trainable params: 81,728
Non-trainable params: 3
Total mult-adds (M): 11.40
========================================================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 111.13
Params size (MB): 0.32
Estimated Total Size (MB): 112.06
========================================================================================================================

Loss: MSELoss

2024-04-04 16:36:09.960562 Epoch 1  	Train Loss = 0.54483 Val Loss = 0.23403
2024-04-04 16:36:12.722183 Epoch 2  	Train Loss = 0.45343 Val Loss = 0.21728
2024-04-04 16:36:15.540056 Epoch 3  	Train Loss = 0.42862 Val Loss = 0.21256
2024-04-04 16:36:18.372083 Epoch 4  	Train Loss = 0.41839 Val Loss = 0.21019
2024-04-04 16:36:21.175287 Epoch 5  	Train Loss = 0.41347 Val Loss = 0.21053
2024-04-04 16:36:23.914981 Epoch 6  	Train Loss = 0.40907 Val Loss = 0.21030
2024-04-04 16:36:26.603323 Epoch 7  	Train Loss = 0.40525 Val Loss = 0.21246
2024-04-04 16:36:29.479434 Epoch 8  	Train Loss = 0.40105 Val Loss = 0.20975
2024-04-04 16:36:32.377569 Epoch 9  	Train Loss = 0.39764 Val Loss = 0.20899
2024-04-04 16:36:35.178664 Epoch 10  	Train Loss = 0.39367 Val Loss = 0.20790
2024-04-04 16:36:37.982072 Epoch 11  	Train Loss = 0.39041 Val Loss = 0.20802
2024-04-04 16:36:40.771435 Epoch 12  	Train Loss = 0.39073 Val Loss = 0.20798
2024-04-04 16:36:43.550696 Epoch 13  	Train Loss = 0.38982 Val Loss = 0.20789
2024-04-04 16:36:46.532027 Epoch 14  	Train Loss = 0.38915 Val Loss = 0.20809
2024-04-04 16:36:49.414964 Epoch 15  	Train Loss = 0.38882 Val Loss = 0.20804
2024-04-04 16:36:52.350252 Epoch 16  	Train Loss = 0.38861 Val Loss = 0.20776
2024-04-04 16:36:55.214984 Epoch 17  	Train Loss = 0.38858 Val Loss = 0.20797
2024-04-04 16:36:58.189243 Epoch 18  	Train Loss = 0.38892 Val Loss = 0.20797
2024-04-04 16:37:00.914986 Epoch 19  	Train Loss = 0.38902 Val Loss = 0.20759
2024-04-04 16:37:03.713484 Epoch 20  	Train Loss = 0.38708 Val Loss = 0.20773
2024-04-04 16:37:06.371878 Epoch 21  	Train Loss = 0.38760 Val Loss = 0.20770
2024-04-04 16:37:09.278572 Epoch 22  	Train Loss = 0.38678 Val Loss = 0.20768
2024-04-04 16:37:11.992478 Epoch 23  	Train Loss = 0.38696 Val Loss = 0.20757
2024-04-04 16:37:14.715767 Epoch 24  	Train Loss = 0.38646 Val Loss = 0.20738
2024-04-04 16:37:17.601350 Epoch 25  	Train Loss = 0.38684 Val Loss = 0.20761
2024-04-04 16:37:20.428785 Epoch 26  	Train Loss = 0.38618 Val Loss = 0.20771
2024-04-04 16:37:23.089918 Epoch 27  	Train Loss = 0.38558 Val Loss = 0.20768
2024-04-04 16:37:25.761743 Epoch 28  	Train Loss = 0.38533 Val Loss = 0.20760
2024-04-04 16:37:28.626316 Epoch 29  	Train Loss = 0.38509 Val Loss = 0.20738
2024-04-04 16:37:31.403471 Epoch 30  	Train Loss = 0.38507 Val Loss = 0.20746
2024-04-04 16:37:34.212406 Epoch 31  	Train Loss = 0.38482 Val Loss = 0.20748
2024-04-04 16:37:36.961282 Epoch 32  	Train Loss = 0.38444 Val Loss = 0.20779
2024-04-04 16:37:39.634217 Epoch 33  	Train Loss = 0.38409 Val Loss = 0.20750
2024-04-04 16:37:42.406103 Epoch 34  	Train Loss = 0.38404 Val Loss = 0.20747
2024-04-04 16:37:45.293145 Epoch 35  	Train Loss = 0.38310 Val Loss = 0.20740
2024-04-04 16:37:48.014725 Epoch 36  	Train Loss = 0.38341 Val Loss = 0.20744
2024-04-04 16:37:50.771824 Epoch 37  	Train Loss = 0.38208 Val Loss = 0.20728
2024-04-04 16:37:53.619774 Epoch 38  	Train Loss = 0.38197 Val Loss = 0.20727
2024-04-04 16:37:56.252424 Epoch 39  	Train Loss = 0.38162 Val Loss = 0.20721
2024-04-04 16:37:58.917438 Epoch 40  	Train Loss = 0.38059 Val Loss = 0.20722
2024-04-04 16:38:01.538816 Epoch 41  	Train Loss = 0.38107 Val Loss = 0.20728
2024-04-04 16:38:04.212229 Epoch 42  	Train Loss = 0.38104 Val Loss = 0.20710
2024-04-04 16:38:06.961105 Epoch 43  	Train Loss = 0.38053 Val Loss = 0.20721
2024-04-04 16:38:09.633735 Epoch 44  	Train Loss = 0.38058 Val Loss = 0.20741
2024-04-04 16:38:12.379393 Epoch 45  	Train Loss = 0.38048 Val Loss = 0.20701
2024-04-04 16:38:15.294175 Epoch 46  	Train Loss = 0.38080 Val Loss = 0.20691
2024-04-04 16:38:18.021742 Epoch 47  	Train Loss = 0.38030 Val Loss = 0.20698
2024-04-04 16:38:20.783329 Epoch 48  	Train Loss = 0.37982 Val Loss = 0.20683
2024-04-04 16:38:23.732450 Epoch 49  	Train Loss = 0.38001 Val Loss = 0.20670
2024-04-04 16:38:26.493191 Epoch 50  	Train Loss = 0.37915 Val Loss = 0.20686
2024-04-04 16:38:29.241246 Epoch 51  	Train Loss = 0.37951 Val Loss = 0.20702
2024-04-04 16:38:32.016835 Epoch 52  	Train Loss = 0.37885 Val Loss = 0.20685
2024-04-04 16:38:34.696997 Epoch 53  	Train Loss = 0.37747 Val Loss = 0.20706
2024-04-04 16:38:37.591083 Epoch 54  	Train Loss = 0.37842 Val Loss = 0.20711
2024-04-04 16:38:40.453484 Epoch 55  	Train Loss = 0.37875 Val Loss = 0.20681
2024-04-04 16:38:43.397258 Epoch 56  	Train Loss = 0.37800 Val Loss = 0.20680
2024-04-04 16:38:46.164914 Epoch 57  	Train Loss = 0.37719 Val Loss = 0.20687
2024-04-04 16:38:48.915038 Epoch 58  	Train Loss = 0.37717 Val Loss = 0.20683
2024-04-04 16:38:51.775374 Epoch 59  	Train Loss = 0.37635 Val Loss = 0.20672
Early stopping at epoch: 59
Best at epoch 49:
Train Loss = 0.38001
Train MSE = 0.36564, MAE = 0.34228
Val Loss = 0.20670
Val MSE = 0.20760, MAE = 0.31428
Model checkpoint saved to: ../saved_models/PatchTST/PatchTST-ETTH2-2024-04-04-16-36-06.pt
--------- Test ---------
All Steps (1-96) MSE = 0.27906, MAE = 0.33822
Inference time: 0.18 s
