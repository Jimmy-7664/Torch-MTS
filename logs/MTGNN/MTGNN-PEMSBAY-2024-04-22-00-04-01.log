PEMSBAY
Trainset:	x-(36465, 12, 325, 2)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 2)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 2)	y-(10419, 12, 325, 1)

Random seed = 233
--------- MTGNN ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        50
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 325,
        "in_dim": 2,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 325, 1]          3,168
├─graph_constructor: 1-1                 [325, 325]                --
│    └─Embedding: 2-1                    [325, 40]                 13,000
│    └─Embedding: 2-2                    [325, 40]                 13,000
│    └─Linear: 2-3                       [325, 40]                 1,640
│    └─Linear: 2-4                       [325, 40]                 1,640
├─Conv2d: 1-2                            [64, 32, 325, 19]         96
├─Conv2d: 1-3                            [64, 64, 325, 1]          2,496
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-5            [64, 32, 325, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-6            [64, 32, 325, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 64, 325, 1]          26,688
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-8                      [64, 32, 325, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 325, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 325, 13]         --
│    │    └─linear: 3-5                  [64, 32, 325, 13]         3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-9                      [64, 32, 325, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 325, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 325, 13]         --
│    │    └─linear: 3-8                  [64, 32, 325, 13]         3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-10                   [64, 32, 325, 13]         270,400
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-11           [64, 32, 325, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-12           [64, 32, 325, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 64, 325, 1]          14,400
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-14                     [64, 32, 325, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 325, 7]          --
│    │    └─linear: 3-13                 [64, 32, 325, 7]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-15                     [64, 32, 325, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 325, 7]          --
│    │    └─linear: 3-16                 [64, 32, 325, 7]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-16                   [64, 32, 325, 7]          145,600
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-17           [64, 32, 325, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-18           [64, 32, 325, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-19                      [64, 64, 325, 1]          2,112
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-20                     [64, 32, 325, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 325, 1]          --
│    │    └─linear: 3-21                 [64, 32, 325, 1]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-21                     [64, 32, 325, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 325, 1]          --
│    │    └─linear: 3-24                 [64, 32, 325, 1]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-22                   [64, 32, 325, 1]          20,800
├─Conv2d: 1-22                           [64, 64, 325, 1]          2,112
├─Conv2d: 1-23                           [64, 128, 325, 1]         8,320
├─Conv2d: 1-24                           [64, 12, 325, 1]          1,548
==========================================================================================
Total params: 573,484
Trainable params: 573,484
Non-trainable params: 0
Total mult-adds (G): 8.94
==========================================================================================
Input size (MB): 2.00
Forward/backward pass size (MB): 817.11
Params size (MB): 2.28
Estimated Total Size (MB): 821.39
==========================================================================================

Loss: MaskedMAELoss

CL target length = 1
2024-04-22 00:04:41.610580 Epoch 1  	Train Loss = 0.95999 Val Loss = 5.72033
2024-04-22 00:05:16.912540 Epoch 2  	Train Loss = 0.87890 Val Loss = 5.72025
2024-04-22 00:05:52.507090 Epoch 3  	Train Loss = 0.86693 Val Loss = 5.71866
2024-04-22 00:06:27.574263 Epoch 4  	Train Loss = 0.86105 Val Loss = 5.71848
CL target length = 2
2024-04-22 00:07:02.769403 Epoch 5  	Train Loss = 0.96901 Val Loss = 5.30646
2024-04-22 00:07:37.511399 Epoch 6  	Train Loss = 0.99151 Val Loss = 5.30385
2024-04-22 00:08:12.160731 Epoch 7  	Train Loss = 0.98254 Val Loss = 5.30299
2024-04-22 00:08:47.139728 Epoch 8  	Train Loss = 0.97861 Val Loss = 5.30291
CL target length = 3
2024-04-22 00:09:22.050347 Epoch 9  	Train Loss = 1.01969 Val Loss = 4.90640
2024-04-22 00:09:57.192713 Epoch 10  	Train Loss = 1.08391 Val Loss = 4.90529
2024-04-22 00:10:32.584532 Epoch 11  	Train Loss = 1.07802 Val Loss = 4.90324
2024-04-22 00:11:07.657698 Epoch 12  	Train Loss = 1.07401 Val Loss = 4.90429
2024-04-22 00:11:42.757729 Epoch 13  	Train Loss = 1.07150 Val Loss = 4.90093
CL target length = 4
2024-04-22 00:12:18.536171 Epoch 14  	Train Loss = 1.15794 Val Loss = 4.51793
2024-04-22 00:12:53.254520 Epoch 15  	Train Loss = 1.15409 Val Loss = 4.51951
2024-04-22 00:13:27.722593 Epoch 16  	Train Loss = 1.15032 Val Loss = 4.51378
2024-04-22 00:14:02.205600 Epoch 17  	Train Loss = 1.14799 Val Loss = 4.51349
CL target length = 5
2024-04-22 00:14:37.502840 Epoch 18  	Train Loss = 1.19220 Val Loss = 4.13551
2024-04-22 00:15:11.955917 Epoch 19  	Train Loss = 1.21674 Val Loss = 4.13632
2024-04-22 00:15:47.266548 Epoch 20  	Train Loss = 1.21258 Val Loss = 4.13707
2024-04-22 00:16:22.454428 Epoch 21  	Train Loss = 1.21186 Val Loss = 4.13721
CL target length = 6
2024-04-22 00:16:57.425087 Epoch 22  	Train Loss = 1.22148 Val Loss = 3.77075
2024-04-22 00:17:31.635100 Epoch 23  	Train Loss = 1.27136 Val Loss = 3.77378
2024-04-22 00:18:06.823194 Epoch 24  	Train Loss = 1.26601 Val Loss = 3.76032
2024-04-22 00:18:43.083700 Epoch 25  	Train Loss = 1.26381 Val Loss = 3.76043
2024-04-22 00:19:17.660941 Epoch 26  	Train Loss = 1.26019 Val Loss = 3.76116
CL target length = 7
2024-04-22 00:19:52.759874 Epoch 27  	Train Loss = 1.30735 Val Loss = 3.40176
2024-04-22 00:20:27.853437 Epoch 28  	Train Loss = 1.31363 Val Loss = 3.39846
2024-04-22 00:21:02.683154 Epoch 29  	Train Loss = 1.30897 Val Loss = 3.40329
2024-04-22 00:21:38.056130 Epoch 30  	Train Loss = 1.30721 Val Loss = 3.39478
CL target length = 8
2024-04-22 00:22:13.809399 Epoch 31  	Train Loss = 1.32560 Val Loss = 3.04005
2024-04-22 00:22:49.062889 Epoch 32  	Train Loss = 1.35092 Val Loss = 3.03234
2024-04-22 00:23:23.980985 Epoch 33  	Train Loss = 1.34693 Val Loss = 3.03337
2024-04-22 00:23:58.392872 Epoch 34  	Train Loss = 1.34524 Val Loss = 3.03961
2024-04-22 00:24:33.189362 Epoch 35  	Train Loss = 1.34451 Val Loss = 3.04290
CL target length = 9
2024-04-22 00:25:08.556073 Epoch 36  	Train Loss = 1.38838 Val Loss = 2.67542
2024-04-22 00:25:44.548639 Epoch 37  	Train Loss = 1.38145 Val Loss = 2.68703
2024-04-22 00:26:20.467897 Epoch 38  	Train Loss = 1.37788 Val Loss = 2.68024
2024-04-22 00:26:56.139040 Epoch 39  	Train Loss = 1.37594 Val Loss = 2.67263
CL target length = 10
2024-04-22 00:27:31.864094 Epoch 40  	Train Loss = 1.40322 Val Loss = 2.31371
2024-04-22 00:28:06.700346 Epoch 41  	Train Loss = 1.41120 Val Loss = 2.32058
2024-04-22 00:28:42.303059 Epoch 42  	Train Loss = 1.41018 Val Loss = 2.31740
2024-04-22 00:29:18.128938 Epoch 43  	Train Loss = 1.40590 Val Loss = 2.31847
CL target length = 11
2024-04-22 00:29:52.712158 Epoch 44  	Train Loss = 1.41618 Val Loss = 1.98215
2024-04-22 00:30:27.460516 Epoch 45  	Train Loss = 1.43795 Val Loss = 1.98155
2024-04-22 00:31:02.816766 Epoch 46  	Train Loss = 1.43503 Val Loss = 1.98805
2024-04-22 00:31:37.677355 Epoch 47  	Train Loss = 1.42894 Val Loss = 1.97305
2024-04-22 00:32:12.314574 Epoch 48  	Train Loss = 1.42883 Val Loss = 1.96601
CL target length = 12
2024-04-22 00:32:47.457606 Epoch 49  	Train Loss = 1.45974 Val Loss = 1.60841
2024-04-22 00:33:23.027850 Epoch 50  	Train Loss = 1.45636 Val Loss = 1.61157
2024-04-22 00:33:58.559068 Epoch 51  	Train Loss = 1.42854 Val Loss = 1.59838
2024-04-22 00:34:33.057098 Epoch 52  	Train Loss = 1.42437 Val Loss = 1.59560
2024-04-22 00:35:07.345336 Epoch 53  	Train Loss = 1.42288 Val Loss = 1.59354
2024-04-22 00:35:42.255656 Epoch 54  	Train Loss = 1.42238 Val Loss = 1.59745
2024-04-22 00:36:17.855858 Epoch 55  	Train Loss = 1.42153 Val Loss = 1.60139
2024-04-22 00:36:53.536350 Epoch 56  	Train Loss = 1.42038 Val Loss = 1.59451
2024-04-22 00:37:28.947411 Epoch 57  	Train Loss = 1.41976 Val Loss = 1.59960
2024-04-22 00:38:04.107943 Epoch 58  	Train Loss = 1.41936 Val Loss = 1.60155
2024-04-22 00:38:38.696010 Epoch 59  	Train Loss = 1.41859 Val Loss = 1.59594
2024-04-22 00:39:13.722085 Epoch 60  	Train Loss = 1.41792 Val Loss = 1.59390
2024-04-22 00:39:49.276443 Epoch 61  	Train Loss = 1.41732 Val Loss = 1.59314
2024-04-22 00:40:24.916059 Epoch 62  	Train Loss = 1.41652 Val Loss = 1.59749
2024-04-22 00:41:01.054159 Epoch 63  	Train Loss = 1.41600 Val Loss = 1.59296
2024-04-22 00:41:35.600124 Epoch 64  	Train Loss = 1.41583 Val Loss = 1.59674
2024-04-22 00:42:09.986552 Epoch 65  	Train Loss = 1.41584 Val Loss = 1.59469
2024-04-22 00:42:44.440819 Epoch 66  	Train Loss = 1.41448 Val Loss = 1.59315
2024-04-22 00:43:19.573535 Epoch 67  	Train Loss = 1.41414 Val Loss = 1.59215
2024-04-22 00:43:54.638048 Epoch 68  	Train Loss = 1.41372 Val Loss = 1.59204
2024-04-22 00:44:29.820818 Epoch 69  	Train Loss = 1.41303 Val Loss = 1.58650
2024-04-22 00:45:04.838228 Epoch 70  	Train Loss = 1.41294 Val Loss = 1.59671
2024-04-22 00:45:40.106190 Epoch 71  	Train Loss = 1.41209 Val Loss = 1.59528
2024-04-22 00:46:14.758111 Epoch 72  	Train Loss = 1.41194 Val Loss = 1.58832
2024-04-22 00:46:49.659214 Epoch 73  	Train Loss = 1.41139 Val Loss = 1.59315
2024-04-22 00:47:24.406776 Epoch 74  	Train Loss = 1.41088 Val Loss = 1.59138
2024-04-22 00:47:59.515777 Epoch 75  	Train Loss = 1.41087 Val Loss = 1.59877
2024-04-22 00:48:33.759657 Epoch 76  	Train Loss = 1.41012 Val Loss = 1.59622
2024-04-22 00:49:08.711011 Epoch 77  	Train Loss = 1.40987 Val Loss = 1.59213
2024-04-22 00:49:44.241223 Epoch 78  	Train Loss = 1.40883 Val Loss = 1.58908
2024-04-22 00:50:19.675343 Epoch 79  	Train Loss = 1.40938 Val Loss = 1.59108
Early stopping at epoch: 79
Best at epoch 69:
Train Loss = 1.41303
Train MAE = 1.38937, RMSE = 3.06436, MAPE = 2.94619
Val Loss = 1.58650
Val MAE = 1.57668, RMSE = 3.62228, MAPE = 3.55873
Model checkpoint saved to: ../saved_models/MTGNN/MTGNN-PEMSBAY-2024-04-22-00-04-01.pt
--------- Test ---------
All Steps (1-12) MAE = 1.57920, RMSE = 3.63562, MAPE = 3.51549
Step 1 MAE = 0.86024, RMSE = 1.55539, MAPE = 1.65010
Step 2 MAE = 1.13256, RMSE = 2.25131, MAPE = 2.27812
Step 3 MAE = 1.32048, RMSE = 2.78893, MAPE = 2.75605
Step 4 MAE = 1.45711, RMSE = 3.19610, MAPE = 3.13200
Step 5 MAE = 1.56023, RMSE = 3.49953, MAPE = 3.43102
Step 6 MAE = 1.64209, RMSE = 3.73268, MAPE = 3.67209
Step 7 MAE = 1.70951, RMSE = 3.91840, MAPE = 3.87105
Step 8 MAE = 1.76518, RMSE = 4.06658, MAPE = 4.03359
Step 9 MAE = 1.81251, RMSE = 4.18772, MAPE = 4.16587
Step 10 MAE = 1.85567, RMSE = 4.28882, MAPE = 4.28716
Step 11 MAE = 1.89635, RMSE = 4.37841, MAPE = 4.39818
Step 12 MAE = 1.93845, RMSE = 4.46765, MAPE = 4.51063
Inference time: 2.80 s
