PEMS07
Trainset:	x-(16921, 12, 883, 2)	y-(16921, 12, 883, 1)
Valset:  	x-(5640, 12, 883, 2)  	y-(5640, 12, 883, 1)
Testset:	x-(5640, 12, 883, 2)	y-(5640, 12, 883, 1)

--------- MTGNN ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "train_size": 0.6,
    "val_size": 0.2,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        108
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 883,
        "in_dim": 2,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 883, 1]          3,168
├─graph_constructor: 1-1                 [883, 883]                --
│    └─Embedding: 2-1                    [883, 40]                 35,320
│    └─Embedding: 2-2                    [883, 40]                 35,320
│    └─Linear: 2-3                       [883, 40]                 1,640
│    └─Linear: 2-4                       [883, 40]                 1,640
├─Conv2d: 1-2                            [64, 32, 883, 19]         96
├─Conv2d: 1-3                            [64, 64, 883, 1]          2,496
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-5            [64, 32, 883, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-6            [64, 32, 883, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 64, 883, 1]          26,688
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-8                      [64, 32, 883, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 883, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 883, 13]         --
│    │    └─linear: 3-5                  [64, 32, 883, 13]         3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-9                      [64, 32, 883, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 883, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 883, 13]         --
│    │    └─linear: 3-8                  [64, 32, 883, 13]         3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-10                   [64, 32, 883, 13]         734,656
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-11           [64, 32, 883, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-12           [64, 32, 883, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 64, 883, 1]          14,400
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-14                     [64, 32, 883, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 883, 7]          --
│    │    └─linear: 3-13                 [64, 32, 883, 7]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-15                     [64, 32, 883, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 883, 7]          --
│    │    └─linear: 3-16                 [64, 32, 883, 7]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-16                   [64, 32, 883, 7]          395,584
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-17           [64, 32, 883, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-18           [64, 32, 883, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-19                      [64, 64, 883, 1]          2,112
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-20                     [64, 32, 883, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 883, 1]          --
│    │    └─linear: 3-21                 [64, 32, 883, 1]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-21                     [64, 32, 883, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 883, 1]          --
│    │    └─linear: 3-24                 [64, 32, 883, 1]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-22                   [64, 32, 883, 1]          56,512
├─Conv2d: 1-22                           [64, 64, 883, 1]          2,112
├─Conv2d: 1-23                           [64, 128, 883, 1]         8,320
├─Conv2d: 1-24                           [64, 12, 883, 1]          1,548
==========================================================================================
Total params: 1,368,076
Trainable params: 1,368,076
Non-trainable params: 0
Total mult-adds (G): 24.34
==========================================================================================
Input size (MB): 5.43
Forward/backward pass size (MB): 2220.02
Params size (MB): 5.46
Estimated Total Size (MB): 2230.90
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-09-02 11:19:26.635323 Epoch 1  	Train Loss = 23.19356 Val Loss = 144.30306
2023-09-02 11:21:07.477404 Epoch 2  	Train Loss = 19.25346 Val Loss = 144.27071
2023-09-02 11:22:49.056727 Epoch 3  	Train Loss = 18.91764 Val Loss = 144.23511
2023-09-02 11:24:30.774730 Epoch 4  	Train Loss = 18.69387 Val Loss = 144.20586
2023-09-02 11:26:12.490355 Epoch 5  	Train Loss = 18.80131 Val Loss = 144.22264
2023-09-02 11:27:54.035543 Epoch 6  	Train Loss = 18.37783 Val Loss = 144.23486
2023-09-02 11:29:35.559019 Epoch 7  	Train Loss = 18.35331 Val Loss = 144.18734
2023-09-02 11:31:17.100478 Epoch 8  	Train Loss = 18.13806 Val Loss = 144.19440
2023-09-02 11:32:58.740848 Epoch 9  	Train Loss = 18.09972 Val Loss = 144.18521
CL target length = 2
2023-09-02 11:34:40.319936 Epoch 10  	Train Loss = 20.77038 Val Loss = 132.98990
2023-09-02 11:36:22.152169 Epoch 11  	Train Loss = 19.03348 Val Loss = 132.80329
2023-09-02 11:38:04.025495 Epoch 12  	Train Loss = 18.77795 Val Loss = 132.83265
2023-09-02 11:39:46.160566 Epoch 13  	Train Loss = 18.66932 Val Loss = 132.78900
2023-09-02 11:41:28.432724 Epoch 14  	Train Loss = 18.60409 Val Loss = 132.81865
2023-09-02 11:43:10.063818 Epoch 15  	Train Loss = 18.55844 Val Loss = 132.78626
2023-09-02 11:44:51.683752 Epoch 16  	Train Loss = 18.37762 Val Loss = 132.76068
2023-09-02 11:46:33.479575 Epoch 17  	Train Loss = 18.25353 Val Loss = 132.76964
2023-09-02 11:48:15.079542 Epoch 18  	Train Loss = 18.21063 Val Loss = 132.73520
CL target length = 3
2023-09-02 11:49:56.754119 Epoch 19  	Train Loss = 19.52017 Val Loss = 121.65915
2023-09-02 11:51:39.129440 Epoch 20  	Train Loss = 18.78492 Val Loss = 121.47678
2023-09-02 11:53:21.292198 Epoch 21  	Train Loss = 18.65623 Val Loss = 121.42253
2023-09-02 11:55:03.707723 Epoch 22  	Train Loss = 18.56478 Val Loss = 121.43039
2023-09-02 11:56:45.814666 Epoch 23  	Train Loss = 18.43395 Val Loss = 121.27532
2023-09-02 11:58:27.717597 Epoch 24  	Train Loss = 18.37955 Val Loss = 121.32664
2023-09-02 12:00:09.627112 Epoch 25  	Train Loss = 18.53186 Val Loss = 121.28958
2023-09-02 12:01:51.562487 Epoch 26  	Train Loss = 18.40475 Val Loss = 121.40107
2023-09-02 12:03:33.512580 Epoch 27  	Train Loss = 18.35960 Val Loss = 121.24626
2023-09-02 12:05:15.588687 Epoch 28  	Train Loss = 18.22532 Val Loss = 121.23096
CL target length = 4
2023-09-02 12:06:57.682643 Epoch 29  	Train Loss = 19.60179 Val Loss = 110.09695
2023-09-02 12:08:39.705696 Epoch 30  	Train Loss = 18.68143 Val Loss = 109.98917
2023-09-02 12:10:21.683436 Epoch 31  	Train Loss = 18.63110 Val Loss = 109.90905
2023-09-02 12:12:03.853757 Epoch 32  	Train Loss = 18.61146 Val Loss = 110.05376
2023-09-02 12:13:46.070912 Epoch 33  	Train Loss = 18.53740 Val Loss = 110.01575
2023-09-02 12:15:28.576399 Epoch 34  	Train Loss = 18.51104 Val Loss = 109.95725
2023-09-02 12:17:11.693073 Epoch 35  	Train Loss = 18.37498 Val Loss = 109.93570
2023-09-02 12:18:54.494393 Epoch 36  	Train Loss = 18.37928 Val Loss = 110.00060
2023-09-02 12:20:36.821729 Epoch 37  	Train Loss = 18.40988 Val Loss = 109.94458
CL target length = 5
2023-09-02 12:22:18.856677 Epoch 38  	Train Loss = 19.26747 Val Loss = 98.62699
2023-09-02 12:24:00.783087 Epoch 39  	Train Loss = 18.71501 Val Loss = 98.67308
2023-09-02 12:25:42.622930 Epoch 40  	Train Loss = 18.68303 Val Loss = 98.89678
2023-09-02 12:27:24.527046 Epoch 41  	Train Loss = 18.69906 Val Loss = 98.62629
2023-09-02 12:29:06.673317 Epoch 42  	Train Loss = 18.58872 Val Loss = 98.53132
2023-09-02 12:30:49.580057 Epoch 43  	Train Loss = 18.56139 Val Loss = 98.57421
2023-09-02 12:32:32.805111 Epoch 44  	Train Loss = 18.51169 Val Loss = 98.52338
2023-09-02 12:34:15.580197 Epoch 45  	Train Loss = 18.46469 Val Loss = 98.49166
2023-09-02 12:35:58.129777 Epoch 46  	Train Loss = 18.42937 Val Loss = 98.47125
2023-09-02 12:37:40.424186 Epoch 47  	Train Loss = 18.41298 Val Loss = 98.58140
CL target length = 6
2023-09-02 12:39:22.578062 Epoch 48  	Train Loss = 19.46190 Val Loss = 87.43165
2023-09-02 12:41:04.664574 Epoch 49  	Train Loss = 18.79668 Val Loss = 87.46127
2023-09-02 12:42:46.991708 Epoch 50  	Train Loss = 18.77838 Val Loss = 87.52373
2023-09-02 12:44:29.772595 Epoch 51  	Train Loss = 18.70098 Val Loss = 87.26697
2023-09-02 12:46:13.001552 Epoch 52  	Train Loss = 18.65842 Val Loss = 87.18549
2023-09-02 12:47:55.928610 Epoch 53  	Train Loss = 18.70999 Val Loss = 87.19654
2023-09-02 12:49:38.387439 Epoch 54  	Train Loss = 18.70199 Val Loss = 87.29805
2023-09-02 12:51:20.734726 Epoch 55  	Train Loss = 18.64573 Val Loss = 87.25808
2023-09-02 12:53:02.806951 Epoch 56  	Train Loss = 18.60892 Val Loss = 87.18470
CL target length = 7
2023-09-02 12:54:44.684457 Epoch 57  	Train Loss = 19.41787 Val Loss = 76.12853
2023-09-02 12:56:26.709594 Epoch 58  	Train Loss = 18.89592 Val Loss = 76.12610
2023-09-02 12:58:09.215395 Epoch 59  	Train Loss = 18.87438 Val Loss = 76.00072
2023-09-02 12:59:52.142179 Epoch 60  	Train Loss = 18.88891 Val Loss = 76.06501
2023-09-02 13:01:35.473246 Epoch 61  	Train Loss = 18.85567 Val Loss = 75.93342
2023-09-02 13:03:17.965846 Epoch 62  	Train Loss = 18.77379 Val Loss = 75.98226
2023-09-02 13:05:00.084835 Epoch 63  	Train Loss = 18.74555 Val Loss = 76.00904
2023-09-02 13:06:41.681552 Epoch 64  	Train Loss = 18.77192 Val Loss = 75.90120
2023-09-02 13:08:23.063803 Epoch 65  	Train Loss = 18.73532 Val Loss = 75.96386
2023-09-02 13:10:04.652203 Epoch 66  	Train Loss = 18.70426 Val Loss = 75.95900
CL target length = 8
2023-09-02 13:11:46.299508 Epoch 67  	Train Loss = 19.60470 Val Loss = 64.78153
2023-09-02 13:13:28.194445 Epoch 68  	Train Loss = 19.00633 Val Loss = 64.93269
2023-09-02 13:15:10.788773 Epoch 69  	Train Loss = 18.94246 Val Loss = 64.72999
2023-09-02 13:16:52.668443 Epoch 70  	Train Loss = 18.92969 Val Loss = 64.81197
2023-09-02 13:18:34.419923 Epoch 71  	Train Loss = 18.90499 Val Loss = 64.64423
2023-09-02 13:20:16.205449 Epoch 72  	Train Loss = 18.88506 Val Loss = 64.86116
2023-09-02 13:21:57.819248 Epoch 73  	Train Loss = 18.90308 Val Loss = 64.84749
2023-09-02 13:23:39.138765 Epoch 74  	Train Loss = 18.88884 Val Loss = 64.62770
2023-09-02 13:25:20.636318 Epoch 75  	Train Loss = 18.83888 Val Loss = 64.58987
CL target length = 9
2023-09-02 13:27:02.432265 Epoch 76  	Train Loss = 19.56221 Val Loss = 53.61937
2023-09-02 13:28:44.426170 Epoch 77  	Train Loss = 19.08829 Val Loss = 53.65248
2023-09-02 13:30:26.405225 Epoch 78  	Train Loss = 19.05351 Val Loss = 53.53734
2023-09-02 13:32:08.101459 Epoch 79  	Train Loss = 19.05923 Val Loss = 53.77252
2023-09-02 13:33:49.746715 Epoch 80  	Train Loss = 19.02724 Val Loss = 53.55904
2023-09-02 13:35:31.111518 Epoch 81  	Train Loss = 18.98215 Val Loss = 53.41686
2023-09-02 13:37:12.495219 Epoch 82  	Train Loss = 19.00859 Val Loss = 53.78986
2023-09-02 13:38:53.826328 Epoch 83  	Train Loss = 18.96158 Val Loss = 53.38309
2023-09-02 13:40:35.355643 Epoch 84  	Train Loss = 18.93558 Val Loss = 53.45940
CL target length = 10
2023-09-02 13:42:17.157861 Epoch 85  	Train Loss = 19.41192 Val Loss = 42.70724
2023-09-02 13:43:59.093674 Epoch 86  	Train Loss = 19.22811 Val Loss = 42.59928
2023-09-02 13:45:40.849252 Epoch 87  	Train Loss = 19.14531 Val Loss = 42.28593
2023-09-02 13:47:22.457204 Epoch 88  	Train Loss = 19.13315 Val Loss = 42.22312
2023-09-02 13:49:03.947609 Epoch 89  	Train Loss = 19.04643 Val Loss = 42.33539
2023-09-02 13:50:45.341390 Epoch 90  	Train Loss = 19.07739 Val Loss = 42.58139
2023-09-02 13:52:26.591098 Epoch 91  	Train Loss = 19.03801 Val Loss = 42.51122
2023-09-02 13:54:07.952273 Epoch 92  	Train Loss = 19.02654 Val Loss = 42.43095
2023-09-02 13:55:49.591583 Epoch 93  	Train Loss = 19.01292 Val Loss = 42.47334
2023-09-02 13:57:31.384152 Epoch 94  	Train Loss = 18.97613 Val Loss = 42.44239
CL target length = 11
2023-09-02 13:59:13.185281 Epoch 95  	Train Loss = 19.67305 Val Loss = 31.42139
2023-09-02 14:00:54.822467 Epoch 96  	Train Loss = 19.21720 Val Loss = 31.10232
2023-09-02 14:02:36.409161 Epoch 97  	Train Loss = 19.18373 Val Loss = 31.13058
2023-09-02 14:04:17.819190 Epoch 98  	Train Loss = 19.13912 Val Loss = 31.03613
2023-09-02 14:05:59.091541 Epoch 99  	Train Loss = 19.13280 Val Loss = 31.52461
2023-09-02 14:07:40.328038 Epoch 100  	Train Loss = 19.11478 Val Loss = 31.17971
2023-09-02 14:09:21.818040 Epoch 101  	Train Loss = 19.13119 Val Loss = 31.27310
2023-09-02 14:11:03.608184 Epoch 102  	Train Loss = 19.10153 Val Loss = 31.17130
2023-09-02 14:12:45.518648 Epoch 103  	Train Loss = 19.11790 Val Loss = 31.03664
CL target length = 12
2023-09-02 14:14:27.265352 Epoch 104  	Train Loss = 19.56517 Val Loss = 19.94898
2023-09-02 14:16:08.926595 Epoch 105  	Train Loss = 19.29393 Val Loss = 20.59044
2023-09-02 14:17:50.407357 Epoch 106  	Train Loss = 19.25805 Val Loss = 20.16740
2023-09-02 14:19:31.769135 Epoch 107  	Train Loss = 19.21081 Val Loss = 20.14842
2023-09-02 14:21:13.006317 Epoch 108  	Train Loss = 19.22371 Val Loss = 19.99140
2023-09-02 14:22:54.315543 Epoch 109  	Train Loss = 18.83517 Val Loss = 19.72666
2023-09-02 14:24:35.825349 Epoch 110  	Train Loss = 18.78627 Val Loss = 19.76816
2023-09-02 14:26:17.319382 Epoch 111  	Train Loss = 18.76401 Val Loss = 19.85259
2023-09-02 14:27:58.785195 Epoch 112  	Train Loss = 18.74486 Val Loss = 19.78141
2023-09-02 14:29:40.122845 Epoch 113  	Train Loss = 18.72801 Val Loss = 19.80072
2023-09-02 14:31:21.307601 Epoch 114  	Train Loss = 18.71818 Val Loss = 19.76056
2023-09-02 14:33:02.451319 Epoch 115  	Train Loss = 18.70855 Val Loss = 19.75379
2023-09-02 14:34:43.721089 Epoch 116  	Train Loss = 18.70356 Val Loss = 19.72815
2023-09-02 14:36:24.834974 Epoch 117  	Train Loss = 18.69590 Val Loss = 19.72904
2023-09-02 14:38:06.282889 Epoch 118  	Train Loss = 18.68821 Val Loss = 19.81798
2023-09-02 14:39:47.856136 Epoch 119  	Train Loss = 18.67792 Val Loss = 19.69998
2023-09-02 14:41:29.525937 Epoch 120  	Train Loss = 18.67983 Val Loss = 19.72817
2023-09-02 14:43:11.189574 Epoch 121  	Train Loss = 18.66718 Val Loss = 19.77477
2023-09-02 14:44:52.726333 Epoch 122  	Train Loss = 18.66372 Val Loss = 19.76947
2023-09-02 14:46:34.191287 Epoch 123  	Train Loss = 18.65626 Val Loss = 19.75038
2023-09-02 14:48:15.509193 Epoch 124  	Train Loss = 18.64892 Val Loss = 19.77705
2023-09-02 14:49:57.053578 Epoch 125  	Train Loss = 18.64335 Val Loss = 19.74823
2023-09-02 14:51:38.503315 Epoch 126  	Train Loss = 18.63821 Val Loss = 19.70905
2023-09-02 14:53:20.113290 Epoch 127  	Train Loss = 18.63177 Val Loss = 19.75963
2023-09-02 14:55:01.799245 Epoch 128  	Train Loss = 18.62653 Val Loss = 19.77837
2023-09-02 14:56:43.532957 Epoch 129  	Train Loss = 18.62034 Val Loss = 19.85778
Early stopping at epoch: 129
Best at epoch 119:
Train Loss = 18.67792
Train RMSE = 31.50740, MAE = 18.98126, MAPE = 8.32482
Val Loss = 19.69998
Val RMSE = 33.35331, MAE = 20.35249, MAPE = 8.87427
--------- Test ---------
All Steps RMSE = 34.37315, MAE = 20.99240, MAPE = 8.78838
Step 1 RMSE = 27.40175, MAE = 17.21900, MAPE = 7.27755
Step 2 RMSE = 29.77513, MAE = 18.48572, MAPE = 7.84566
Step 3 RMSE = 31.30838, MAE = 19.36465, MAPE = 8.14773
Step 4 RMSE = 32.48381, MAE = 20.04382, MAPE = 8.37989
Step 5 RMSE = 33.48768, MAE = 20.61139, MAPE = 8.59195
Step 6 RMSE = 34.37273, MAE = 21.10976, MAPE = 8.78775
Step 7 RMSE = 35.20209, MAE = 21.57153, MAPE = 8.99184
Step 8 RMSE = 35.95805, MAE = 21.96685, MAPE = 9.16756
Step 9 RMSE = 36.60660, MAE = 22.28443, MAPE = 9.30619
Step 10 RMSE = 37.25309, MAE = 22.61763, MAPE = 9.43787
Step 11 RMSE = 37.94253, MAE = 23.02481, MAPE = 9.61866
Step 12 RMSE = 38.76395, MAE = 23.60612, MAPE = 9.90668
Inference time: 11.50 s
