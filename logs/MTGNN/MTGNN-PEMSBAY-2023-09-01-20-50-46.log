PEMSBAY
Trainset:	x-(36465, 12, 325, 2)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 2)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 2)	y-(10419, 12, 325, 1)

--------- MTGNN ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        50
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 325,
        "in_dim": 2,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 325, 1]          3,168
├─graph_constructor: 1-1                 [325, 325]                --
│    └─Embedding: 2-1                    [325, 40]                 13,000
│    └─Embedding: 2-2                    [325, 40]                 13,000
│    └─Linear: 2-3                       [325, 40]                 1,640
│    └─Linear: 2-4                       [325, 40]                 1,640
├─Conv2d: 1-2                            [64, 32, 325, 19]         96
├─Conv2d: 1-3                            [64, 64, 325, 1]          2,496
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-5            [64, 32, 325, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-6            [64, 32, 325, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 64, 325, 1]          26,688
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-8                      [64, 32, 325, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 325, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 325, 13]         --
│    │    └─linear: 3-5                  [64, 32, 325, 13]         3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-9                      [64, 32, 325, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 325, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 325, 13]         --
│    │    └─linear: 3-8                  [64, 32, 325, 13]         3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-10                   [64, 32, 325, 13]         270,400
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-11           [64, 32, 325, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-12           [64, 32, 325, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 64, 325, 1]          14,400
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-14                     [64, 32, 325, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 325, 7]          --
│    │    └─linear: 3-13                 [64, 32, 325, 7]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-15                     [64, 32, 325, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 325, 7]          --
│    │    └─linear: 3-16                 [64, 32, 325, 7]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-16                   [64, 32, 325, 7]          145,600
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-17           [64, 32, 325, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-18           [64, 32, 325, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-19                      [64, 64, 325, 1]          2,112
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-20                     [64, 32, 325, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 325, 1]          --
│    │    └─linear: 3-21                 [64, 32, 325, 1]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-21                     [64, 32, 325, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 325, 1]          --
│    │    └─linear: 3-24                 [64, 32, 325, 1]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-22                   [64, 32, 325, 1]          20,800
├─Conv2d: 1-22                           [64, 64, 325, 1]          2,112
├─Conv2d: 1-23                           [64, 128, 325, 1]         8,320
├─Conv2d: 1-24                           [64, 12, 325, 1]          1,548
==========================================================================================
Total params: 573,484
Trainable params: 573,484
Non-trainable params: 0
Total mult-adds (G): 8.94
==========================================================================================
Input size (MB): 2.00
Forward/backward pass size (MB): 817.11
Params size (MB): 2.28
Estimated Total Size (MB): 821.39
==========================================================================================

Loss: MaskedMAELoss

CL target length = 1
2023-09-01 20:52:03.808508 Epoch 1  	Train Loss = 0.96606 Val Loss = 5.72120
2023-09-01 20:53:14.744645 Epoch 2  	Train Loss = 0.87717 Val Loss = 5.72081
2023-09-01 20:54:26.359815 Epoch 3  	Train Loss = 0.86759 Val Loss = 5.71870
2023-09-01 20:55:38.511420 Epoch 4  	Train Loss = 0.85854 Val Loss = 5.71837
CL target length = 2
2023-09-01 20:56:50.873760 Epoch 5  	Train Loss = 0.96944 Val Loss = 5.30805
2023-09-01 20:58:03.244919 Epoch 6  	Train Loss = 0.99119 Val Loss = 5.30346
2023-09-01 20:59:15.717011 Epoch 7  	Train Loss = 0.98342 Val Loss = 5.30307
2023-09-01 21:00:27.982112 Epoch 8  	Train Loss = 0.97893 Val Loss = 5.30379
CL target length = 3
2023-09-01 21:01:40.379313 Epoch 9  	Train Loss = 1.02063 Val Loss = 4.90651
2023-09-01 21:02:52.760193 Epoch 10  	Train Loss = 1.08518 Val Loss = 4.90626
2023-09-01 21:04:05.078735 Epoch 11  	Train Loss = 1.07845 Val Loss = 4.90285
2023-09-01 21:05:17.319904 Epoch 12  	Train Loss = 1.07513 Val Loss = 4.90274
2023-09-01 21:06:29.898542 Epoch 13  	Train Loss = 1.07175 Val Loss = 4.90119
CL target length = 4
2023-09-01 21:07:43.251016 Epoch 14  	Train Loss = 1.15920 Val Loss = 4.51751
2023-09-01 21:08:56.929082 Epoch 15  	Train Loss = 1.15452 Val Loss = 4.51974
2023-09-01 21:10:10.783439 Epoch 16  	Train Loss = 1.15203 Val Loss = 4.51393
2023-09-01 21:11:24.055543 Epoch 17  	Train Loss = 1.14817 Val Loss = 4.51438
CL target length = 5
2023-09-01 21:12:36.965279 Epoch 18  	Train Loss = 1.19318 Val Loss = 4.13902
2023-09-01 21:13:49.959372 Epoch 19  	Train Loss = 1.21859 Val Loss = 4.13767
2023-09-01 21:15:02.797004 Epoch 20  	Train Loss = 1.21495 Val Loss = 4.13901
2023-09-01 21:16:15.429588 Epoch 21  	Train Loss = 1.21166 Val Loss = 4.13666
CL target length = 6
2023-09-01 21:17:27.974849 Epoch 22  	Train Loss = 1.22297 Val Loss = 3.77395
2023-09-01 21:18:40.367750 Epoch 23  	Train Loss = 1.27338 Val Loss = 3.77299
2023-09-01 21:19:52.836455 Epoch 24  	Train Loss = 1.26780 Val Loss = 3.76419
2023-09-01 21:21:05.469737 Epoch 25  	Train Loss = 1.26484 Val Loss = 3.76091
2023-09-01 21:22:18.817409 Epoch 26  	Train Loss = 1.26239 Val Loss = 3.76194
CL target length = 7
2023-09-01 21:23:32.775757 Epoch 27  	Train Loss = 1.31031 Val Loss = 3.40340
2023-09-01 21:24:46.431112 Epoch 28  	Train Loss = 1.31528 Val Loss = 3.40151
2023-09-01 21:25:59.540884 Epoch 29  	Train Loss = 1.31110 Val Loss = 3.39845
2023-09-01 21:27:12.295924 Epoch 30  	Train Loss = 1.30979 Val Loss = 3.40213
CL target length = 8
2023-09-01 21:28:25.158920 Epoch 31  	Train Loss = 1.32836 Val Loss = 3.04045
2023-09-01 21:29:37.985993 Epoch 32  	Train Loss = 1.35331 Val Loss = 3.03348
2023-09-01 21:30:50.839486 Epoch 33  	Train Loss = 1.35093 Val Loss = 3.03309
2023-09-01 21:32:03.476740 Epoch 34  	Train Loss = 1.35021 Val Loss = 3.03274
2023-09-01 21:33:15.971853 Epoch 35  	Train Loss = 1.34656 Val Loss = 3.03730
CL target length = 9
2023-09-01 21:34:28.846315 Epoch 36  	Train Loss = 1.39161 Val Loss = 2.67393
2023-09-01 21:35:41.476025 Epoch 37  	Train Loss = 1.38552 Val Loss = 2.68092
2023-09-01 21:36:54.671239 Epoch 38  	Train Loss = 1.38157 Val Loss = 2.67779
2023-09-01 21:38:08.380220 Epoch 39  	Train Loss = 1.38275 Val Loss = 2.66955
CL target length = 10
2023-09-01 21:39:21.825869 Epoch 40  	Train Loss = 1.40743 Val Loss = 2.31851
2023-09-01 21:40:34.639026 Epoch 41  	Train Loss = 1.41610 Val Loss = 2.32153
2023-09-01 21:41:47.584920 Epoch 42  	Train Loss = 1.41493 Val Loss = 2.32088
2023-09-01 21:43:00.418395 Epoch 43  	Train Loss = 1.41103 Val Loss = 2.32557
CL target length = 11
2023-09-01 21:44:13.354674 Epoch 44  	Train Loss = 1.42045 Val Loss = 1.99633
2023-09-01 21:45:25.859725 Epoch 45  	Train Loss = 1.44349 Val Loss = 1.96250
2023-09-01 21:46:38.128947 Epoch 46  	Train Loss = 1.44006 Val Loss = 1.98814
2023-09-01 21:47:50.191044 Epoch 47  	Train Loss = 1.43440 Val Loss = 1.98497
2023-09-01 21:49:02.485858 Epoch 48  	Train Loss = 1.43433 Val Loss = 1.96441
CL target length = 12
2023-09-01 21:50:15.075821 Epoch 49  	Train Loss = 1.46528 Val Loss = 1.61782
2023-09-01 21:51:28.415791 Epoch 50  	Train Loss = 1.46095 Val Loss = 1.62024
2023-09-01 21:52:41.997406 Epoch 51  	Train Loss = 1.43391 Val Loss = 1.59806
2023-09-01 21:53:55.266480 Epoch 52  	Train Loss = 1.43018 Val Loss = 1.59512
2023-09-01 21:55:08.108403 Epoch 53  	Train Loss = 1.42896 Val Loss = 1.59359
2023-09-01 21:56:20.725315 Epoch 54  	Train Loss = 1.42823 Val Loss = 1.59745
2023-09-01 21:57:33.315241 Epoch 55  	Train Loss = 1.42715 Val Loss = 1.59888
2023-09-01 21:58:45.769553 Epoch 56  	Train Loss = 1.42656 Val Loss = 1.59595
2023-09-01 21:59:58.197809 Epoch 57  	Train Loss = 1.42580 Val Loss = 1.59828
2023-09-01 22:01:10.375975 Epoch 58  	Train Loss = 1.42490 Val Loss = 1.59634
2023-09-01 22:02:22.462641 Epoch 59  	Train Loss = 1.42436 Val Loss = 1.59557
2023-09-01 22:03:34.572717 Epoch 60  	Train Loss = 1.42349 Val Loss = 1.59324
2023-09-01 22:04:47.230040 Epoch 61  	Train Loss = 1.42294 Val Loss = 1.59694
2023-09-01 22:06:00.483335 Epoch 62  	Train Loss = 1.42240 Val Loss = 1.59391
2023-09-01 22:07:14.559353 Epoch 63  	Train Loss = 1.42205 Val Loss = 1.58934
2023-09-01 22:08:27.987297 Epoch 64  	Train Loss = 1.42113 Val Loss = 1.59644
2023-09-01 22:09:40.705525 Epoch 65  	Train Loss = 1.42134 Val Loss = 1.59336
2023-09-01 22:10:53.307365 Epoch 66  	Train Loss = 1.41981 Val Loss = 1.59364
2023-09-01 22:12:05.705890 Epoch 67  	Train Loss = 1.41975 Val Loss = 1.59893
2023-09-01 22:13:18.025957 Epoch 68  	Train Loss = 1.41978 Val Loss = 1.59246
2023-09-01 22:14:30.289766 Epoch 69  	Train Loss = 1.41853 Val Loss = 1.58648
2023-09-01 22:15:42.380977 Epoch 70  	Train Loss = 1.41837 Val Loss = 1.58891
2023-09-01 22:16:54.419547 Epoch 71  	Train Loss = 1.41800 Val Loss = 1.59290
2023-09-01 22:18:06.459087 Epoch 72  	Train Loss = 1.41740 Val Loss = 1.58843
2023-09-01 22:19:18.879727 Epoch 73  	Train Loss = 1.41687 Val Loss = 1.59639
2023-09-01 22:20:32.106084 Epoch 74  	Train Loss = 1.41671 Val Loss = 1.59058
2023-09-01 22:21:45.513292 Epoch 75  	Train Loss = 1.41598 Val Loss = 1.59598
2023-09-01 22:22:58.625853 Epoch 76  	Train Loss = 1.41592 Val Loss = 1.59848
2023-09-01 22:24:11.049580 Epoch 77  	Train Loss = 1.41535 Val Loss = 1.58970
2023-09-01 22:25:23.234577 Epoch 78  	Train Loss = 1.41437 Val Loss = 1.58812
2023-09-01 22:26:35.285531 Epoch 79  	Train Loss = 1.41460 Val Loss = 1.59237
Early stopping at epoch: 79
Best at epoch 69:
Train Loss = 1.41853
Train RMSE = 3.08727, MAE = 1.39280, MAPE = 2.97874
Val Loss = 1.58648
Val RMSE = 3.64658, MAE = 1.58230, MAPE = 3.59048
--------- Test ---------
All Steps RMSE = 3.67573, MAE = 1.59165, MAPE = 3.58549
Step 1 RMSE = 1.55690, MAE = 0.86237, MAPE = 1.65944
Step 2 RMSE = 2.25889, MAE = 1.13526, MAPE = 2.29634
Step 3 RMSE = 2.79812, MAE = 1.32466, MAPE = 2.78586
Step 4 RMSE = 3.21418, MAE = 1.46536, MAPE = 3.18318
Step 5 RMSE = 3.53376, MAE = 1.57124, MAPE = 3.49601
Step 6 RMSE = 3.77744, MAE = 1.65458, MAPE = 3.74470
Step 7 RMSE = 3.96450, MAE = 1.72292, MAPE = 3.94738
Step 8 RMSE = 4.11168, MAE = 1.77950, MAPE = 4.11623
Step 9 RMSE = 4.23663, MAE = 1.82961, MAPE = 4.26502
Step 10 RMSE = 4.34289, MAE = 1.87416, MAPE = 4.39233
Step 11 RMSE = 4.44007, MAE = 1.91772, MAPE = 4.51143
Step 12 RMSE = 4.53226, MAE = 1.96235, MAPE = 4.62799
Inference time: 6.92 s
