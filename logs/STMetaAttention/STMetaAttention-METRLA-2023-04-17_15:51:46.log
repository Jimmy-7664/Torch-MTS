METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.0001,
    "weight_decay": 0,
    "milestones": [],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "z_dim": 32,
        "learner_hidden_dim": 64,
        "feed_forward_dim": 64,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 207, 1]          13,248
├─Linear: 1-1                                 [64, 12, 207, 32]         64
├─Sequential: 1-2                             [64, 207, 32]             --
│    └─Linear: 2-1                            [64, 207, 32]             416
│    └─Tanh: 2-2                              [64, 207, 32]             --
│    └─Linear: 2-3                            [64, 207, 32]             1,056
│    └─Tanh: 2-4                              [64, 207, 32]             --
│    └─Linear: 2-5                            [64, 207, 32]             1,056
├─Sequential: 1-3                             [64, 207, 32]             --
│    └─Linear: 2-6                            [64, 207, 32]             416
│    └─Tanh: 2-7                              [64, 207, 32]             --
│    └─Linear: 2-8                            [64, 207, 32]             1,056
│    └─Tanh: 2-9                              [64, 207, 32]             --
│    └─Linear: 2-10                           [64, 207, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-1                   [64, 207, 3072]           207,872
│    │    └─Sequential: 3-2                   [64, 207, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-3         [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-4                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 207, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-7                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 207, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-9                   [64, 207, 3072]           207,872
│    │    └─Sequential: 3-10                  [64, 207, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-11        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-12                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-15                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 207, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 207, 12]         156
├─Linear: 1-7                                 [64, 12, 207, 1]          33
===============================================================================================
Total params: 473,917
Trainable params: 473,917
Non-trainable params: 0
Total mult-adds (M): 29.48
===============================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 1290.04
Params size (MB): 1.84
Estimated Total Size (MB): 1293.79
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 15:52:17.919663 Epoch 1  	Train Loss = 9.29471 Val Loss = 7.20427
2023-04-17 15:52:47.117625 Epoch 2  	Train Loss = 6.25903 Val Loss = 5.42558
2023-04-17 15:53:16.351713 Epoch 3  	Train Loss = 5.00674 Val Loss = 4.39623
2023-04-17 15:53:46.064391 Epoch 4  	Train Loss = 4.10047 Val Loss = 3.57975
2023-04-17 15:54:16.953824 Epoch 5  	Train Loss = 3.60343 Val Loss = 3.37950
2023-04-17 15:54:48.427990 Epoch 6  	Train Loss = 3.51676 Val Loss = 3.31906
2023-04-17 15:55:20.105963 Epoch 7  	Train Loss = 3.48476 Val Loss = 3.30667
2023-04-17 15:55:52.044334 Epoch 8  	Train Loss = 3.45906 Val Loss = 3.27075
2023-04-17 15:56:24.054810 Epoch 9  	Train Loss = 3.43844 Val Loss = 3.29885
2023-04-17 15:56:56.215963 Epoch 10  	Train Loss = 3.42176 Val Loss = 3.24411
2023-04-17 15:57:28.478220 Epoch 11  	Train Loss = 3.40595 Val Loss = 3.23800
2023-04-17 15:58:00.629641 Epoch 12  	Train Loss = 3.39167 Val Loss = 3.22970
2023-04-17 15:58:32.828881 Epoch 13  	Train Loss = 3.37357 Val Loss = 3.20982
2023-04-17 15:59:04.892862 Epoch 14  	Train Loss = 3.36199 Val Loss = 3.20553
2023-04-17 15:59:37.055436 Epoch 15  	Train Loss = 3.34870 Val Loss = 3.19719
2023-04-17 16:00:09.399917 Epoch 16  	Train Loss = 3.33728 Val Loss = 3.17638
2023-04-17 16:00:42.215584 Epoch 17  	Train Loss = 3.32166 Val Loss = 3.16773
2023-04-17 16:01:15.208722 Epoch 18  	Train Loss = 3.31362 Val Loss = 3.16732
2023-04-17 16:01:48.242309 Epoch 19  	Train Loss = 3.30002 Val Loss = 3.15021
2023-04-17 16:02:21.311229 Epoch 20  	Train Loss = 3.28981 Val Loss = 3.14733
2023-04-17 16:02:54.425057 Epoch 21  	Train Loss = 3.28116 Val Loss = 3.13014
2023-04-17 16:03:27.397410 Epoch 22  	Train Loss = 3.26911 Val Loss = 3.13384
2023-04-17 16:03:59.852655 Epoch 23  	Train Loss = 3.26209 Val Loss = 3.12092
2023-04-17 16:04:31.659448 Epoch 24  	Train Loss = 3.25374 Val Loss = 3.12260
2023-04-17 16:05:03.157079 Epoch 25  	Train Loss = 3.24418 Val Loss = 3.12387
2023-04-17 16:05:34.679577 Epoch 26  	Train Loss = 3.23531 Val Loss = 3.10224
2023-04-17 16:06:06.209704 Epoch 27  	Train Loss = 3.22943 Val Loss = 3.09320
2023-04-17 16:06:37.801954 Epoch 28  	Train Loss = 3.21975 Val Loss = 3.10613
2023-04-17 16:07:09.403705 Epoch 29  	Train Loss = 3.21187 Val Loss = 3.09076
2023-04-17 16:07:41.089250 Epoch 30  	Train Loss = 3.20503 Val Loss = 3.09051
2023-04-17 16:08:12.764025 Epoch 31  	Train Loss = 3.19548 Val Loss = 3.06770
2023-04-17 16:08:44.592854 Epoch 32  	Train Loss = 3.18676 Val Loss = 3.06605
2023-04-17 16:09:16.477953 Epoch 33  	Train Loss = 3.17445 Val Loss = 3.06669
2023-04-17 16:09:48.411454 Epoch 34  	Train Loss = 3.16363 Val Loss = 3.05704
2023-04-17 16:10:20.386637 Epoch 35  	Train Loss = 3.14999 Val Loss = 3.04519
2023-04-17 16:10:52.261691 Epoch 36  	Train Loss = 3.13523 Val Loss = 3.03892
2023-04-17 16:11:24.287207 Epoch 37  	Train Loss = 3.11912 Val Loss = 3.00784
2023-04-17 16:11:56.304234 Epoch 38  	Train Loss = 3.10379 Val Loss = 3.00412
2023-04-17 16:12:28.322273 Epoch 39  	Train Loss = 3.09405 Val Loss = 2.99339
2023-04-17 16:13:00.382710 Epoch 40  	Train Loss = 3.07869 Val Loss = 2.98745
2023-04-17 16:13:32.314450 Epoch 41  	Train Loss = 3.06963 Val Loss = 2.98380
2023-04-17 16:14:04.342916 Epoch 42  	Train Loss = 3.05868 Val Loss = 2.96800
2023-04-17 16:14:36.559533 Epoch 43  	Train Loss = 3.05145 Val Loss = 2.97136
2023-04-17 16:15:09.081711 Epoch 44  	Train Loss = 3.04235 Val Loss = 2.96750
2023-04-17 16:15:41.883809 Epoch 45  	Train Loss = 3.03403 Val Loss = 2.97588
2023-04-17 16:16:14.720008 Epoch 46  	Train Loss = 3.02644 Val Loss = 2.94129
2023-04-17 16:16:47.582535 Epoch 47  	Train Loss = 3.02219 Val Loss = 2.95282
2023-04-17 16:17:20.477106 Epoch 48  	Train Loss = 3.01241 Val Loss = 2.95064
2023-04-17 16:17:53.186796 Epoch 49  	Train Loss = 3.00546 Val Loss = 2.94754
2023-04-17 16:18:25.354169 Epoch 50  	Train Loss = 3.00082 Val Loss = 2.93904
2023-04-17 16:18:56.986691 Epoch 51  	Train Loss = 2.99368 Val Loss = 2.94359
2023-04-17 16:19:28.316179 Epoch 52  	Train Loss = 2.98894 Val Loss = 2.92133
2023-04-17 16:19:59.605215 Epoch 53  	Train Loss = 2.98131 Val Loss = 2.93283
2023-04-17 16:20:31.004568 Epoch 54  	Train Loss = 2.97707 Val Loss = 2.93268
2023-04-17 16:21:02.474760 Epoch 55  	Train Loss = 2.97282 Val Loss = 2.93461
2023-04-17 16:21:34.033261 Epoch 56  	Train Loss = 2.96694 Val Loss = 2.92530
2023-04-17 16:22:05.587187 Epoch 57  	Train Loss = 2.96274 Val Loss = 2.92460
2023-04-17 16:22:37.233872 Epoch 58  	Train Loss = 2.95667 Val Loss = 2.92507
2023-04-17 16:23:08.925724 Epoch 59  	Train Loss = 2.95297 Val Loss = 2.92715
2023-04-17 16:23:40.698023 Epoch 60  	Train Loss = 2.94626 Val Loss = 2.91473
2023-04-17 16:24:12.493825 Epoch 61  	Train Loss = 2.94217 Val Loss = 2.92548
2023-04-17 16:24:44.369355 Epoch 62  	Train Loss = 2.93808 Val Loss = 2.92845
2023-04-17 16:25:16.171729 Epoch 63  	Train Loss = 2.93264 Val Loss = 2.91266
2023-04-17 16:25:48.028228 Epoch 64  	Train Loss = 2.92711 Val Loss = 2.91341
2023-04-17 16:26:20.000974 Epoch 65  	Train Loss = 2.92413 Val Loss = 2.91389
2023-04-17 16:26:51.937188 Epoch 66  	Train Loss = 2.91804 Val Loss = 2.91151
2023-04-17 16:27:23.725423 Epoch 67  	Train Loss = 2.91386 Val Loss = 2.91508
2023-04-17 16:27:55.484780 Epoch 68  	Train Loss = 2.91030 Val Loss = 2.91173
2023-04-17 16:28:27.221623 Epoch 69  	Train Loss = 2.90650 Val Loss = 2.91409
2023-04-17 16:28:59.009010 Epoch 70  	Train Loss = 2.90159 Val Loss = 2.90992
2023-04-17 16:29:30.857633 Epoch 71  	Train Loss = 2.89883 Val Loss = 2.90436
2023-04-17 16:30:02.660110 Epoch 72  	Train Loss = 2.89221 Val Loss = 2.89231
2023-04-17 16:30:34.364942 Epoch 73  	Train Loss = 2.89083 Val Loss = 2.89815
2023-04-17 16:31:06.048999 Epoch 74  	Train Loss = 2.88498 Val Loss = 2.89299
2023-04-17 16:31:37.793938 Epoch 75  	Train Loss = 2.88035 Val Loss = 2.88173
2023-04-17 16:32:09.548259 Epoch 76  	Train Loss = 2.87835 Val Loss = 2.89129
2023-04-17 16:32:40.993540 Epoch 77  	Train Loss = 2.87237 Val Loss = 2.88950
2023-04-17 16:33:11.976224 Epoch 78  	Train Loss = 2.87069 Val Loss = 2.88684
2023-04-17 16:33:42.794298 Epoch 79  	Train Loss = 2.86502 Val Loss = 2.89090
2023-04-17 16:34:13.675100 Epoch 80  	Train Loss = 2.86088 Val Loss = 2.88475
2023-04-17 16:34:44.543500 Epoch 81  	Train Loss = 2.85815 Val Loss = 2.88424
2023-04-17 16:35:15.573002 Epoch 82  	Train Loss = 2.85558 Val Loss = 2.89406
2023-04-17 16:35:46.627353 Epoch 83  	Train Loss = 2.84923 Val Loss = 2.89732
2023-04-17 16:36:17.766181 Epoch 84  	Train Loss = 2.84780 Val Loss = 2.89015
2023-04-17 16:36:49.015466 Epoch 85  	Train Loss = 2.84400 Val Loss = 2.89893
Early stopping at epoch: 85
Best at epoch 75:
Train Loss = 2.88035
Train RMSE = 5.64460, MAE = 2.83860, MAPE = 7.53779
Val Loss = 2.88173
Val RMSE = 6.09885, MAE = 2.93367, MAPE = 8.31580
--------- Test ---------
All Steps RMSE = 6.35759, MAE = 3.15286, MAPE = 8.74849
Step 1 RMSE = 4.46566, MAE = 2.46739, MAPE = 6.22161
Step 2 RMSE = 5.07495, MAE = 2.67484, MAPE = 6.91403
Step 3 RMSE = 5.50104, MAE = 2.83666, MAPE = 7.50025
Step 4 RMSE = 5.82107, MAE = 2.96244, MAPE = 7.99062
Step 5 RMSE = 6.12642, MAE = 3.07434, MAPE = 8.42546
Step 6 RMSE = 6.41483, MAE = 3.17888, MAPE = 8.84737
Step 7 RMSE = 6.58831, MAE = 3.26768, MAPE = 9.20543
Step 8 RMSE = 6.78036, MAE = 3.34113, MAPE = 9.50989
Step 9 RMSE = 6.95855, MAE = 3.40705, MAPE = 9.73766
Step 10 RMSE = 7.10976, MAE = 3.47089, MAPE = 9.96512
Step 11 RMSE = 7.25129, MAE = 3.53319, MAPE = 10.19737
Step 12 RMSE = 7.45461, MAE = 3.61989, MAPE = 10.46725
Inference time: 2.73 s
