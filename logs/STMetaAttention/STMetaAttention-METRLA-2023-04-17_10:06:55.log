METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        20,
        40
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "learner_hidden_dim": 128,
        "z_dim": 32,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0.1,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 207, 1]          13,248
├─Linear: 1-1                                 [64, 12, 207, 32]         64
├─Sequential: 1-2                             [64, 207, 32]             --
│    └─Linear: 2-1                            [64, 207, 32]             416
│    └─Tanh: 2-2                              [64, 207, 32]             --
│    └─Linear: 2-3                            [64, 207, 32]             1,056
│    └─Tanh: 2-4                              [64, 207, 32]             --
│    └─Linear: 2-5                            [64, 207, 32]             1,056
├─Sequential: 1-3                             [64, 207, 32]             --
│    └─Linear: 2-6                            [64, 207, 32]             416
│    └─Tanh: 2-7                              [64, 207, 32]             --
│    └─Linear: 2-8                            [64, 207, 32]             1,056
│    └─Tanh: 2-9                              [64, 207, 32]             --
│    └─Linear: 2-10                           [64, 207, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-1                   [64, 207, 3072]           412,672
│    │    └─Sequential: 3-2                   [64, 207, 96]             28,768
│    │    └─STMetaAttentionLayer: 3-3         [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-4                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 207, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-7                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 207, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-9                   [64, 207, 3072]           412,672
│    │    └─Sequential: 3-10                  [64, 207, 96]             28,768
│    │    └─STMetaAttentionLayer: 3-11        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-12                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-15                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 207, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 207, 12]         156
├─Linear: 1-7                                 [64, 12, 207, 1]          33
===============================================================================================
Total params: 920,509
Trainable params: 920,509
Non-trainable params: 0
Total mult-adds (M): 58.06
===============================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 1479.96
Params size (MB): 3.63
Estimated Total Size (MB): 1485.50
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 10:07:28.719395 Epoch 1  	Train Loss = 5.24685 Val Loss = 3.48003
2023-04-17 10:07:59.462961 Epoch 2  	Train Loss = 3.55730 Val Loss = 3.32586
2023-04-17 10:08:30.496232 Epoch 3  	Train Loss = 3.42849 Val Loss = 3.25776
2023-04-17 10:09:02.312945 Epoch 4  	Train Loss = 3.33919 Val Loss = 3.15410
2023-04-17 10:09:34.586287 Epoch 5  	Train Loss = 3.27485 Val Loss = 3.10741
2023-04-17 10:10:07.147613 Epoch 6  	Train Loss = 3.23044 Val Loss = 3.09900
2023-04-17 10:10:39.802260 Epoch 7  	Train Loss = 3.17469 Val Loss = 3.04179
2023-04-17 10:11:12.475837 Epoch 8  	Train Loss = 3.09915 Val Loss = 3.01272
2023-04-17 10:11:45.324751 Epoch 9  	Train Loss = 3.02373 Val Loss = 2.98878
2023-04-17 10:12:18.253414 Epoch 10  	Train Loss = 2.98005 Val Loss = 2.92975
2023-04-17 10:12:51.231057 Epoch 11  	Train Loss = 2.93975 Val Loss = 2.93412
2023-04-17 10:13:24.231379 Epoch 12  	Train Loss = 2.90401 Val Loss = 2.93916
2023-04-17 10:13:57.220343 Epoch 13  	Train Loss = 2.87790 Val Loss = 2.93103
2023-04-17 10:14:30.251457 Epoch 14  	Train Loss = 2.84921 Val Loss = 2.93694
2023-04-17 10:15:03.233646 Epoch 15  	Train Loss = 2.82378 Val Loss = 2.92083
2023-04-17 10:15:36.118573 Epoch 16  	Train Loss = 2.80484 Val Loss = 2.94433
2023-04-17 10:16:08.897838 Epoch 17  	Train Loss = 2.78065 Val Loss = 2.92999
2023-04-17 10:16:41.543361 Epoch 18  	Train Loss = 2.76321 Val Loss = 2.91693
2023-04-17 10:17:14.173871 Epoch 19  	Train Loss = 2.74464 Val Loss = 2.93665
2023-04-17 10:17:46.799391 Epoch 20  	Train Loss = 2.73348 Val Loss = 2.96869
2023-04-17 10:18:19.406867 Epoch 21  	Train Loss = 2.65344 Val Loss = 2.92801
2023-04-17 10:18:52.024278 Epoch 22  	Train Loss = 2.62938 Val Loss = 2.92533
2023-04-17 10:19:24.656422 Epoch 23  	Train Loss = 2.61967 Val Loss = 2.94671
2023-04-17 10:19:57.355498 Epoch 24  	Train Loss = 2.61301 Val Loss = 2.93654
2023-04-17 10:20:30.050841 Epoch 25  	Train Loss = 2.60692 Val Loss = 2.95106
2023-04-17 10:21:02.775761 Epoch 26  	Train Loss = 2.60082 Val Loss = 2.94637
2023-04-17 10:21:35.517091 Epoch 27  	Train Loss = 2.59617 Val Loss = 2.95915
2023-04-17 10:22:08.330381 Epoch 28  	Train Loss = 2.59153 Val Loss = 2.96463
Early stopping at epoch: 28
Best at epoch 18:
Train Loss = 2.76321
Train RMSE = 5.05678, MAE = 2.59839, MAPE = 6.59545
Val Loss = 2.91693
Val RMSE = 6.37345, MAE = 3.00119, MAPE = 8.58401
--------- Test ---------
All Steps RMSE = 6.69233, MAE = 3.23823, MAPE = 8.99984
Step 1 RMSE = 4.84397, MAE = 2.57717, MAPE = 6.55374
Step 2 RMSE = 5.34741, MAE = 2.76191, MAPE = 7.15065
Step 3 RMSE = 5.74149, MAE = 2.91153, MAPE = 7.70080
Step 4 RMSE = 6.11186, MAE = 3.04972, MAPE = 8.23980
Step 5 RMSE = 6.45206, MAE = 3.16887, MAPE = 8.71535
Step 6 RMSE = 6.72425, MAE = 3.27100, MAPE = 9.12376
Step 7 RMSE = 6.94726, MAE = 3.35786, MAPE = 9.46723
Step 8 RMSE = 7.14842, MAE = 3.43268, MAPE = 9.76691
Step 9 RMSE = 7.32704, MAE = 3.49763, MAPE = 10.02437
Step 10 RMSE = 7.49057, MAE = 3.55674, MAPE = 10.23756
Step 11 RMSE = 7.64044, MAE = 3.60954, MAPE = 10.42021
Step 12 RMSE = 7.79049, MAE = 3.66419, MAPE = 10.59798
Inference time: 3.00 s
