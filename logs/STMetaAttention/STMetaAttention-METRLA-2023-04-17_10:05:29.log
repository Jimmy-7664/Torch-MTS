METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        10,
        40
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "learner_hidden_dim": 96,
        "z_dim": 32,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 207, 1]          13,248
├─Linear: 1-1                                 [64, 12, 207, 32]         64
├─Sequential: 1-2                             [64, 207, 32]             --
│    └─Linear: 2-1                            [64, 207, 32]             416
│    └─Tanh: 2-2                              [64, 207, 32]             --
│    └─Linear: 2-3                            [64, 207, 32]             1,056
│    └─Tanh: 2-4                              [64, 207, 32]             --
│    └─Linear: 2-5                            [64, 207, 32]             1,056
├─Sequential: 1-3                             [64, 207, 32]             --
│    └─Linear: 2-6                            [64, 207, 32]             416
│    └─Tanh: 2-7                              [64, 207, 32]             --
│    └─Linear: 2-8                            [64, 207, 32]             1,056
│    └─Tanh: 2-9                              [64, 207, 32]             --
│    └─Linear: 2-10                           [64, 207, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-1                   [64, 207, 3072]           310,272
│    │    └─Sequential: 3-2                   [64, 207, 96]             21,600
│    │    └─STMetaAttentionLayer: 3-3         [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-4                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 207, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-7                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 207, 12, 32]         64
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-9                   [64, 207, 3072]           310,272
│    │    └─Sequential: 3-10                  [64, 207, 96]             21,600
│    │    └─STMetaAttentionLayer: 3-11        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-12                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-15                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 207, 12, 32]         64
│    └─STMetaSelfAttentionLayer: 2-13         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-17                  [64, 207, 3072]           310,272
│    │    └─Sequential: 3-18                  [64, 207, 96]             21,600
│    │    └─STMetaAttentionLayer: 3-19        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-20                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-21                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-22                  [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-23                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-24                   [64, 207, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-14         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-25                  [64, 207, 3072]           310,272
│    │    └─Sequential: 3-26                  [64, 207, 96]             21,600
│    │    └─STMetaAttentionLayer: 3-27        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-28                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-29                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-30                  [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-31                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-32                   [64, 207, 12, 32]         64
│    └─STMetaSelfAttentionLayer: 2-15         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-33                  [64, 207, 3072]           310,272
│    │    └─Sequential: 3-34                  [64, 207, 96]             21,600
│    │    └─STMetaAttentionLayer: 3-35        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-36                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-37                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-38                  [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-39                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-40                   [64, 207, 12, 32]         64
│    └─STMetaSelfAttentionLayer: 2-16         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-41                  [64, 207, 3072]           310,272
│    │    └─Sequential: 3-42                  [64, 207, 96]             21,600
│    │    └─STMetaAttentionLayer: 3-43        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-44                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-45                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-46                  [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-47                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-48                   [64, 207, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 207, 12]         156
├─Linear: 1-7                                 [64, 12, 207, 1]          33
===============================================================================================
Total params: 2,067,005
Trainable params: 2,067,005
Non-trainable params: 0
Total mult-adds (M): 131.44
===============================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 4193.15
Params size (MB): 8.22
Estimated Total Size (MB): 4203.27
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 10:07:00.336723 Epoch 1  	Train Loss = 6.02807 Val Loss = 3.74699
2023-04-17 10:08:29.416300 Epoch 2  	Train Loss = 3.69424 Val Loss = 3.47816
2023-04-17 10:09:58.505655 Epoch 3  	Train Loss = 3.52923 Val Loss = 3.30580
2023-04-17 10:11:27.878616 Epoch 4  	Train Loss = 3.43307 Val Loss = 3.22506
2023-04-17 10:12:57.014896 Epoch 5  	Train Loss = 3.35851 Val Loss = 3.23056
2023-04-17 10:14:26.161567 Epoch 6  	Train Loss = 3.29971 Val Loss = 3.15021
2023-04-17 10:15:55.272556 Epoch 7  	Train Loss = 3.24354 Val Loss = 3.11131
2023-04-17 10:17:24.365109 Epoch 8  	Train Loss = 3.19675 Val Loss = 3.07116
2023-04-17 10:18:53.366183 Epoch 9  	Train Loss = 3.15098 Val Loss = 3.05428
2023-04-17 10:20:22.404838 Epoch 10  	Train Loss = 3.12401 Val Loss = 3.04771
2023-04-17 10:21:51.462063 Epoch 11  	Train Loss = 3.03679 Val Loss = 3.00145
2023-04-17 10:23:20.486057 Epoch 12  	Train Loss = 3.01825 Val Loss = 3.00690
2023-04-17 10:24:49.441486 Epoch 13  	Train Loss = 3.00599 Val Loss = 2.99548
2023-04-17 10:26:18.419114 Epoch 14  	Train Loss = 2.99656 Val Loss = 3.00324
2023-04-17 10:27:47.713039 Epoch 15  	Train Loss = 2.98743 Val Loss = 3.01027
2023-04-17 10:29:16.766126 Epoch 16  	Train Loss = 2.97851 Val Loss = 2.99860
2023-04-17 10:30:45.860231 Epoch 17  	Train Loss = 2.96845 Val Loss = 2.99609
2023-04-17 10:32:14.862557 Epoch 18  	Train Loss = 2.95913 Val Loss = 3.01181
2023-04-17 10:33:43.947915 Epoch 19  	Train Loss = 2.95046 Val Loss = 3.00961
2023-04-17 10:35:13.013453 Epoch 20  	Train Loss = 2.94162 Val Loss = 3.00548
2023-04-17 10:36:42.116695 Epoch 21  	Train Loss = 2.93243 Val Loss = 3.01752
2023-04-17 10:38:11.278404 Epoch 22  	Train Loss = 2.92475 Val Loss = 3.01376
2023-04-17 10:39:40.350332 Epoch 23  	Train Loss = 2.91637 Val Loss = 3.00855
Early stopping at epoch: 23
Best at epoch 13:
Train Loss = 3.00599
Train RMSE = 5.90903, MAE = 2.90423, MAPE = 7.69523
Val Loss = 2.99548
Val RMSE = 6.49554, MAE = 3.05127, MAPE = 8.78294
--------- Test ---------
All Steps RMSE = 6.89744, MAE = 3.32565, MAPE = 9.35953
Step 1 RMSE = 6.50013, MAE = 3.17064, MAPE = 8.76141
Step 2 RMSE = 6.42525, MAE = 3.14202, MAPE = 8.69580
Step 3 RMSE = 6.36699, MAE = 3.11838, MAPE = 8.64220
Step 4 RMSE = 6.38218, MAE = 3.12097, MAPE = 8.67509
Step 5 RMSE = 6.43964, MAE = 3.13741, MAPE = 8.76561
Step 6 RMSE = 6.56129, MAE = 3.18260, MAPE = 8.92120
Step 7 RMSE = 6.72208, MAE = 3.24683, MAPE = 9.13419
Step 8 RMSE = 6.91841, MAE = 3.32963, MAPE = 9.40277
Step 9 RMSE = 7.14859, MAE = 3.42756, MAPE = 9.74115
Step 10 RMSE = 7.40441, MAE = 3.54416, MAPE = 10.10683
Step 11 RMSE = 7.68567, MAE = 3.67488, MAPE = 10.51685
Step 12 RMSE = 7.97022, MAE = 3.81283, MAPE = 10.95145
Inference time: 7.64 s
