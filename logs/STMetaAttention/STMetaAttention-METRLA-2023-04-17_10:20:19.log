METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        20
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "learner_hidden_dim": 128,
        "z_dim": 32,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 207, 1]          13,248
├─Linear: 1-1                                 [64, 12, 207, 32]         64
├─Sequential: 1-2                             [64, 207, 32]             --
│    └─Linear: 2-1                            [64, 207, 32]             416
│    └─Tanh: 2-2                              [64, 207, 32]             --
│    └─Linear: 2-3                            [64, 207, 32]             1,056
│    └─Tanh: 2-4                              [64, 207, 32]             --
│    └─Linear: 2-5                            [64, 207, 32]             1,056
├─Sequential: 1-3                             [64, 207, 32]             --
│    └─Linear: 2-6                            [64, 207, 32]             416
│    └─Tanh: 2-7                              [64, 207, 32]             --
│    └─Linear: 2-8                            [64, 207, 32]             1,056
│    └─Tanh: 2-9                              [64, 207, 32]             --
│    └─Linear: 2-10                           [64, 207, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-1                   [64, 207, 3072]           412,672
│    │    └─Sequential: 3-2                   [64, 207, 96]             28,768
│    │    └─STMetaAttentionLayer: 3-3         [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-4                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 207, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-7                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 207, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-9                   [64, 207, 3072]           412,672
│    │    └─Sequential: 3-10                  [64, 207, 96]             28,768
│    │    └─STMetaAttentionLayer: 3-11        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-12                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 207, 12, 32]         8,352
│    │    └─Dropout: 3-15                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 207, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 207, 12]         156
├─Linear: 1-7                                 [64, 12, 207, 1]          33
===============================================================================================
Total params: 920,509
Trainable params: 920,509
Non-trainable params: 0
Total mult-adds (M): 58.06
===============================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 1479.96
Params size (MB): 3.63
Estimated Total Size (MB): 1485.50
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 10:20:52.003352 Epoch 1  	Train Loss = 5.21333 Val Loss = 3.49506
2023-04-17 10:21:23.241167 Epoch 2  	Train Loss = 3.53098 Val Loss = 3.28849
2023-04-17 10:21:56.325333 Epoch 3  	Train Loss = 3.41767 Val Loss = 3.19344
2023-04-17 10:22:31.221553 Epoch 4  	Train Loss = 3.32347 Val Loss = 3.15786
2023-04-17 10:23:06.752760 Epoch 5  	Train Loss = 3.25291 Val Loss = 3.08030
2023-04-17 10:23:42.781463 Epoch 6  	Train Loss = 3.20935 Val Loss = 3.06834
2023-04-17 10:24:19.018011 Epoch 7  	Train Loss = 3.14487 Val Loss = 3.02674
2023-04-17 10:24:55.331832 Epoch 8  	Train Loss = 3.07910 Val Loss = 3.02520
2023-04-17 10:25:31.479105 Epoch 9  	Train Loss = 2.99934 Val Loss = 2.96247
2023-04-17 10:26:07.722401 Epoch 10  	Train Loss = 2.94055 Val Loss = 2.92470
2023-04-17 10:26:43.984039 Epoch 11  	Train Loss = 2.85692 Val Loss = 2.88106
2023-04-17 10:27:20.174366 Epoch 12  	Train Loss = 2.83800 Val Loss = 2.89116
2023-04-17 10:27:56.557269 Epoch 13  	Train Loss = 2.82736 Val Loss = 2.87968
2023-04-17 10:28:32.872830 Epoch 14  	Train Loss = 2.81855 Val Loss = 2.88455
2023-04-17 10:29:09.235496 Epoch 15  	Train Loss = 2.80957 Val Loss = 2.87931
2023-04-17 10:29:45.071041 Epoch 16  	Train Loss = 2.80152 Val Loss = 2.86815
2023-04-17 10:30:20.529403 Epoch 17  	Train Loss = 2.79229 Val Loss = 2.89160
2023-04-17 10:30:55.838094 Epoch 18  	Train Loss = 2.78503 Val Loss = 2.87890
2023-04-17 10:31:31.173939 Epoch 19  	Train Loss = 2.77607 Val Loss = 2.88305
2023-04-17 10:32:06.656127 Epoch 20  	Train Loss = 2.76882 Val Loss = 2.88087
2023-04-17 10:32:42.134893 Epoch 21  	Train Loss = 2.75485 Val Loss = 2.87622
2023-04-17 10:33:17.852478 Epoch 22  	Train Loss = 2.75170 Val Loss = 2.87351
2023-04-17 10:33:53.635696 Epoch 23  	Train Loss = 2.75079 Val Loss = 2.87598
2023-04-17 10:34:29.558659 Epoch 24  	Train Loss = 2.74975 Val Loss = 2.87475
2023-04-17 10:35:05.735420 Epoch 25  	Train Loss = 2.74893 Val Loss = 2.87201
2023-04-17 10:35:42.002778 Epoch 26  	Train Loss = 2.74668 Val Loss = 2.87131
Early stopping at epoch: 26
Best at epoch 16:
Train Loss = 2.80152
Train RMSE = 5.42251, MAE = 2.74624, MAPE = 7.25223
Val Loss = 2.86815
Val RMSE = 6.05857, MAE = 2.90991, MAPE = 8.24283
--------- Test ---------
All Steps RMSE = 6.37363, MAE = 3.14921, MAPE = 8.79937
Step 1 RMSE = 4.67241, MAE = 2.53817, MAPE = 6.50329
Step 2 RMSE = 5.19354, MAE = 2.71490, MAPE = 7.10550
Step 3 RMSE = 5.56864, MAE = 2.85695, MAPE = 7.62819
Step 4 RMSE = 5.81382, MAE = 2.95260, MAPE = 8.00803
Step 5 RMSE = 6.09849, MAE = 3.05933, MAPE = 8.44346
Step 6 RMSE = 6.39775, MAE = 3.16661, MAPE = 8.86571
Step 7 RMSE = 6.56446, MAE = 3.25385, MAPE = 9.24752
Step 8 RMSE = 6.77278, MAE = 3.32290, MAPE = 9.53370
Step 9 RMSE = 6.95189, MAE = 3.38220, MAPE = 9.70990
Step 10 RMSE = 7.10967, MAE = 3.44943, MAPE = 9.93708
Step 11 RMSE = 7.23506, MAE = 3.49988, MAPE = 10.16308
Step 12 RMSE = 7.45368, MAE = 3.59370, MAPE = 10.44730
Inference time: 2.92 s
