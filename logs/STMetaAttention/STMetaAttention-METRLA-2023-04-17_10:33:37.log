METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        20
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "learner_hidden_dim": 64,
        "z_dim": 32,
        "feed_forward_dim": 64,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 207, 1]          13,248
├─Linear: 1-1                                 [64, 12, 207, 32]         64
├─Sequential: 1-2                             [64, 207, 32]             --
│    └─Linear: 2-1                            [64, 207, 32]             416
│    └─Tanh: 2-2                              [64, 207, 32]             --
│    └─Linear: 2-3                            [64, 207, 32]             1,056
│    └─Tanh: 2-4                              [64, 207, 32]             --
│    └─Linear: 2-5                            [64, 207, 32]             1,056
├─Sequential: 1-3                             [64, 207, 32]             --
│    └─Linear: 2-6                            [64, 207, 32]             416
│    └─Tanh: 2-7                              [64, 207, 32]             --
│    └─Linear: 2-8                            [64, 207, 32]             1,056
│    └─Tanh: 2-9                              [64, 207, 32]             --
│    └─Linear: 2-10                           [64, 207, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-1                   [64, 207, 3072]           207,872
│    │    └─Sequential: 3-2                   [64, 207, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-3         [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-4                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 207, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-7                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 207, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-9                   [64, 207, 3072]           207,872
│    │    └─Sequential: 3-10                  [64, 207, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-11        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-12                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-15                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 207, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 207, 12]         156
├─Linear: 1-7                                 [64, 12, 207, 1]          33
===============================================================================================
Total params: 473,917
Trainable params: 473,917
Non-trainable params: 0
Total mult-adds (M): 29.48
===============================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 1290.04
Params size (MB): 1.84
Estimated Total Size (MB): 1293.79
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 10:34:08.312848 Epoch 1  	Train Loss = 5.04598 Val Loss = 3.48594
2023-04-17 10:34:41.242414 Epoch 2  	Train Loss = 3.53096 Val Loss = 3.28509
2023-04-17 10:35:14.608481 Epoch 3  	Train Loss = 3.41344 Val Loss = 3.19859
2023-04-17 10:35:48.322818 Epoch 4  	Train Loss = 3.33817 Val Loss = 3.16124
2023-04-17 10:36:22.976985 Epoch 5  	Train Loss = 3.26771 Val Loss = 3.11328
2023-04-17 10:36:57.555583 Epoch 6  	Train Loss = 3.20642 Val Loss = 3.05894
2023-04-17 10:37:32.176536 Epoch 7  	Train Loss = 3.12545 Val Loss = 3.00434
2023-04-17 10:38:06.472007 Epoch 8  	Train Loss = 3.05503 Val Loss = 2.94146
2023-04-17 10:38:37.260045 Epoch 9  	Train Loss = 3.00345 Val Loss = 2.94130
2023-04-17 10:39:07.913008 Epoch 10  	Train Loss = 2.96501 Val Loss = 2.93587
2023-04-17 10:39:38.763206 Epoch 11  	Train Loss = 2.88887 Val Loss = 2.88610
2023-04-17 10:40:13.272820 Epoch 12  	Train Loss = 2.87494 Val Loss = 2.88590
2023-04-17 10:40:45.138565 Epoch 13  	Train Loss = 2.86556 Val Loss = 2.88434
2023-04-17 10:41:15.847786 Epoch 14  	Train Loss = 2.85921 Val Loss = 2.88132
2023-04-17 10:41:46.683692 Epoch 15  	Train Loss = 2.85236 Val Loss = 2.88401
2023-04-17 10:42:21.108273 Epoch 16  	Train Loss = 2.84455 Val Loss = 2.87956
2023-04-17 10:42:51.948538 Epoch 17  	Train Loss = 2.83842 Val Loss = 2.87548
2023-04-17 10:43:23.345710 Epoch 18  	Train Loss = 2.83133 Val Loss = 2.87855
2023-04-17 10:43:57.697136 Epoch 19  	Train Loss = 2.82591 Val Loss = 2.87236
2023-04-17 10:44:31.879774 Epoch 20  	Train Loss = 2.81975 Val Loss = 2.87012
2023-04-17 10:45:02.774860 Epoch 21  	Train Loss = 2.80841 Val Loss = 2.86925
2023-04-17 10:45:33.557387 Epoch 22  	Train Loss = 2.80694 Val Loss = 2.86904
2023-04-17 10:46:04.510573 Epoch 23  	Train Loss = 2.80581 Val Loss = 2.87045
2023-04-17 10:46:35.580558 Epoch 24  	Train Loss = 2.80603 Val Loss = 2.87298
2023-04-17 10:47:06.381311 Epoch 25  	Train Loss = 2.80477 Val Loss = 2.87198
2023-04-17 10:47:37.210340 Epoch 26  	Train Loss = 2.80364 Val Loss = 2.86881
2023-04-17 10:48:08.100033 Epoch 27  	Train Loss = 2.80282 Val Loss = 2.86665
2023-04-17 10:48:39.098767 Epoch 28  	Train Loss = 2.80236 Val Loss = 2.86797
2023-04-17 10:49:10.560120 Epoch 29  	Train Loss = 2.80091 Val Loss = 2.87039
2023-04-17 10:49:41.084295 Epoch 30  	Train Loss = 2.80002 Val Loss = 2.86880
2023-04-17 10:50:11.523816 Epoch 31  	Train Loss = 2.79959 Val Loss = 2.86929
2023-04-17 10:50:42.077633 Epoch 32  	Train Loss = 2.79931 Val Loss = 2.86777
2023-04-17 10:51:12.969992 Epoch 33  	Train Loss = 2.79887 Val Loss = 2.86987
2023-04-17 10:51:43.985096 Epoch 34  	Train Loss = 2.79794 Val Loss = 2.87022
2023-04-17 10:52:14.980686 Epoch 35  	Train Loss = 2.79684 Val Loss = 2.86872
2023-04-17 10:52:45.828056 Epoch 36  	Train Loss = 2.79657 Val Loss = 2.86368
2023-04-17 10:53:16.614158 Epoch 37  	Train Loss = 2.79582 Val Loss = 2.87080
2023-04-17 10:53:47.620322 Epoch 38  	Train Loss = 2.79509 Val Loss = 2.86775
2023-04-17 10:54:18.679718 Epoch 39  	Train Loss = 2.79478 Val Loss = 2.86330
2023-04-17 10:54:49.729127 Epoch 40  	Train Loss = 2.79296 Val Loss = 2.86519
2023-04-17 10:55:20.856880 Epoch 41  	Train Loss = 2.79315 Val Loss = 2.86337
2023-04-17 10:55:51.962226 Epoch 42  	Train Loss = 2.79222 Val Loss = 2.86525
2023-04-17 10:56:23.011006 Epoch 43  	Train Loss = 2.79187 Val Loss = 2.86798
2023-04-17 10:56:53.942332 Epoch 44  	Train Loss = 2.79125 Val Loss = 2.86424
2023-04-17 10:57:24.812816 Epoch 45  	Train Loss = 2.79027 Val Loss = 2.86378
2023-04-17 10:57:57.217387 Epoch 46  	Train Loss = 2.78961 Val Loss = 2.87091
2023-04-17 10:58:31.541221 Epoch 47  	Train Loss = 2.78933 Val Loss = 2.86659
2023-04-17 10:59:05.710109 Epoch 48  	Train Loss = 2.78801 Val Loss = 2.86570
2023-04-17 10:59:39.688983 Epoch 49  	Train Loss = 2.78764 Val Loss = 2.87150
Early stopping at epoch: 49
Best at epoch 39:
Train Loss = 2.79478
Train RMSE = 5.50597, MAE = 2.78700, MAPE = 7.37784
Val Loss = 2.86330
Val RMSE = 5.98896, MAE = 2.90078, MAPE = 8.19520
--------- Test ---------
All Steps RMSE = 6.34326, MAE = 3.14086, MAPE = 8.74397
Step 1 RMSE = 4.55085, MAE = 2.49940, MAPE = 6.34483
Step 2 RMSE = 5.11036, MAE = 2.69042, MAPE = 6.98179
Step 3 RMSE = 5.51766, MAE = 2.84476, MAPE = 7.54946
Step 4 RMSE = 5.80794, MAE = 2.95813, MAPE = 7.99580
Step 5 RMSE = 6.10179, MAE = 3.06482, MAPE = 8.42461
Step 6 RMSE = 6.39392, MAE = 3.16624, MAPE = 8.84060
Step 7 RMSE = 6.55832, MAE = 3.25168, MAPE = 9.20274
Step 8 RMSE = 6.75667, MAE = 3.31996, MAPE = 9.48845
Step 9 RMSE = 6.92528, MAE = 3.37731, MAPE = 9.68059
Step 10 RMSE = 7.07549, MAE = 3.43958, MAPE = 9.89517
Step 11 RMSE = 7.21381, MAE = 3.49694, MAPE = 10.12748
Step 12 RMSE = 7.41641, MAE = 3.58115, MAPE = 10.39628
Inference time: 2.71 s
