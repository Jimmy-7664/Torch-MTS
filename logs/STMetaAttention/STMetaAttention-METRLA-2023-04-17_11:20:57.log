METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        30
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "z_dim": 32,
        "learner_hidden_dim": 64,
        "feed_forward_dim": 64,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 207, 1]          13,248
├─Linear: 1-1                                 [64, 12, 207, 32]         64
├─Sequential: 1-2                             [64, 207, 32]             --
│    └─Linear: 2-1                            [64, 207, 32]             416
│    └─Tanh: 2-2                              [64, 207, 32]             --
│    └─Linear: 2-3                            [64, 207, 32]             1,056
│    └─Tanh: 2-4                              [64, 207, 32]             --
│    └─Linear: 2-5                            [64, 207, 32]             1,056
├─Sequential: 1-3                             [64, 207, 32]             --
│    └─Linear: 2-6                            [64, 207, 32]             416
│    └─Tanh: 2-7                              [64, 207, 32]             --
│    └─Linear: 2-8                            [64, 207, 32]             1,056
│    └─Tanh: 2-9                              [64, 207, 32]             --
│    └─Linear: 2-10                           [64, 207, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-1                   [64, 207, 4096]           274,432
│    │    └─Sequential: 3-2                   [64, 207, 128]            16,512
│    │    └─STMetaAttentionLayer: 3-3         [64, 207, 12, 32]         --
│    │    └─Dropout: 3-4                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 207, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-7                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 207, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-9                   [64, 207, 4096]           274,432
│    │    └─Sequential: 3-10                  [64, 207, 128]            16,512
│    │    └─STMetaAttentionLayer: 3-11        [64, 207, 12, 32]         --
│    │    └─Dropout: 3-12                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-15                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 207, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 207, 12]         156
├─Linear: 1-7                                 [64, 12, 207, 1]          33
===============================================================================================
Total params: 609,085
Trainable params: 609,085
Non-trainable params: 0
Total mult-adds (M): 38.13
===============================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 1432.48
Params size (MB): 2.38
Estimated Total Size (MB): 1436.77
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 11:21:29.843292 Epoch 1  	Train Loss = 5.13674 Val Loss = 3.53018
2023-04-17 11:21:59.977718 Epoch 2  	Train Loss = 3.57665 Val Loss = 3.37355
2023-04-17 11:22:30.320466 Epoch 3  	Train Loss = 3.46472 Val Loss = 3.29484
2023-04-17 11:23:01.190503 Epoch 4  	Train Loss = 3.37522 Val Loss = 3.22809
2023-04-17 11:23:32.724203 Epoch 5  	Train Loss = 3.30381 Val Loss = 3.18863
2023-04-17 11:24:04.537517 Epoch 6  	Train Loss = 3.24966 Val Loss = 3.13370
2023-04-17 11:24:36.594332 Epoch 7  	Train Loss = 3.19122 Val Loss = 3.08701
2023-04-17 11:25:08.737489 Epoch 8  	Train Loss = 3.15211 Val Loss = 3.04926
2023-04-17 11:25:40.932454 Epoch 9  	Train Loss = 3.11505 Val Loss = 3.05737
2023-04-17 11:26:13.126746 Epoch 10  	Train Loss = 3.08501 Val Loss = 3.02231
2023-04-17 11:26:45.367884 Epoch 11  	Train Loss = 3.01261 Val Loss = 2.98284
2023-04-17 11:27:17.554381 Epoch 12  	Train Loss = 2.99508 Val Loss = 2.98091
2023-04-17 11:27:49.644605 Epoch 13  	Train Loss = 2.98597 Val Loss = 2.97215
2023-04-17 11:28:21.659704 Epoch 14  	Train Loss = 2.97537 Val Loss = 2.97439
2023-04-17 11:28:53.620784 Epoch 15  	Train Loss = 2.96446 Val Loss = 2.96819
2023-04-17 11:29:25.603520 Epoch 16  	Train Loss = 2.95182 Val Loss = 2.95706
2023-04-17 11:29:57.578078 Epoch 17  	Train Loss = 2.93797 Val Loss = 2.94322
2023-04-17 11:30:29.607438 Epoch 18  	Train Loss = 2.92199 Val Loss = 2.94013
2023-04-17 11:31:01.643467 Epoch 19  	Train Loss = 2.90754 Val Loss = 2.93220
2023-04-17 11:31:33.787363 Epoch 20  	Train Loss = 2.89283 Val Loss = 2.92639
2023-04-17 11:32:05.923720 Epoch 21  	Train Loss = 2.87935 Val Loss = 2.91315
2023-04-17 11:32:38.057010 Epoch 22  	Train Loss = 2.86760 Val Loss = 2.91035
2023-04-17 11:33:10.243959 Epoch 23  	Train Loss = 2.85706 Val Loss = 2.91236
2023-04-17 11:33:42.455164 Epoch 24  	Train Loss = 2.84784 Val Loss = 2.91060
2023-04-17 11:34:14.717917 Epoch 25  	Train Loss = 2.83875 Val Loss = 2.90824
2023-04-17 11:34:47.004523 Epoch 26  	Train Loss = 2.83000 Val Loss = 2.90471
2023-04-17 11:35:19.330439 Epoch 27  	Train Loss = 2.82230 Val Loss = 2.90715
2023-04-17 11:35:51.670318 Epoch 28  	Train Loss = 2.81471 Val Loss = 2.89762
2023-04-17 11:36:24.038527 Epoch 29  	Train Loss = 2.80675 Val Loss = 2.89926
2023-04-17 11:36:56.492130 Epoch 30  	Train Loss = 2.80062 Val Loss = 2.89341
2023-04-17 11:37:28.908806 Epoch 31  	Train Loss = 2.78624 Val Loss = 2.89012
2023-04-17 11:38:01.323947 Epoch 32  	Train Loss = 2.78394 Val Loss = 2.89278
2023-04-17 11:38:33.719253 Epoch 33  	Train Loss = 2.78272 Val Loss = 2.89355
2023-04-17 11:39:06.158994 Epoch 34  	Train Loss = 2.78179 Val Loss = 2.89730
2023-04-17 11:39:38.605489 Epoch 35  	Train Loss = 2.78089 Val Loss = 2.89653
2023-04-17 11:40:11.002921 Epoch 36  	Train Loss = 2.78004 Val Loss = 2.89593
2023-04-17 11:40:43.442340 Epoch 37  	Train Loss = 2.77886 Val Loss = 2.89184
2023-04-17 11:41:15.843297 Epoch 38  	Train Loss = 2.77744 Val Loss = 2.89728
2023-04-17 11:41:48.169269 Epoch 39  	Train Loss = 2.77797 Val Loss = 2.89483
2023-04-17 11:42:20.405254 Epoch 40  	Train Loss = 2.77722 Val Loss = 2.89239
2023-04-17 11:42:52.496667 Epoch 41  	Train Loss = 2.77639 Val Loss = 2.89201
Early stopping at epoch: 41
Best at epoch 31:
Train Loss = 2.78624
Train RMSE = 5.51261, MAE = 2.77500, MAPE = 7.38364
Val Loss = 2.89012
Val RMSE = 6.09294, MAE = 2.93246, MAPE = 8.32839
--------- Test ---------
All Steps RMSE = 6.58043, MAE = 3.21665, MAPE = 9.07796
Step 1 RMSE = 5.20331, MAE = 2.71781, MAPE = 7.17764
Step 2 RMSE = 5.56329, MAE = 2.83855, MAPE = 7.58409
Step 3 RMSE = 5.86342, MAE = 2.94760, MAPE = 7.98819
Step 4 RMSE = 6.03638, MAE = 3.01343, MAPE = 8.26226
Step 5 RMSE = 6.27837, MAE = 3.10755, MAPE = 8.65207
Step 6 RMSE = 6.54511, MAE = 3.20420, MAPE = 9.03240
Step 7 RMSE = 6.70698, MAE = 3.28845, MAPE = 9.42075
Step 8 RMSE = 6.90249, MAE = 3.35704, MAPE = 9.70922
Step 9 RMSE = 7.09865, MAE = 3.42247, MAPE = 9.89497
Step 10 RMSE = 7.26685, MAE = 3.49738, MAPE = 10.13871
Step 11 RMSE = 7.39553, MAE = 3.55325, MAPE = 10.38964
Step 12 RMSE = 7.61638, MAE = 3.65215, MAPE = 10.68584
Inference time: 2.92 s
