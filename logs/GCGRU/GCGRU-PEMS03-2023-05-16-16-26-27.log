PEMS03
Trainset:	x-(15711, 12, 358, 1)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 1)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 1)	y-(5237, 12, 358, 1)

--------- GCGRU ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        12,
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 358,
        "input_dim": 1,
        "output_dim": 1,
        "horizon": 12,
        "rnn_units": 64,
        "num_layers": 1,
        "cheb_k": 3,
        "adj_path": "../data/PEMS03/adj_PEMS03.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0"
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GCGRU                                    [64, 12, 358, 1]          --
├─Encoder: 1-1                           [64, 358, 64]             --
│    └─ModuleList: 2-1                   --                        --
│    │    └─GRUCell: 3-1                 [64, 358, 64]             75,072
│    │    └─GRUCell: 3-2                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-3                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-4                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-5                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-6                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-7                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-8                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-9                 [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-10                [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-11                [64, 358, 64]             (recursive)
│    │    └─GRUCell: 3-12                [64, 358, 64]             (recursive)
├─Decoder: 1-2                           [64, 358, 64]             --
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-13                [64, 358, 64]             75,072
├─Sequential: 1-3                        [64, 358, 1]              --
│    └─Linear: 2-3                       [64, 358, 1]              65
├─Decoder: 1-4                           [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-14                [64, 358, 64]             (recursive)
├─Sequential: 1-5                        [64, 358, 1]              (recursive)
│    └─Linear: 2-5                       [64, 358, 1]              (recursive)
├─Decoder: 1-6                           [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-15                [64, 358, 64]             (recursive)
├─Sequential: 1-7                        [64, 358, 1]              (recursive)
│    └─Linear: 2-7                       [64, 358, 1]              (recursive)
├─Decoder: 1-8                           [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-16                [64, 358, 64]             (recursive)
├─Sequential: 1-9                        [64, 358, 1]              (recursive)
│    └─Linear: 2-9                       [64, 358, 1]              (recursive)
├─Decoder: 1-10                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-17                [64, 358, 64]             (recursive)
├─Sequential: 1-11                       [64, 358, 1]              (recursive)
│    └─Linear: 2-11                      [64, 358, 1]              (recursive)
├─Decoder: 1-12                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-18                [64, 358, 64]             (recursive)
├─Sequential: 1-13                       [64, 358, 1]              (recursive)
│    └─Linear: 2-13                      [64, 358, 1]              (recursive)
├─Decoder: 1-14                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-19                [64, 358, 64]             (recursive)
├─Sequential: 1-15                       [64, 358, 1]              (recursive)
│    └─Linear: 2-15                      [64, 358, 1]              (recursive)
├─Decoder: 1-16                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-20                [64, 358, 64]             (recursive)
├─Sequential: 1-17                       [64, 358, 1]              (recursive)
│    └─Linear: 2-17                      [64, 358, 1]              (recursive)
├─Decoder: 1-18                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-21                [64, 358, 64]             (recursive)
├─Sequential: 1-19                       [64, 358, 1]              (recursive)
│    └─Linear: 2-19                      [64, 358, 1]              (recursive)
├─Decoder: 1-20                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-22                [64, 358, 64]             (recursive)
├─Sequential: 1-21                       [64, 358, 1]              (recursive)
│    └─Linear: 2-21                      [64, 358, 1]              (recursive)
├─Decoder: 1-22                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-23                [64, 358, 64]             (recursive)
├─Sequential: 1-23                       [64, 358, 1]              (recursive)
│    └─Linear: 2-23                      [64, 358, 1]              (recursive)
├─Decoder: 1-24                          [64, 358, 64]             (recursive)
│    └─ModuleList: 2-24                  --                        (recursive)
│    │    └─GRUCell: 3-24                [64, 358, 64]             (recursive)
├─Sequential: 1-25                       [64, 358, 1]              (recursive)
│    └─Linear: 2-25                      [64, 358, 1]              (recursive)
==========================================================================================
Total params: 150,209
Trainable params: 150,209
Non-trainable params: 0
Total mult-adds (M): 0.05
==========================================================================================
Input size (MB): 1.10
Forward/backward pass size (MB): 846.83
Params size (MB): 0.60
Estimated Total Size (MB): 848.53
==========================================================================================

Loss: HuberLoss

2023-05-16 16:27:15.126473 Epoch 1  	Train Loss = 27.36471 Val Loss = 20.84096
2023-05-16 16:28:00.558665 Epoch 2  	Train Loss = 20.50709 Val Loss = 20.88158
2023-05-16 16:28:46.415130 Epoch 3  	Train Loss = 20.31981 Val Loss = 19.81982
2023-05-16 16:29:32.837365 Epoch 4  	Train Loss = 19.86842 Val Loss = 19.50819
2023-05-16 16:30:19.980338 Epoch 5  	Train Loss = 19.47892 Val Loss = 19.54579
2023-05-16 16:31:07.943301 Epoch 6  	Train Loss = 19.42499 Val Loss = 19.28442
2023-05-16 16:31:56.314656 Epoch 7  	Train Loss = 19.13356 Val Loss = 19.40555
2023-05-16 16:32:44.894822 Epoch 8  	Train Loss = 19.07022 Val Loss = 19.11237
2023-05-16 16:33:33.823703 Epoch 9  	Train Loss = 18.83620 Val Loss = 18.88989
2023-05-16 16:34:22.299807 Epoch 10  	Train Loss = 18.72747 Val Loss = 18.82824
2023-05-16 16:35:10.847037 Epoch 11  	Train Loss = 18.52217 Val Loss = 18.32422
2023-05-16 16:35:59.411786 Epoch 12  	Train Loss = 18.38735 Val Loss = 18.44195
2023-05-16 16:36:47.980517 Epoch 13  	Train Loss = 17.97837 Val Loss = 18.04552
2023-05-16 16:37:36.610191 Epoch 14  	Train Loss = 17.93076 Val Loss = 17.96588
2023-05-16 16:38:25.007969 Epoch 15  	Train Loss = 17.90181 Val Loss = 17.95105
2023-05-16 16:39:13.463298 Epoch 16  	Train Loss = 17.87050 Val Loss = 17.89496
2023-05-16 16:40:01.662300 Epoch 17  	Train Loss = 17.85512 Val Loss = 17.89477
2023-05-16 16:40:49.916104 Epoch 18  	Train Loss = 17.81248 Val Loss = 17.90403
2023-05-16 16:41:37.999128 Epoch 19  	Train Loss = 17.79347 Val Loss = 17.83542
2023-05-16 16:42:25.980788 Epoch 20  	Train Loss = 17.75833 Val Loss = 17.83318
2023-05-16 16:43:14.064794 Epoch 21  	Train Loss = 17.71593 Val Loss = 17.75952
2023-05-16 16:44:02.354497 Epoch 22  	Train Loss = 17.69727 Val Loss = 17.76477
2023-05-16 16:44:50.828428 Epoch 23  	Train Loss = 17.65955 Val Loss = 17.80988
2023-05-16 16:45:39.432415 Epoch 24  	Train Loss = 17.63170 Val Loss = 17.71744
2023-05-16 16:46:27.750923 Epoch 25  	Train Loss = 17.61208 Val Loss = 17.69658
2023-05-16 16:47:16.459550 Epoch 26  	Train Loss = 17.55730 Val Loss = 17.70335
2023-05-16 16:48:05.379555 Epoch 27  	Train Loss = 17.52391 Val Loss = 17.68418
2023-05-16 16:48:54.005889 Epoch 28  	Train Loss = 17.48906 Val Loss = 17.55874
2023-05-16 16:49:42.411223 Epoch 29  	Train Loss = 17.46499 Val Loss = 17.53946
2023-05-16 16:50:30.961287 Epoch 30  	Train Loss = 17.43085 Val Loss = 17.55286
2023-05-16 16:51:19.578034 Epoch 31  	Train Loss = 17.41119 Val Loss = 17.51473
2023-05-16 16:52:07.981856 Epoch 32  	Train Loss = 17.37594 Val Loss = 17.54849
2023-05-16 16:52:56.276086 Epoch 33  	Train Loss = 17.36008 Val Loss = 17.44856
2023-05-16 16:53:44.742163 Epoch 34  	Train Loss = 17.31238 Val Loss = 17.47694
2023-05-16 16:54:32.916374 Epoch 35  	Train Loss = 17.30841 Val Loss = 17.47609
2023-05-16 16:55:21.184380 Epoch 36  	Train Loss = 17.27399 Val Loss = 17.39452
2023-05-16 16:56:09.828156 Epoch 37  	Train Loss = 17.24632 Val Loss = 17.36225
2023-05-16 16:56:58.296702 Epoch 38  	Train Loss = 17.21495 Val Loss = 17.37008
2023-05-16 16:57:46.589429 Epoch 39  	Train Loss = 17.19071 Val Loss = 17.34518
2023-05-16 16:58:35.269300 Epoch 40  	Train Loss = 17.16087 Val Loss = 17.30924
2023-05-16 16:59:23.835711 Epoch 41  	Train Loss = 17.14584 Val Loss = 17.30446
2023-05-16 17:00:12.224405 Epoch 42  	Train Loss = 17.13258 Val Loss = 17.25654
2023-05-16 17:01:00.907415 Epoch 43  	Train Loss = 17.09071 Val Loss = 17.25919
2023-05-16 17:01:49.609009 Epoch 44  	Train Loss = 17.08967 Val Loss = 17.25951
2023-05-16 17:02:38.185893 Epoch 45  	Train Loss = 17.04940 Val Loss = 17.21682
2023-05-16 17:03:26.492297 Epoch 46  	Train Loss = 17.02831 Val Loss = 17.26547
2023-05-16 17:04:14.964887 Epoch 47  	Train Loss = 17.01237 Val Loss = 17.20623
2023-05-16 17:05:03.509632 Epoch 48  	Train Loss = 16.98882 Val Loss = 17.19546
2023-05-16 17:05:52.024066 Epoch 49  	Train Loss = 16.96373 Val Loss = 17.19246
2023-05-16 17:06:40.600067 Epoch 50  	Train Loss = 16.96776 Val Loss = 17.14050
2023-05-16 17:07:28.961907 Epoch 51  	Train Loss = 16.85274 Val Loss = 17.07616
2023-05-16 17:08:16.768795 Epoch 52  	Train Loss = 16.84642 Val Loss = 17.06161
2023-05-16 17:09:05.105825 Epoch 53  	Train Loss = 16.84265 Val Loss = 17.06731
2023-05-16 17:09:53.512199 Epoch 54  	Train Loss = 16.83742 Val Loss = 17.06172
2023-05-16 17:10:42.655142 Epoch 55  	Train Loss = 16.83734 Val Loss = 17.06228
2023-05-16 17:11:31.417319 Epoch 56  	Train Loss = 16.83545 Val Loss = 17.08337
2023-05-16 17:12:19.768871 Epoch 57  	Train Loss = 16.83085 Val Loss = 17.05578
2023-05-16 17:13:07.860026 Epoch 58  	Train Loss = 16.82420 Val Loss = 17.04782
2023-05-16 17:13:56.229534 Epoch 59  	Train Loss = 16.82751 Val Loss = 17.04250
2023-05-16 17:14:44.825019 Epoch 60  	Train Loss = 16.82687 Val Loss = 17.05995
2023-05-16 17:15:33.636061 Epoch 61  	Train Loss = 16.82121 Val Loss = 17.06895
2023-05-16 17:16:22.069498 Epoch 62  	Train Loss = 16.82360 Val Loss = 17.03675
2023-05-16 17:17:10.430322 Epoch 63  	Train Loss = 16.81511 Val Loss = 17.05305
2023-05-16 17:17:59.115304 Epoch 64  	Train Loss = 16.81349 Val Loss = 17.04085
2023-05-16 17:18:50.420168 Epoch 65  	Train Loss = 16.80969 Val Loss = 17.03891
2023-05-16 17:19:38.820100 Epoch 66  	Train Loss = 16.80421 Val Loss = 17.04666
2023-05-16 17:20:27.478868 Epoch 67  	Train Loss = 16.80180 Val Loss = 17.04155
2023-05-16 17:21:15.984033 Epoch 68  	Train Loss = 16.80295 Val Loss = 17.03592
2023-05-16 17:22:04.459303 Epoch 69  	Train Loss = 16.80209 Val Loss = 17.02463
2023-05-16 17:22:52.811274 Epoch 70  	Train Loss = 16.79996 Val Loss = 17.05023
2023-05-16 17:23:40.943539 Epoch 71  	Train Loss = 16.79449 Val Loss = 17.03961
2023-05-16 17:24:29.341494 Epoch 72  	Train Loss = 16.79121 Val Loss = 17.02383
2023-05-16 17:25:17.979722 Epoch 73  	Train Loss = 16.78866 Val Loss = 17.02747
2023-05-16 17:26:06.818271 Epoch 74  	Train Loss = 16.78883 Val Loss = 17.02850
2023-05-16 17:26:55.708092 Epoch 75  	Train Loss = 16.78441 Val Loss = 17.01037
2023-05-16 17:27:44.351034 Epoch 76  	Train Loss = 16.78621 Val Loss = 17.02656
2023-05-16 17:28:33.065142 Epoch 77  	Train Loss = 16.78121 Val Loss = 17.01580
2023-05-16 17:29:21.884261 Epoch 78  	Train Loss = 16.77540 Val Loss = 17.01755
2023-05-16 17:30:10.544082 Epoch 79  	Train Loss = 16.77517 Val Loss = 17.02153
2023-05-16 17:30:59.284174 Epoch 80  	Train Loss = 16.77186 Val Loss = 17.00159
2023-05-16 17:31:48.251211 Epoch 81  	Train Loss = 16.76613 Val Loss = 17.00674
2023-05-16 17:32:37.959249 Epoch 82  	Train Loss = 16.76158 Val Loss = 17.00591
2023-05-16 17:33:27.559820 Epoch 83  	Train Loss = 16.76046 Val Loss = 17.00645
2023-05-16 17:34:16.585165 Epoch 84  	Train Loss = 16.76312 Val Loss = 17.01767
2023-05-16 17:35:05.654451 Epoch 85  	Train Loss = 16.76129 Val Loss = 16.99594
2023-05-16 17:35:55.084917 Epoch 86  	Train Loss = 16.75483 Val Loss = 17.00448
2023-05-16 17:36:44.051281 Epoch 87  	Train Loss = 16.75355 Val Loss = 16.99064
2023-05-16 17:37:32.875782 Epoch 88  	Train Loss = 16.75358 Val Loss = 16.99354
2023-05-16 17:38:21.612669 Epoch 89  	Train Loss = 16.74566 Val Loss = 16.97115
2023-05-16 17:39:10.187445 Epoch 90  	Train Loss = 16.74846 Val Loss = 17.00340
2023-05-16 17:39:58.550613 Epoch 91  	Train Loss = 16.74921 Val Loss = 16.99986
2023-05-16 17:40:46.547400 Epoch 92  	Train Loss = 16.73821 Val Loss = 16.98516
2023-05-16 17:41:34.380372 Epoch 93  	Train Loss = 16.74070 Val Loss = 16.97640
2023-05-16 17:42:22.186089 Epoch 94  	Train Loss = 16.73867 Val Loss = 16.99054
2023-05-16 17:43:09.926477 Epoch 95  	Train Loss = 16.73560 Val Loss = 16.96708
2023-05-16 17:43:58.081553 Epoch 96  	Train Loss = 16.73423 Val Loss = 16.98681
2023-05-16 17:44:46.341455 Epoch 97  	Train Loss = 16.73079 Val Loss = 16.97998
2023-05-16 17:45:34.472944 Epoch 98  	Train Loss = 16.72669 Val Loss = 16.99155
2023-05-16 17:46:23.117465 Epoch 99  	Train Loss = 16.72436 Val Loss = 16.98476
2023-05-16 17:47:12.321638 Epoch 100  	Train Loss = 16.72773 Val Loss = 16.97352
2023-05-16 17:48:01.776449 Epoch 101  	Train Loss = 16.71875 Val Loss = 16.95534
2023-05-16 17:48:51.618818 Epoch 102  	Train Loss = 16.71718 Val Loss = 16.96852
2023-05-16 17:49:41.689181 Epoch 103  	Train Loss = 16.71933 Val Loss = 16.96730
2023-05-16 17:50:30.967273 Epoch 104  	Train Loss = 16.72115 Val Loss = 16.95694
2023-05-16 17:51:19.550149 Epoch 105  	Train Loss = 16.70980 Val Loss = 16.98431
2023-05-16 17:52:08.126647 Epoch 106  	Train Loss = 16.71288 Val Loss = 16.96552
2023-05-16 17:52:56.585366 Epoch 107  	Train Loss = 16.70747 Val Loss = 16.95000
2023-05-16 17:53:45.021292 Epoch 108  	Train Loss = 16.70492 Val Loss = 16.95030
2023-05-16 17:54:33.701648 Epoch 109  	Train Loss = 16.70595 Val Loss = 16.97724
2023-05-16 17:55:22.369018 Epoch 110  	Train Loss = 16.69888 Val Loss = 16.94214
2023-05-16 17:56:10.976786 Epoch 111  	Train Loss = 16.69544 Val Loss = 16.97719
2023-05-16 17:56:59.330616 Epoch 112  	Train Loss = 16.69596 Val Loss = 16.96311
2023-05-16 17:57:47.499711 Epoch 113  	Train Loss = 16.69863 Val Loss = 16.95292
2023-05-16 17:58:35.683649 Epoch 114  	Train Loss = 16.69332 Val Loss = 16.95612
2023-05-16 17:59:23.571454 Epoch 115  	Train Loss = 16.69156 Val Loss = 16.95154
2023-05-16 18:00:11.260539 Epoch 116  	Train Loss = 16.69033 Val Loss = 16.93136
2023-05-16 18:00:58.647243 Epoch 117  	Train Loss = 16.68767 Val Loss = 16.94902
2023-05-16 18:01:45.903231 Epoch 118  	Train Loss = 16.67859 Val Loss = 16.95274
2023-05-16 18:02:32.976369 Epoch 119  	Train Loss = 16.67991 Val Loss = 16.93489
2023-05-16 18:03:20.251905 Epoch 120  	Train Loss = 16.67863 Val Loss = 16.94127
2023-05-16 18:04:07.899261 Epoch 121  	Train Loss = 16.67904 Val Loss = 16.92704
2023-05-16 18:04:55.876896 Epoch 122  	Train Loss = 16.67783 Val Loss = 16.95607
2023-05-16 18:05:44.083163 Epoch 123  	Train Loss = 16.67437 Val Loss = 16.96382
2023-05-16 18:06:32.555804 Epoch 124  	Train Loss = 16.67675 Val Loss = 16.94308
2023-05-16 18:07:21.033166 Epoch 125  	Train Loss = 16.66719 Val Loss = 16.91899
2023-05-16 18:08:09.640614 Epoch 126  	Train Loss = 16.67065 Val Loss = 16.93683
2023-05-16 18:08:58.643257 Epoch 127  	Train Loss = 16.66320 Val Loss = 16.95579
2023-05-16 18:09:47.657410 Epoch 128  	Train Loss = 16.66043 Val Loss = 16.91453
2023-05-16 18:10:36.200080 Epoch 129  	Train Loss = 16.65680 Val Loss = 16.91908
2023-05-16 18:11:24.981809 Epoch 130  	Train Loss = 16.66229 Val Loss = 16.91272
2023-05-16 18:12:13.732379 Epoch 131  	Train Loss = 16.65813 Val Loss = 16.92912
2023-05-16 18:13:01.699882 Epoch 132  	Train Loss = 16.65583 Val Loss = 16.89266
2023-05-16 18:13:49.264902 Epoch 133  	Train Loss = 16.65119 Val Loss = 16.89510
2023-05-16 18:14:36.908170 Epoch 134  	Train Loss = 16.65150 Val Loss = 16.91673
2023-05-16 18:15:24.601383 Epoch 135  	Train Loss = 16.64785 Val Loss = 16.93670
2023-05-16 18:16:12.416998 Epoch 136  	Train Loss = 16.64607 Val Loss = 16.92396
2023-05-16 18:17:00.365734 Epoch 137  	Train Loss = 16.64138 Val Loss = 16.91075
2023-05-16 18:17:48.653695 Epoch 138  	Train Loss = 16.64381 Val Loss = 16.91653
2023-05-16 18:18:37.321873 Epoch 139  	Train Loss = 16.64030 Val Loss = 16.90480
2023-05-16 18:19:26.090252 Epoch 140  	Train Loss = 16.63919 Val Loss = 16.93674
2023-05-16 18:20:15.049010 Epoch 141  	Train Loss = 16.63427 Val Loss = 16.90680
2023-05-16 18:21:04.051337 Epoch 142  	Train Loss = 16.63310 Val Loss = 16.90776
Early stopping at epoch: 142
Best at epoch 132:
Train Loss = 16.65583
Train RMSE = 26.67550, MAE = 17.17349, MAPE = 16.60686
Val Loss = 16.89266
Val RMSE = 26.99823, MAE = 17.45542, MAPE = 16.61764
--------- Test ---------
All Steps RMSE = 28.50430, MAE = 17.41122, MAPE = 17.12072
Step 1 RMSE = 21.30835, MAE = 13.11754, MAPE = 13.22916
Step 2 RMSE = 23.57095, MAE = 14.39006, MAPE = 14.17317
Step 3 RMSE = 25.08751, MAE = 15.28706, MAPE = 15.02207
Step 4 RMSE = 26.18801, MAE = 15.94320, MAPE = 15.59465
Step 5 RMSE = 27.13253, MAE = 16.52796, MAPE = 16.17052
Step 6 RMSE = 28.09241, MAE = 17.15540, MAPE = 16.78065
Step 7 RMSE = 29.08397, MAE = 17.82961, MAPE = 17.39932
Step 8 RMSE = 29.98804, MAE = 18.46843, MAPE = 18.02324
Step 9 RMSE = 30.79927, MAE = 19.05126, MAPE = 18.63845
Step 10 RMSE = 31.60766, MAE = 19.64051, MAPE = 19.29216
Step 11 RMSE = 32.57981, MAE = 20.32708, MAPE = 20.05944
Step 12 RMSE = 33.84541, MAE = 21.19635, MAPE = 21.06568
Inference time: 6.27 s
