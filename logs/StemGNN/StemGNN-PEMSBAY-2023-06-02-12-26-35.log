PEMSBAY
Trainset:	x-(36465, 12, 325, 1)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 1)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 1)	y-(10419, 12, 325, 1)

--------- StemGNN ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "lr": 0.0002,
    "weight_decay": 0,
    "milestones": [
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "units": 325,
        "stack_cnt": 2,
        "time_step": 12,
        "horizon": 12,
        "multi_layer": 5,
        "dropout_rate": 0.5,
        "leaky_rate": 0.2
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
StemGNN                                  [64, 12, 325, 1]          650
├─GRU: 1-1                               [325, 64, 325]            330,525
├─LeakyReLU: 1-2                         [64, 325, 325]            --
├─Dropout: 1-3                           [64, 325, 325]            --
├─ModuleList: 1-4                        --                        --
│    └─StockBlockLayer: 2-1              [64, 325, 12]             14,400
│    │    └─ModuleList: 3-1              --                        509,760
│    │    └─Linear: 3-2                  [64, 1, 325, 60]          3,660
│    │    └─Linear: 3-3                  [64, 325, 12]             732
│    │    └─Linear: 3-4                  [64, 1, 1, 325, 12]       156
│    │    └─Linear: 3-5                  [64, 1, 325, 12]          732
│    └─StockBlockLayer: 2-2              [64, 325, 12]             14,556
│    │    └─ModuleList: 3-6              --                        509,760
│    │    └─Linear: 3-7                  [64, 1, 325, 60]          3,660
│    │    └─Linear: 3-8                  [64, 325, 12]             732
├─Sequential: 1-5                        [64, 325, 12]             --
│    └─Linear: 2-3                       [64, 325, 12]             156
│    └─LeakyReLU: 2-4                    [64, 325, 12]             --
│    └─Linear: 2-5                       [64, 325, 12]             156
==========================================================================================
Total params: 1,389,635
Trainable params: 1,389,635
Non-trainable params: 0
Total mult-adds (G): 6.94
==========================================================================================
Input size (MB): 1.00
Forward/backward pass size (MB): 1044.49
Params size (MB): 5.44
Estimated Total Size (MB): 1050.93
==========================================================================================

Loss: MaskedMAELoss

2023-06-02 12:27:04.023413 Epoch 1  	Train Loss = 3.59750 Val Loss = 2.83402
2023-06-02 12:27:27.496711 Epoch 2  	Train Loss = 2.04039 Val Loss = 2.92263
2023-06-02 12:27:50.797751 Epoch 3  	Train Loss = 1.96498 Val Loss = 2.70365
2023-06-02 12:28:14.466302 Epoch 4  	Train Loss = 1.93697 Val Loss = 2.52528
2023-06-02 12:28:37.973323 Epoch 5  	Train Loss = 1.91051 Val Loss = 2.70715
2023-06-02 12:29:01.572940 Epoch 6  	Train Loss = 1.88414 Val Loss = 2.52130
2023-06-02 12:29:25.102226 Epoch 7  	Train Loss = 1.86164 Val Loss = 2.44037
2023-06-02 12:29:48.509894 Epoch 8  	Train Loss = 1.84631 Val Loss = 2.46262
2023-06-02 12:30:11.897005 Epoch 9  	Train Loss = 1.83076 Val Loss = 2.34169
2023-06-02 12:30:35.202514 Epoch 10  	Train Loss = 1.82098 Val Loss = 2.38676
2023-06-02 12:30:58.562215 Epoch 11  	Train Loss = 1.81008 Val Loss = 2.37860
2023-06-02 12:31:22.016436 Epoch 12  	Train Loss = 1.80487 Val Loss = 2.38690
2023-06-02 12:31:45.576960 Epoch 13  	Train Loss = 1.79559 Val Loss = 2.29254
2023-06-02 12:32:09.477526 Epoch 14  	Train Loss = 1.78800 Val Loss = 2.33523
2023-06-02 12:32:33.187313 Epoch 15  	Train Loss = 1.78063 Val Loss = 2.25188
2023-06-02 12:32:56.842789 Epoch 16  	Train Loss = 1.77575 Val Loss = 2.31330
2023-06-02 12:33:20.383407 Epoch 17  	Train Loss = 1.77295 Val Loss = 2.26763
2023-06-02 12:33:43.945866 Epoch 18  	Train Loss = 1.76685 Val Loss = 2.25907
2023-06-02 12:34:07.695093 Epoch 19  	Train Loss = 1.76317 Val Loss = 2.18308
2023-06-02 12:34:31.204611 Epoch 20  	Train Loss = 1.75795 Val Loss = 2.21240
2023-06-02 12:34:54.721247 Epoch 21  	Train Loss = 1.75270 Val Loss = 2.27114
2023-06-02 12:35:18.087636 Epoch 22  	Train Loss = 1.74978 Val Loss = 2.20721
2023-06-02 12:35:41.458938 Epoch 23  	Train Loss = 1.74636 Val Loss = 2.19095
2023-06-02 12:36:05.162539 Epoch 24  	Train Loss = 1.74224 Val Loss = 2.22088
2023-06-02 12:36:29.086682 Epoch 25  	Train Loss = 1.73735 Val Loss = 2.13913
2023-06-02 12:36:52.578769 Epoch 26  	Train Loss = 1.73535 Val Loss = 2.20415
2023-06-02 12:37:16.046221 Epoch 27  	Train Loss = 1.73222 Val Loss = 2.23381
2023-06-02 12:37:39.502163 Epoch 28  	Train Loss = 1.72833 Val Loss = 2.19327
2023-06-02 12:38:03.023606 Epoch 29  	Train Loss = 1.72684 Val Loss = 2.15878
2023-06-02 12:38:26.658595 Epoch 30  	Train Loss = 1.72243 Val Loss = 2.12255
2023-06-02 12:38:49.986796 Epoch 31  	Train Loss = 1.71906 Val Loss = 2.22378
2023-06-02 12:39:13.460117 Epoch 32  	Train Loss = 1.71531 Val Loss = 2.21237
2023-06-02 12:39:36.995326 Epoch 33  	Train Loss = 1.71325 Val Loss = 2.15185
2023-06-02 12:40:00.402928 Epoch 34  	Train Loss = 1.71139 Val Loss = 2.17321
2023-06-02 12:40:23.956164 Epoch 35  	Train Loss = 1.71307 Val Loss = 2.13428
2023-06-02 12:40:47.464979 Epoch 36  	Train Loss = 1.70651 Val Loss = 2.14918
2023-06-02 12:41:10.963888 Epoch 37  	Train Loss = 1.70132 Val Loss = 2.15799
2023-06-02 12:41:34.391411 Epoch 38  	Train Loss = 1.69957 Val Loss = 2.22865
2023-06-02 12:41:57.801580 Epoch 39  	Train Loss = 1.69637 Val Loss = 2.14375
2023-06-02 12:42:21.361863 Epoch 40  	Train Loss = 1.69598 Val Loss = 2.17731
Early stopping at epoch: 40
Best at epoch 30:
Train Loss = 1.72243
Train RMSE = 3.73986, MAE = 1.68380, MAPE = 3.72302
Val Loss = 2.12255
Val RMSE = 4.83963, MAE = 2.14938, MAPE = 5.29895
--------- Test ---------
All Steps RMSE = 4.67401, MAE = 2.05665, MAPE = 4.95328
Step 1 RMSE = 2.01838, MAE = 1.09746, MAPE = 2.26916
Step 2 RMSE = 2.65760, MAE = 1.36210, MAPE = 2.89526
Step 3 RMSE = 3.24483, MAE = 1.57075, MAPE = 3.44653
Step 4 RMSE = 3.73416, MAE = 1.77368, MAPE = 4.03206
Step 5 RMSE = 4.16909, MAE = 1.92072, MAPE = 4.49786
Step 6 RMSE = 4.57428, MAE = 2.06830, MAPE = 4.97754
Step 7 RMSE = 4.92844, MAE = 2.21305, MAPE = 5.43505
Step 8 RMSE = 5.22821, MAE = 2.32926, MAPE = 5.78561
Step 9 RMSE = 5.51595, MAE = 2.44853, MAPE = 6.15084
Step 10 RMSE = 5.76201, MAE = 2.54686, MAPE = 6.42862
Step 11 RMSE = 5.93011, MAE = 2.62462, MAPE = 6.61252
Step 12 RMSE = 6.15022, MAE = 2.72449, MAPE = 6.90838
Inference time: 2.39 s
