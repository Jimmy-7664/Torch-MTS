PEMS03
Trainset:	x-(15711, 12, 358, 1)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 1)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 1)	y-(5237, 12, 358, 1)

--------- StemGNN ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        50
    ],
    "clip_grad": 0,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "units": 358,
        "stack_cnt": 2,
        "time_step": 12,
        "horizon": 12,
        "multi_layer": 5,
        "dropout_rate": 0.5,
        "leaky_rate": 0.2
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
StemGNN                                  [64, 12, 358, 1]          716
├─GRU: 1-1                               [358, 64, 358]            399,528
├─LeakyReLU: 1-2                         [64, 358, 358]            --
├─Dropout: 1-3                           [64, 358, 358]            --
├─ModuleList: 1-4                        --                        --
│    └─StockBlockLayer: 2-1              [64, 358, 12]             14,400
│    │    └─ModuleList: 3-1              --                        509,760
│    │    └─Linear: 3-2                  [64, 1, 358, 60]          3,660
│    │    └─Linear: 3-3                  [64, 358, 12]             732
│    │    └─Linear: 3-4                  [64, 1, 1, 358, 12]       156
│    │    └─Linear: 3-5                  [64, 1, 358, 12]          732
│    └─StockBlockLayer: 2-2              [64, 358, 12]             14,556
│    │    └─ModuleList: 3-6              --                        509,760
│    │    └─Linear: 3-7                  [64, 1, 358, 60]          3,660
│    │    └─Linear: 3-8                  [64, 358, 12]             732
├─Sequential: 1-5                        [64, 358, 12]             --
│    └─Linear: 2-3                       [64, 358, 12]             156
│    └─LeakyReLU: 2-4                    [64, 358, 12]             --
│    └─Linear: 2-5                       [64, 358, 12]             156
==========================================================================================
Total params: 1,458,704
Trainable params: 1,458,704
Non-trainable params: 0
Total mult-adds (G): 9.22
==========================================================================================
Input size (MB): 1.10
Forward/backward pass size (MB): 1156.60
Params size (MB): 5.72
Estimated Total Size (MB): 1163.41
==========================================================================================

Loss: HuberLoss

2023-06-02 17:15:14.877852 Epoch 1  	Train Loss = 42.18394 Val Loss = 33.84697
2023-06-02 17:15:26.876050 Epoch 2  	Train Loss = 20.44384 Val Loss = 24.14418
2023-06-02 17:15:38.879561 Epoch 3  	Train Loss = 19.13685 Val Loss = 22.14122
2023-06-02 17:15:50.816166 Epoch 4  	Train Loss = 18.42930 Val Loss = 20.48739
2023-06-02 17:16:02.715434 Epoch 5  	Train Loss = 18.03476 Val Loss = 19.08513
2023-06-02 17:16:14.739066 Epoch 6  	Train Loss = 17.63512 Val Loss = 18.93480
2023-06-02 17:16:26.641727 Epoch 7  	Train Loss = 17.58237 Val Loss = 19.39757
2023-06-02 17:16:38.547307 Epoch 8  	Train Loss = 17.22715 Val Loss = 18.34361
2023-06-02 17:16:50.461130 Epoch 9  	Train Loss = 17.14327 Val Loss = 18.41167
2023-06-02 17:17:02.393035 Epoch 10  	Train Loss = 16.95718 Val Loss = 17.88535
2023-06-02 17:17:14.484470 Epoch 11  	Train Loss = 16.86511 Val Loss = 18.45490
2023-06-02 17:17:26.448712 Epoch 12  	Train Loss = 16.75235 Val Loss = 18.07803
2023-06-02 17:17:38.428691 Epoch 13  	Train Loss = 16.74028 Val Loss = 18.39739
2023-06-02 17:17:50.363867 Epoch 14  	Train Loss = 16.53695 Val Loss = 18.06031
2023-06-02 17:18:02.309166 Epoch 15  	Train Loss = 16.42093 Val Loss = 17.96667
2023-06-02 17:18:14.353526 Epoch 16  	Train Loss = 16.33962 Val Loss = 17.71637
2023-06-02 17:18:26.325925 Epoch 17  	Train Loss = 16.35013 Val Loss = 17.75208
2023-06-02 17:18:38.268603 Epoch 18  	Train Loss = 16.25778 Val Loss = 17.38571
2023-06-02 17:18:50.200395 Epoch 19  	Train Loss = 16.11525 Val Loss = 17.58667
2023-06-02 17:19:02.200775 Epoch 20  	Train Loss = 16.03297 Val Loss = 17.18802
2023-06-02 17:19:14.194553 Epoch 21  	Train Loss = 15.98241 Val Loss = 17.35344
2023-06-02 17:19:26.175454 Epoch 22  	Train Loss = 15.93674 Val Loss = 17.56690
2023-06-02 17:19:38.119254 Epoch 23  	Train Loss = 15.89785 Val Loss = 17.07755
2023-06-02 17:19:50.150290 Epoch 24  	Train Loss = 15.75553 Val Loss = 17.18962
2023-06-02 17:20:02.334570 Epoch 25  	Train Loss = 15.84553 Val Loss = 17.26856
2023-06-02 17:20:14.637382 Epoch 26  	Train Loss = 15.64358 Val Loss = 16.90597
2023-06-02 17:20:26.681878 Epoch 27  	Train Loss = 15.56609 Val Loss = 17.58313
2023-06-02 17:20:38.637691 Epoch 28  	Train Loss = 15.57828 Val Loss = 17.23759
2023-06-02 17:20:50.650707 Epoch 29  	Train Loss = 15.47379 Val Loss = 16.98732
2023-06-02 17:21:02.625448 Epoch 30  	Train Loss = 15.45463 Val Loss = 17.21631
2023-06-02 17:21:14.653533 Epoch 31  	Train Loss = 15.45972 Val Loss = 17.25475
2023-06-02 17:21:26.737869 Epoch 32  	Train Loss = 15.34732 Val Loss = 16.95845
2023-06-02 17:21:38.703904 Epoch 33  	Train Loss = 15.42127 Val Loss = 17.04705
2023-06-02 17:21:50.664496 Epoch 34  	Train Loss = 15.32730 Val Loss = 16.90601
2023-06-02 17:22:02.748672 Epoch 35  	Train Loss = 15.19691 Val Loss = 16.99755
2023-06-02 17:22:14.760167 Epoch 36  	Train Loss = 15.29790 Val Loss = 17.00957
Early stopping at epoch: 36
Best at epoch 26:
Train Loss = 15.64358
Train RMSE = 24.32968, MAE = 15.49993, MAPE = 14.93106
Val Loss = 16.90597
Val RMSE = 27.15430, MAE = 17.54078, MAPE = 18.68178
--------- Test ---------
All Steps RMSE = 28.60831, MAE = 17.44030, MAPE = 19.73715
Step 1 RMSE = 22.63349, MAE = 13.65058, MAPE = 15.58941
Step 2 RMSE = 24.04873, MAE = 14.48549, MAPE = 16.51051
Step 3 RMSE = 25.37786, MAE = 15.32890, MAPE = 17.35105
Step 4 RMSE = 26.47876, MAE = 16.03082, MAPE = 17.73703
Step 5 RMSE = 27.52377, MAE = 16.75469, MAPE = 19.09738
Step 6 RMSE = 28.43597, MAE = 17.37509, MAPE = 19.42506
Step 7 RMSE = 29.16757, MAE = 17.89253, MAPE = 20.28318
Step 8 RMSE = 29.93967, MAE = 18.41905, MAPE = 20.80599
Step 9 RMSE = 30.89415, MAE = 19.08630, MAPE = 20.57323
Step 10 RMSE = 31.50182, MAE = 19.55592, MAPE = 22.28288
Step 11 RMSE = 32.12816, MAE = 20.03555, MAPE = 23.72041
Step 12 RMSE = 33.06245, MAE = 20.66878, MAPE = 23.46958
Inference time: 1.36 s
