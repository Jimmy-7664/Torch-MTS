PEMS07
Trainset:	x-(16921, 12, 883, 1)	y-(16921, 12, 883, 1)
Valset:  	x-(5640, 12, 883, 1)  	y-(5640, 12, 883, 1)
Testset:	x-(5640, 12, 883, 1)	y-(5640, 12, 883, 1)

--------- STWA ---------
{
    "in_steps": 12,
    "out_steps": 12,
    "lr": 0.001,
    "milestones": [],
    "clip_grad": false,
    "batch_size": 16,
    "max_epochs": 200,
    "early_stop": 15,
    "pass_device": true,
    "model_args": {
        "num_nodes": 883,
        "input_dim": 1,
        "output_dim": 1,
        "lag": 12,
        "horizon": 12,
        "device": "cuda:0",
        "channels": 16,
        "memory_size": 16,
        "dynamic": true
    }
}
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
STWA                                               [16, 12, 883, 1]          --
├─Sequential: 1-1                                  [16, 883, 16]             --
│    └─Linear: 2-1                                 [16, 883, 32]             416
│    └─Tanh: 2-2                                   [16, 883, 32]             --
│    └─Linear: 2-3                                 [16, 883, 32]             1,056
│    └─Tanh: 2-4                                   [16, 883, 32]             --
│    └─Linear: 2-5                                 [16, 883, 16]             528
├─Sequential: 1-2                                  [16, 883, 16]             --
│    └─Linear: 2-6                                 [16, 883, 32]             416
│    └─Tanh: 2-7                                   [16, 883, 32]             --
│    └─Linear: 2-8                                 [16, 883, 32]             1,056
│    └─Tanh: 2-9                                   [16, 883, 32]             --
│    └─Linear: 2-10                                [16, 883, 16]             528
├─Linear: 1-3                                      [16, 12, 883, 16]         32
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-11                                 [16, 12, 883, 16]         367,328
│    │    └─ModuleList: 3-1                        --                        6,100
│    │    └─ModuleList: 3-2                        --                        6,100
│    │    └─TemporalAttention: 3-3                 [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-4                  [16, 2, 883, 16]          544
│    │    └─Sequential: 3-5                        [16, 2, 883, 16]          544
│    │    └─TemporalAttention: 3-6                 [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-7                  [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-8                        [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-9                 [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-10                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-11                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-12                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-13                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-14                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-15                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-16                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-17                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-18                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-19                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-20                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-21                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-22                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-23                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-24                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-25                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-26                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-27                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-28                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-29                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-30                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-31                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-32                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-33                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-34                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-35                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-36                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-37                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-38                       [16, 2, 883, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-12                                [16, 883, 256]            49,408
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-13                                 [16, 3, 883, 16]          113,024
│    │    └─ModuleList: 3-39                       --                        6,100
│    │    └─ModuleList: 3-40                       --                        6,100
│    │    └─TemporalAttention: 3-41                [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-42                 [16, 2, 883, 16]          544
│    │    └─Sequential: 3-43                       [16, 2, 883, 16]          544
│    │    └─TemporalAttention: 3-44                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-45                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-46                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-47                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-48                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-49                       [16, 2, 883, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-14                                [16, 883, 256]            12,544
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-15                                 [16, 1, 883, 16]          56,512
│    │    └─ModuleList: 3-50                       --                        6,100
│    │    └─ModuleList: 3-51                       --                        6,100
│    │    └─TemporalAttention: 3-52                [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-53                 [16, 2, 883, 16]          544
│    │    └─Sequential: 3-54                       [16, 2, 883, 16]          544
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-16                                [16, 883, 256]            4,352
├─Sequential: 1-10                                 [16, 883, 12]             --
│    └─Linear: 2-17                                [16, 883, 512]            131,584
│    └─ReLU: 2-18                                  [16, 883, 512]            --
│    └─Linear: 2-19                                [16, 883, 12]             6,156
====================================================================================================
Total params: 786,436
Trainable params: 786,436
Non-trainable params: 0
Total mult-adds (M): 4.33
====================================================================================================
Input size (MB): 0.68
Forward/backward pass size (MB): 720.19
Params size (MB): 1.00
Estimated Total Size (MB): 721.87
====================================================================================================

Loss: HuberLoss

2023-12-09 10:58:44.733300 Epoch 1  	Train Loss = 38.85354 Val Loss = 32.13175
2023-12-09 11:09:25.522265 Epoch 2  	Train Loss = 27.29339 Val Loss = 25.68590
2023-12-09 11:20:06.720990 Epoch 3  	Train Loss = 24.92395 Val Loss = 24.29040
2023-12-09 11:30:47.783948 Epoch 4  	Train Loss = 23.75337 Val Loss = 23.26541
2023-12-09 11:41:29.506431 Epoch 5  	Train Loss = 23.11745 Val Loss = 23.36861
2023-12-09 11:52:09.736553 Epoch 6  	Train Loss = 22.66979 Val Loss = 24.02194
2023-12-09 12:02:49.893135 Epoch 7  	Train Loss = 22.31797 Val Loss = 22.49233
2023-12-09 12:13:30.301817 Epoch 8  	Train Loss = 22.01662 Val Loss = 22.72137
2023-12-09 12:24:10.083407 Epoch 9  	Train Loss = 21.77678 Val Loss = 22.18482
2023-12-09 12:34:49.716501 Epoch 10  	Train Loss = 21.45803 Val Loss = 21.78256
2023-12-09 12:45:29.338554 Epoch 11  	Train Loss = 21.29955 Val Loss = 22.16001
2023-12-09 12:56:09.112298 Epoch 12  	Train Loss = 21.09856 Val Loss = 21.73856
2023-12-09 13:06:49.770026 Epoch 13  	Train Loss = 20.99160 Val Loss = 21.25659
2023-12-09 13:17:29.939987 Epoch 14  	Train Loss = 20.80953 Val Loss = 21.70087
2023-12-09 13:28:09.263272 Epoch 15  	Train Loss = 20.70267 Val Loss = 21.41525
2023-12-09 13:38:49.103683 Epoch 16  	Train Loss = 20.58674 Val Loss = 21.50327
2023-12-09 13:49:29.550984 Epoch 17  	Train Loss = 20.45738 Val Loss = 21.00425
2023-12-09 14:00:09.196485 Epoch 18  	Train Loss = 20.39423 Val Loss = 21.11622
2023-12-09 14:10:49.112242 Epoch 19  	Train Loss = 20.26927 Val Loss = 20.99611
2023-12-09 14:21:30.006558 Epoch 20  	Train Loss = 20.18810 Val Loss = 21.23310
2023-12-09 14:32:10.250576 Epoch 21  	Train Loss = 20.06781 Val Loss = 21.20617
2023-12-09 14:42:51.355262 Epoch 22  	Train Loss = 20.05344 Val Loss = 20.91834
2023-12-09 14:53:32.053099 Epoch 23  	Train Loss = 19.95616 Val Loss = 20.68421
2023-12-09 15:04:11.774985 Epoch 24  	Train Loss = 19.84728 Val Loss = 20.59743
2023-12-09 15:14:51.751206 Epoch 25  	Train Loss = 19.84022 Val Loss = 20.72855
2023-12-09 15:25:31.814390 Epoch 26  	Train Loss = 19.78587 Val Loss = 20.58644
2023-12-09 15:36:11.363449 Epoch 27  	Train Loss = 19.68249 Val Loss = 20.86721
2023-12-09 15:46:51.144996 Epoch 28  	Train Loss = 19.66082 Val Loss = 20.70881
2023-12-09 15:57:30.763776 Epoch 29  	Train Loss = 19.59393 Val Loss = 20.39502
2023-12-09 16:08:11.268319 Epoch 30  	Train Loss = 19.56884 Val Loss = 20.43953
2023-12-09 16:18:51.016092 Epoch 31  	Train Loss = 19.49054 Val Loss = 20.40184
2023-12-09 16:29:31.651059 Epoch 32  	Train Loss = 19.43593 Val Loss = 20.49439
2023-12-09 16:40:11.501302 Epoch 33  	Train Loss = 19.39741 Val Loss = 20.41081
2023-12-09 16:50:51.370297 Epoch 34  	Train Loss = 19.39410 Val Loss = 20.26648
2023-12-09 17:01:31.584619 Epoch 35  	Train Loss = 19.33462 Val Loss = 20.47637
2023-12-09 17:12:11.114323 Epoch 36  	Train Loss = 19.30122 Val Loss = 20.39610
2023-12-09 17:22:51.087051 Epoch 37  	Train Loss = 19.26764 Val Loss = 20.34379
2023-12-09 17:33:31.692569 Epoch 38  	Train Loss = 19.22129 Val Loss = 20.52552
2023-12-09 17:44:11.193871 Epoch 39  	Train Loss = 19.23006 Val Loss = 20.07632
2023-12-09 17:54:51.840812 Epoch 40  	Train Loss = 19.10084 Val Loss = 20.32216
2023-12-09 18:05:31.984099 Epoch 41  	Train Loss = 19.87899 Val Loss = 21.91725
2023-12-09 18:16:12.052475 Epoch 42  	Train Loss = 21.59912 Val Loss = 22.19730
2023-12-09 18:26:51.461003 Epoch 43  	Train Loss = 20.58878 Val Loss = 21.19886
2023-12-09 18:37:32.200323 Epoch 44  	Train Loss = 20.03255 Val Loss = 21.11161
2023-12-09 18:48:11.944396 Epoch 45  	Train Loss = 20.07850 Val Loss = 21.75382
2023-12-09 18:58:51.082602 Epoch 46  	Train Loss = 34.07990 Val Loss = 155.90213
2023-12-09 19:09:20.981248 Epoch 47  	Train Loss = 159.61774 Val Loss = 155.80630
2023-12-09 19:19:52.223577 Epoch 48  	Train Loss = 159.28418 Val Loss = 155.76860
2023-12-09 19:30:22.022012 Epoch 49  	Train Loss = 159.28735 Val Loss = 155.71796
2023-12-09 19:40:53.366906 Epoch 50  	Train Loss = 159.28110 Val Loss = 155.72236
2023-12-09 19:51:26.213101 Epoch 51  	Train Loss = 159.27298 Val Loss = 155.66664
2023-12-09 20:01:56.236130 Epoch 52  	Train Loss = 159.28316 Val Loss = 155.70529
2023-12-09 20:12:29.339718 Epoch 53  	Train Loss = 159.28666 Val Loss = 155.73432
2023-12-09 20:22:59.552656 Epoch 54  	Train Loss = 159.27613 Val Loss = 155.66644
Early stopping at epoch: 54
Best at epoch 39:
Train Loss = 19.23006
Train RMSE = 31.90077, MAE = 19.51980, MAPE = 8.71312
Val Loss = 20.07632
Val RMSE = 33.52168, MAE = 20.60120, MAPE = 9.19141
--------- Test ---------
All Steps RMSE = 33.98483, MAE = 20.97003, MAPE = 8.99640
Step 1 RMSE = 28.26844, MAE = 17.88259, MAPE = 7.93018
Step 2 RMSE = 30.15581, MAE = 18.82875, MAPE = 8.14079
Step 3 RMSE = 31.48564, MAE = 19.58980, MAPE = 8.48786
Step 4 RMSE = 32.48898, MAE = 20.19078, MAPE = 8.53898
Step 5 RMSE = 33.27336, MAE = 20.58149, MAPE = 8.77784
Step 6 RMSE = 34.01617, MAE = 20.98881, MAPE = 9.11393
Step 7 RMSE = 34.66583, MAE = 21.34156, MAPE = 9.22449
Step 8 RMSE = 35.26752, MAE = 21.66363, MAPE = 9.16993
Step 9 RMSE = 35.87221, MAE = 22.06216, MAPE = 9.33907
Step 10 RMSE = 36.43847, MAE = 22.45482, MAPE = 9.56221
Step 11 RMSE = 36.97708, MAE = 22.82727, MAPE = 9.76352
Step 12 RMSE = 37.58302, MAE = 23.22566, MAPE = 9.90632
Inference time: 49.22 s
