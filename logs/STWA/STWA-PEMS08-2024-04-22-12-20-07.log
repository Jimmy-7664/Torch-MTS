PEMS08
Trainset:	x-(10700, 12, 170, 1)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 1)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 1)	y-(3566, 12, 170, 1)

Random seed = 233
--------- STWA ---------
{
    "in_steps": 12,
    "out_steps": 12,
    "lr": 0.001,
    "milestones": [],
    "clip_grad": false,
    "batch_size": 16,
    "max_epochs": 200,
    "early_stop": 15,
    "pass_device": true,
    "model_args": {
        "num_nodes": 170,
        "input_dim": 1,
        "output_dim": 1,
        "lag": 12,
        "horizon": 12,
        "device": "cuda:0",
        "channels": 16,
        "memory_size": 16,
        "dynamic": true
    }
}
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
STWA                                               [16, 12, 170, 1]          --
├─Sequential: 1-1                                  [16, 170, 16]             --
│    └─Linear: 2-1                                 [16, 170, 32]             416
│    └─Tanh: 2-2                                   [16, 170, 32]             --
│    └─Linear: 2-3                                 [16, 170, 32]             1,056
│    └─Tanh: 2-4                                   [16, 170, 32]             --
│    └─Linear: 2-5                                 [16, 170, 16]             528
├─Sequential: 1-2                                  [16, 170, 16]             --
│    └─Linear: 2-6                                 [16, 170, 32]             416
│    └─Tanh: 2-7                                   [16, 170, 32]             --
│    └─Linear: 2-8                                 [16, 170, 32]             1,056
│    └─Tanh: 2-9                                   [16, 170, 32]             --
│    └─Linear: 2-10                                [16, 170, 16]             528
├─Linear: 1-3                                      [16, 12, 170, 16]         32
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-11                                 [16, 12, 170, 16]         70,720
│    │    └─ModuleList: 3-1                        --                        6,100
│    │    └─ModuleList: 3-2                        --                        6,100
│    │    └─TemporalAttention: 3-3                 [16, 2, 170, 16]          544
│    │    └─SpatialAttention: 3-4                  [16, 2, 170, 16]          544
│    │    └─Sequential: 3-5                        [16, 2, 170, 16]          544
│    │    └─TemporalAttention: 3-6                 [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-7                  [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-8                        [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-9                 [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-10                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-11                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-12                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-13                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-14                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-15                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-16                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-17                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-18                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-19                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-20                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-21                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-22                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-23                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-24                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-25                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-26                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-27                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-28                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-29                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-30                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-31                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-32                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-33                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-34                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-35                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-36                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-37                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-38                       [16, 2, 170, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-12                                [16, 170, 256]            49,408
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-13                                 [16, 3, 170, 16]          21,760
│    │    └─ModuleList: 3-39                       --                        6,100
│    │    └─ModuleList: 3-40                       --                        6,100
│    │    └─TemporalAttention: 3-41                [16, 2, 170, 16]          544
│    │    └─SpatialAttention: 3-42                 [16, 2, 170, 16]          544
│    │    └─Sequential: 3-43                       [16, 2, 170, 16]          544
│    │    └─TemporalAttention: 3-44                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-45                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-46                       [16, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-47                [16, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-48                 [16, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-49                       [16, 2, 170, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-14                                [16, 170, 256]            12,544
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-15                                 [16, 1, 170, 16]          10,880
│    │    └─ModuleList: 3-50                       --                        6,100
│    │    └─ModuleList: 3-51                       --                        6,100
│    │    └─TemporalAttention: 3-52                [16, 2, 170, 16]          544
│    │    └─SpatialAttention: 3-53                 [16, 2, 170, 16]          544
│    │    └─Sequential: 3-54                       [16, 2, 170, 16]          544
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-16                                [16, 170, 256]            4,352
├─Sequential: 1-10                                 [16, 170, 12]             --
│    └─Linear: 2-17                                [16, 170, 512]            131,584
│    └─ReLU: 2-18                                  [16, 170, 512]            --
│    └─Linear: 2-19                                [16, 170, 12]             6,156
====================================================================================================
Total params: 352,932
Trainable params: 352,932
Non-trainable params: 0
Total mult-adds (M): 4.33
====================================================================================================
Input size (MB): 0.13
Forward/backward pass size (MB): 192.97
Params size (MB): 1.00
Estimated Total Size (MB): 194.10
====================================================================================================

Loss: HuberLoss

2024-04-22 12:20:59.590517 Epoch 1  	Train Loss = 28.95706 Val Loss = 21.51690
2024-04-22 12:21:49.621817 Epoch 2  	Train Loss = 19.67005 Val Loss = 19.28677
2024-04-22 12:22:37.571047 Epoch 3  	Train Loss = 18.38820 Val Loss = 19.60926
2024-04-22 12:23:25.466287 Epoch 4  	Train Loss = 17.58400 Val Loss = 17.76247
2024-04-22 12:24:18.997434 Epoch 5  	Train Loss = 17.08328 Val Loss = 17.72366
2024-04-22 12:25:06.660954 Epoch 6  	Train Loss = 16.77534 Val Loss = 18.06046
2024-04-22 12:25:54.816684 Epoch 7  	Train Loss = 16.41953 Val Loss = 16.81488
2024-04-22 12:26:43.071990 Epoch 8  	Train Loss = 16.19174 Val Loss = 17.16176
2024-04-22 12:27:30.736136 Epoch 9  	Train Loss = 16.00785 Val Loss = 17.02616
2024-04-22 12:28:21.043662 Epoch 10  	Train Loss = 15.72357 Val Loss = 16.97923
2024-04-22 12:29:09.002331 Epoch 11  	Train Loss = 15.58817 Val Loss = 16.72075
2024-04-22 12:29:57.256525 Epoch 12  	Train Loss = 15.47022 Val Loss = 16.60167
2024-04-22 12:30:45.444233 Epoch 13  	Train Loss = 15.32699 Val Loss = 17.05697
2024-04-22 12:31:37.691750 Epoch 14  	Train Loss = 15.15914 Val Loss = 16.75471
2024-04-22 12:32:32.465796 Epoch 15  	Train Loss = 15.08941 Val Loss = 16.36579
2024-04-22 12:33:26.159655 Epoch 16  	Train Loss = 15.01554 Val Loss = 16.08283
2024-04-22 12:34:14.640586 Epoch 17  	Train Loss = 14.91898 Val Loss = 16.16089
2024-04-22 12:35:02.529393 Epoch 18  	Train Loss = 14.85495 Val Loss = 16.04484
2024-04-22 12:35:50.985053 Epoch 19  	Train Loss = 14.74783 Val Loss = 16.11709
2024-04-22 12:36:39.220942 Epoch 20  	Train Loss = 14.67787 Val Loss = 16.64507
2024-04-22 12:37:27.478655 Epoch 21  	Train Loss = 14.55561 Val Loss = 16.55904
2024-04-22 12:38:15.405671 Epoch 22  	Train Loss = 14.51981 Val Loss = 16.27381
2024-04-22 12:39:04.975277 Epoch 23  	Train Loss = 14.48600 Val Loss = 16.00483
2024-04-22 12:39:53.060800 Epoch 24  	Train Loss = 14.40405 Val Loss = 16.57660
2024-04-22 12:40:41.077392 Epoch 25  	Train Loss = 14.35351 Val Loss = 16.56188
2024-04-22 12:41:29.522602 Epoch 26  	Train Loss = 14.26057 Val Loss = 16.03139
2024-04-22 12:42:17.626742 Epoch 27  	Train Loss = 14.27228 Val Loss = 16.07556
2024-04-22 12:43:07.703888 Epoch 28  	Train Loss = 14.20671 Val Loss = 16.16876
2024-04-22 12:43:55.611909 Epoch 29  	Train Loss = 14.20280 Val Loss = 16.41244
2024-04-22 12:44:43.879702 Epoch 30  	Train Loss = 14.17586 Val Loss = 16.15366
2024-04-22 12:45:32.608114 Epoch 31  	Train Loss = 14.05550 Val Loss = 16.06687
2024-04-22 12:46:20.512594 Epoch 32  	Train Loss = 14.05356 Val Loss = 16.15667
2024-04-22 12:47:10.920772 Epoch 33  	Train Loss = 14.03743 Val Loss = 16.13238
2024-04-22 12:47:59.338971 Epoch 34  	Train Loss = 13.97953 Val Loss = 16.24314
2024-04-22 12:48:47.557685 Epoch 35  	Train Loss = 13.90849 Val Loss = 15.93178
2024-04-22 12:49:36.608834 Epoch 36  	Train Loss = 13.95069 Val Loss = 15.92298
2024-04-22 12:50:25.478709 Epoch 37  	Train Loss = 13.86547 Val Loss = 16.06731
2024-04-22 12:51:13.965588 Epoch 38  	Train Loss = 13.85062 Val Loss = 16.05294
2024-04-22 12:52:03.149957 Epoch 39  	Train Loss = 13.80430 Val Loss = 16.08705
2024-04-22 12:52:51.461212 Epoch 40  	Train Loss = 13.77680 Val Loss = 15.89649
2024-04-22 12:53:42.394994 Epoch 41  	Train Loss = 13.80503 Val Loss = 15.85157
2024-04-22 12:54:30.986125 Epoch 42  	Train Loss = 13.73764 Val Loss = 15.94563
2024-04-22 12:55:18.928302 Epoch 43  	Train Loss = 13.66878 Val Loss = 16.09570
2024-04-22 12:56:07.413106 Epoch 44  	Train Loss = 13.67368 Val Loss = 15.93125
2024-04-22 12:56:55.151652 Epoch 45  	Train Loss = 13.67057 Val Loss = 16.08050
2024-04-22 12:57:43.194674 Epoch 46  	Train Loss = 13.62483 Val Loss = 15.94433
2024-04-22 12:58:30.510886 Epoch 47  	Train Loss = 13.62637 Val Loss = 16.03139
2024-04-22 12:59:17.782201 Epoch 48  	Train Loss = 13.55766 Val Loss = 16.00508
2024-04-22 13:00:05.128591 Epoch 49  	Train Loss = 13.59490 Val Loss = 15.95059
2024-04-22 13:00:52.431119 Epoch 50  	Train Loss = 13.51996 Val Loss = 15.92417
2024-04-22 13:01:39.795486 Epoch 51  	Train Loss = 13.50386 Val Loss = 15.91721
2024-04-22 13:02:27.164122 Epoch 52  	Train Loss = 13.51021 Val Loss = 15.88689
2024-04-22 13:03:14.492321 Epoch 53  	Train Loss = 13.46765 Val Loss = 16.18571
2024-04-22 13:04:02.065334 Epoch 54  	Train Loss = 13.45994 Val Loss = 16.01966
2024-04-22 13:04:49.896878 Epoch 55  	Train Loss = 13.41855 Val Loss = 15.96599
2024-04-22 13:05:41.068785 Epoch 56  	Train Loss = 13.42233 Val Loss = 16.03609
Early stopping at epoch: 56
Best at epoch 41:
Train Loss = 13.80503
Train MAE = 14.14324, RMSE = 23.31724, MAPE = 9.62890
Val Loss = 15.85157
Val MAE = 16.31348, RMSE = 26.26634, MAPE = 12.67100
Model checkpoint saved to: ../saved_models/STWA/STWA-PEMS08-2024-04-22-12-20-07.pt
--------- Test ---------
All Steps (1-12) MAE = 16.02615, RMSE = 25.25099, MAPE = 10.85486
Step 1 MAE = 14.34649, RMSE = 22.06026, MAPE = 10.86025
Step 2 MAE = 14.74396, RMSE = 22.85811, MAPE = 10.69364
Step 3 MAE = 15.08170, RMSE = 23.47211, MAPE = 11.01007
Step 4 MAE = 15.36248, RMSE = 24.03906, MAPE = 10.23969
Step 5 MAE = 15.69706, RMSE = 24.63231, MAPE = 10.56013
Step 6 MAE = 16.01648, RMSE = 25.15664, MAPE = 11.21097
Step 7 MAE = 16.20548, RMSE = 25.60747, MAPE = 10.47151
Step 8 MAE = 16.45649, RMSE = 26.06085, MAPE = 11.11753
Step 9 MAE = 16.65472, RMSE = 26.46135, MAPE = 10.87447
Step 10 MAE = 16.92583, RMSE = 26.88672, MAPE = 10.76035
Step 11 MAE = 17.21850, RMSE = 27.28501, MAPE = 11.03139
Step 12 MAE = 17.60478, RMSE = 27.77248, MAPE = 11.42832
Inference time: 3.80 s
