PEMS07
Trainset:	x-(16921, 12, 883, 1)	y-(16921, 12, 883, 1)
Valset:  	x-(5640, 12, 883, 1)  	y-(5640, 12, 883, 1)
Testset:	x-(5640, 12, 883, 1)	y-(5640, 12, 883, 1)

--------- STWA ---------
{
    "in_steps": 12,
    "out_steps": 12,
    "lr": 0.001,
    "milestones": [],
    "clip_grad": false,
    "batch_size": 16,
    "max_epochs": 200,
    "early_stop": 15,
    "pass_device": true,
    "model_args": {
        "num_nodes": 883,
        "input_dim": 1,
        "output_dim": 1,
        "lag": 12,
        "horizon": 12,
        "device": "cuda:0",
        "channels": 16,
        "memory_size": 16,
        "dynamic": true
    }
}
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
STWA                                               [16, 12, 883, 1]          --
├─Sequential: 1-1                                  [16, 883, 16]             --
│    └─Linear: 2-1                                 [16, 883, 32]             416
│    └─Tanh: 2-2                                   [16, 883, 32]             --
│    └─Linear: 2-3                                 [16, 883, 32]             1,056
│    └─Tanh: 2-4                                   [16, 883, 32]             --
│    └─Linear: 2-5                                 [16, 883, 16]             528
├─Sequential: 1-2                                  [16, 883, 16]             --
│    └─Linear: 2-6                                 [16, 883, 32]             416
│    └─Tanh: 2-7                                   [16, 883, 32]             --
│    └─Linear: 2-8                                 [16, 883, 32]             1,056
│    └─Tanh: 2-9                                   [16, 883, 32]             --
│    └─Linear: 2-10                                [16, 883, 16]             528
├─Linear: 1-3                                      [16, 12, 883, 16]         32
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-11                                 [16, 12, 883, 16]         367,328
│    │    └─ModuleList: 3-1                        --                        6,100
│    │    └─ModuleList: 3-2                        --                        6,100
│    │    └─TemporalAttention: 3-3                 [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-4                  [16, 2, 883, 16]          544
│    │    └─Sequential: 3-5                        [16, 2, 883, 16]          544
│    │    └─TemporalAttention: 3-6                 [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-7                  [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-8                        [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-9                 [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-10                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-11                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-12                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-13                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-14                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-15                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-16                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-17                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-18                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-19                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-20                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-21                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-22                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-23                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-24                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-25                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-26                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-27                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-28                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-29                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-30                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-31                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-32                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-33                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-34                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-35                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-36                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-37                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-38                       [16, 2, 883, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-12                                [16, 883, 256]            49,408
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-13                                 [16, 3, 883, 16]          113,024
│    │    └─ModuleList: 3-39                       --                        6,100
│    │    └─ModuleList: 3-40                       --                        6,100
│    │    └─TemporalAttention: 3-41                [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-42                 [16, 2, 883, 16]          544
│    │    └─Sequential: 3-43                       [16, 2, 883, 16]          544
│    │    └─TemporalAttention: 3-44                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-45                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-46                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-47                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-48                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-49                       [16, 2, 883, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-14                                [16, 883, 256]            12,544
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-15                                 [16, 1, 883, 16]          56,512
│    │    └─ModuleList: 3-50                       --                        6,100
│    │    └─ModuleList: 3-51                       --                        6,100
│    │    └─TemporalAttention: 3-52                [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-53                 [16, 2, 883, 16]          544
│    │    └─Sequential: 3-54                       [16, 2, 883, 16]          544
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-16                                [16, 883, 256]            4,352
├─Sequential: 1-10                                 [16, 883, 12]             --
│    └─Linear: 2-17                                [16, 883, 512]            131,584
│    └─ReLU: 2-18                                  [16, 883, 512]            --
│    └─Linear: 2-19                                [16, 883, 12]             6,156
====================================================================================================
Total params: 786,436
Trainable params: 786,436
Non-trainable params: 0
Total mult-adds (M): 4.33
====================================================================================================
Input size (MB): 0.68
Forward/backward pass size (MB): 720.19
Params size (MB): 1.00
Estimated Total Size (MB): 721.87
====================================================================================================

Loss: HuberLoss

2023-12-05 17:08:24.014780 Epoch 1  	Train Loss = 38.85354 Val Loss = 32.13175
2023-12-05 17:19:07.171807 Epoch 2  	Train Loss = 27.29339 Val Loss = 25.68590
2023-12-05 17:29:51.741614 Epoch 3  	Train Loss = 24.92395 Val Loss = 24.29040
2023-12-05 17:40:34.661491 Epoch 4  	Train Loss = 23.75337 Val Loss = 23.26541
2023-12-05 17:51:18.856545 Epoch 5  	Train Loss = 23.11745 Val Loss = 23.36861
2023-12-05 18:02:02.577629 Epoch 6  	Train Loss = 22.66979 Val Loss = 24.02194
2023-12-05 18:12:46.653269 Epoch 7  	Train Loss = 22.31797 Val Loss = 22.49233
2023-12-05 18:23:30.181595 Epoch 8  	Train Loss = 22.01662 Val Loss = 22.72137
2023-12-05 18:34:13.069108 Epoch 9  	Train Loss = 21.77678 Val Loss = 22.18482
2023-12-05 18:44:55.199036 Epoch 10  	Train Loss = 21.45803 Val Loss = 21.78256
2023-12-05 18:55:38.293989 Epoch 11  	Train Loss = 21.29955 Val Loss = 22.16001
2023-12-05 19:06:20.157440 Epoch 12  	Train Loss = 21.09856 Val Loss = 21.73856
2023-12-05 19:17:01.961859 Epoch 13  	Train Loss = 20.99160 Val Loss = 21.25659
2023-12-05 19:27:43.578981 Epoch 14  	Train Loss = 20.80953 Val Loss = 21.70087
2023-12-05 19:38:25.425246 Epoch 15  	Train Loss = 20.70267 Val Loss = 21.41525
2023-12-05 19:49:08.362600 Epoch 16  	Train Loss = 20.58674 Val Loss = 21.50327
2023-12-05 19:59:51.748306 Epoch 17  	Train Loss = 20.45738 Val Loss = 21.00425
2023-12-05 20:10:36.002594 Epoch 18  	Train Loss = 20.39423 Val Loss = 21.11622
2023-12-05 20:21:19.393733 Epoch 19  	Train Loss = 20.26927 Val Loss = 20.99611
2023-12-05 20:32:03.555330 Epoch 20  	Train Loss = 20.18810 Val Loss = 21.23310
2023-12-05 20:42:47.394565 Epoch 21  	Train Loss = 20.06781 Val Loss = 21.20617
2023-12-05 20:53:31.577799 Epoch 22  	Train Loss = 20.05344 Val Loss = 20.91834
2023-12-05 21:04:16.091620 Epoch 23  	Train Loss = 19.95616 Val Loss = 20.68421
2023-12-05 21:14:58.774960 Epoch 24  	Train Loss = 19.84728 Val Loss = 20.59743
2023-12-05 21:25:39.538751 Epoch 25  	Train Loss = 19.84022 Val Loss = 20.72855
2023-12-05 21:36:20.619400 Epoch 26  	Train Loss = 19.78587 Val Loss = 20.58644
2023-12-05 21:47:01.330046 Epoch 27  	Train Loss = 19.68249 Val Loss = 20.86721
2023-12-05 21:57:42.300888 Epoch 28  	Train Loss = 19.66082 Val Loss = 20.70881
2023-12-05 22:08:24.194498 Epoch 29  	Train Loss = 19.59393 Val Loss = 20.39502
2023-12-05 22:19:06.674189 Epoch 30  	Train Loss = 19.56884 Val Loss = 20.43953
2023-12-05 22:29:49.489720 Epoch 31  	Train Loss = 19.49054 Val Loss = 20.40184
2023-12-05 22:40:32.215442 Epoch 32  	Train Loss = 19.43593 Val Loss = 20.49439
2023-12-05 22:51:14.461767 Epoch 33  	Train Loss = 19.39741 Val Loss = 20.41081
2023-12-05 23:01:56.429152 Epoch 34  	Train Loss = 19.39410 Val Loss = 20.26648
2023-12-05 23:12:38.864657 Epoch 35  	Train Loss = 19.33462 Val Loss = 20.47637
2023-12-05 23:23:21.043582 Epoch 36  	Train Loss = 19.30122 Val Loss = 20.39610
2023-12-05 23:34:03.005449 Epoch 37  	Train Loss = 19.26764 Val Loss = 20.34379
2023-12-05 23:44:45.542180 Epoch 38  	Train Loss = 19.22129 Val Loss = 20.52552
2023-12-05 23:55:28.533929 Epoch 39  	Train Loss = 19.23006 Val Loss = 20.07632
2023-12-06 00:06:11.370928 Epoch 40  	Train Loss = 19.10084 Val Loss = 20.32216
2023-12-06 00:16:53.163066 Epoch 41  	Train Loss = 19.87899 Val Loss = 21.91725
2023-12-06 00:27:35.877720 Epoch 42  	Train Loss = 21.59912 Val Loss = 22.19730
2023-12-06 00:38:17.119881 Epoch 43  	Train Loss = 20.58878 Val Loss = 21.19886
2023-12-06 00:48:58.480715 Epoch 44  	Train Loss = 20.03255 Val Loss = 21.11161
2023-12-06 00:59:40.662427 Epoch 45  	Train Loss = 20.07850 Val Loss = 21.75382
2023-12-06 01:10:21.186369 Epoch 46  	Train Loss = 34.07990 Val Loss = 155.90213
2023-12-06 01:20:52.550996 Epoch 47  	Train Loss = 159.61774 Val Loss = 155.80630
2023-12-06 01:31:24.867954 Epoch 48  	Train Loss = 159.28418 Val Loss = 155.76860
2023-12-06 01:41:56.288025 Epoch 49  	Train Loss = 159.28735 Val Loss = 155.71796
2023-12-06 01:52:27.676918 Epoch 50  	Train Loss = 159.28110 Val Loss = 155.72236
2023-12-06 02:02:59.079451 Epoch 51  	Train Loss = 159.27298 Val Loss = 155.66664
2023-12-06 02:13:30.719847 Epoch 52  	Train Loss = 159.28316 Val Loss = 155.70529
2023-12-06 02:24:01.837408 Epoch 53  	Train Loss = 159.28666 Val Loss = 155.73432
2023-12-06 02:34:33.153936 Epoch 54  	Train Loss = 159.27613 Val Loss = 155.66644
Early stopping at epoch: 54
Best at epoch 39:
Train Loss = 19.23006
Train RMSE = 188.95979, MAE = 159.13795, MAPE = 213.64138
Val Loss = 20.07632
Val RMSE = 185.24188, MAE = 155.65643, MAPE = 214.39984
--------- Test ---------
All Steps RMSE = 186.19264, MAE = 156.20050, MAPE = 200.06561
Step 1 RMSE = 186.22868, MAE = 156.20949, MAPE = 199.55716
Step 2 RMSE = 186.22118, MAE = 156.20624, MAPE = 199.63849
Step 3 RMSE = 186.21097, MAE = 156.20221, MAPE = 199.75065
Step 4 RMSE = 186.20204, MAE = 156.19897, MAPE = 199.85809
Step 5 RMSE = 186.19366, MAE = 156.19688, MAPE = 199.97251
Step 6 RMSE = 186.18788, MAE = 156.19547, MAPE = 200.05250
Step 7 RMSE = 186.19122, MAE = 156.19788, MAPE = 200.03543
Step 8 RMSE = 186.19072, MAE = 156.19878, MAPE = 200.06955
Step 9 RMSE = 186.18752, MAE = 156.19908, MAPE = 200.14246
Step 10 RMSE = 186.16782, MAE = 156.19269, MAPE = 200.42512
Step 11 RMSE = 186.16159, MAE = 156.19247, MAPE = 200.55447
Step 12 RMSE = 186.15565, MAE = 156.19344, MAPE = 200.70205
Inference time: 48.94 s
