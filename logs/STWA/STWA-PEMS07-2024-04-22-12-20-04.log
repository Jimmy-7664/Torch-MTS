PEMS07
Trainset:	x-(16921, 12, 883, 1)	y-(16921, 12, 883, 1)
Valset:  	x-(5640, 12, 883, 1)  	y-(5640, 12, 883, 1)
Testset:	x-(5640, 12, 883, 1)	y-(5640, 12, 883, 1)

Random seed = 233
--------- STWA ---------
{
    "in_steps": 12,
    "out_steps": 12,
    "lr": 0.001,
    "milestones": [],
    "clip_grad": false,
    "batch_size": 16,
    "max_epochs": 200,
    "early_stop": 15,
    "pass_device": true,
    "model_args": {
        "num_nodes": 883,
        "input_dim": 1,
        "output_dim": 1,
        "lag": 12,
        "horizon": 12,
        "device": "cuda:0",
        "channels": 16,
        "memory_size": 16,
        "dynamic": true
    }
}
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
STWA                                               [16, 12, 883, 1]          --
├─Sequential: 1-1                                  [16, 883, 16]             --
│    └─Linear: 2-1                                 [16, 883, 32]             416
│    └─Tanh: 2-2                                   [16, 883, 32]             --
│    └─Linear: 2-3                                 [16, 883, 32]             1,056
│    └─Tanh: 2-4                                   [16, 883, 32]             --
│    └─Linear: 2-5                                 [16, 883, 16]             528
├─Sequential: 1-2                                  [16, 883, 16]             --
│    └─Linear: 2-6                                 [16, 883, 32]             416
│    └─Tanh: 2-7                                   [16, 883, 32]             --
│    └─Linear: 2-8                                 [16, 883, 32]             1,056
│    └─Tanh: 2-9                                   [16, 883, 32]             --
│    └─Linear: 2-10                                [16, 883, 16]             528
├─Linear: 1-3                                      [16, 12, 883, 16]         32
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-11                                 [16, 12, 883, 16]         367,328
│    │    └─ModuleList: 3-1                        --                        6,100
│    │    └─ModuleList: 3-2                        --                        6,100
│    │    └─TemporalAttention: 3-3                 [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-4                  [16, 2, 883, 16]          544
│    │    └─Sequential: 3-5                        [16, 2, 883, 16]          544
│    │    └─TemporalAttention: 3-6                 [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-7                  [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-8                        [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-9                 [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-10                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-11                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-12                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-13                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-14                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-15                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-16                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-17                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-18                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-19                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-20                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-21                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-22                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-23                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-24                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-25                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-26                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-27                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-28                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-29                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-30                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-31                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-32                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-33                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-34                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-35                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-36                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-37                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-38                       [16, 2, 883, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-12                                [16, 883, 256]            49,408
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-13                                 [16, 3, 883, 16]          113,024
│    │    └─ModuleList: 3-39                       --                        6,100
│    │    └─ModuleList: 3-40                       --                        6,100
│    │    └─TemporalAttention: 3-41                [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-42                 [16, 2, 883, 16]          544
│    │    └─Sequential: 3-43                       [16, 2, 883, 16]          544
│    │    └─TemporalAttention: 3-44                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-45                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-46                       [16, 2, 883, 16]          (recursive)
│    │    └─TemporalAttention: 3-47                [16, 2, 883, 16]          (recursive)
│    │    └─SpatialAttention: 3-48                 [16, 2, 883, 16]          (recursive)
│    │    └─Sequential: 3-49                       [16, 2, 883, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-14                                [16, 883, 256]            12,544
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-15                                 [16, 1, 883, 16]          56,512
│    │    └─ModuleList: 3-50                       --                        6,100
│    │    └─ModuleList: 3-51                       --                        6,100
│    │    └─TemporalAttention: 3-52                [16, 2, 883, 16]          544
│    │    └─SpatialAttention: 3-53                 [16, 2, 883, 16]          544
│    │    └─Sequential: 3-54                       [16, 2, 883, 16]          544
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-16                                [16, 883, 256]            4,352
├─Sequential: 1-10                                 [16, 883, 12]             --
│    └─Linear: 2-17                                [16, 883, 512]            131,584
│    └─ReLU: 2-18                                  [16, 883, 512]            --
│    └─Linear: 2-19                                [16, 883, 12]             6,156
====================================================================================================
Total params: 786,436
Trainable params: 786,436
Non-trainable params: 0
Total mult-adds (M): 4.33
====================================================================================================
Input size (MB): 0.68
Forward/backward pass size (MB): 1002.30
Params size (MB): 1.00
Estimated Total Size (MB): 1003.97
====================================================================================================

Loss: HuberLoss

2024-04-22 12:31:40.841417 Epoch 1  	Train Loss = 38.78010 Val Loss = 34.33623
2024-04-22 12:43:11.841840 Epoch 2  	Train Loss = 27.49036 Val Loss = 26.10125
2024-04-22 12:54:42.188820 Epoch 3  	Train Loss = 24.90912 Val Loss = 24.59615
2024-04-22 13:06:11.835996 Epoch 4  	Train Loss = 23.67441 Val Loss = 23.84747
2024-04-22 13:17:40.784900 Epoch 5  	Train Loss = 23.14197 Val Loss = 23.11718
2024-04-22 13:29:11.929635 Epoch 6  	Train Loss = 22.58534 Val Loss = 22.96981
2024-04-22 13:40:41.786534 Epoch 7  	Train Loss = 22.28578 Val Loss = 22.40612
2024-04-22 13:52:09.891840 Epoch 8  	Train Loss = 22.05495 Val Loss = 22.01425
2024-04-22 14:03:38.893284 Epoch 9  	Train Loss = 21.75387 Val Loss = 22.56967
2024-04-22 14:15:08.982920 Epoch 10  	Train Loss = 21.50220 Val Loss = 21.72717
2024-04-22 14:26:38.704294 Epoch 11  	Train Loss = 21.33336 Val Loss = 22.19489
2024-04-22 14:38:06.439363 Epoch 12  	Train Loss = 21.10476 Val Loss = 21.61803
2024-04-22 14:49:34.183535 Epoch 13  	Train Loss = 20.95095 Val Loss = 21.42586
2024-04-22 15:01:01.879164 Epoch 14  	Train Loss = 20.84576 Val Loss = 21.55360
2024-04-22 15:12:29.677922 Epoch 15  	Train Loss = 20.78223 Val Loss = 21.21240
2024-04-22 15:23:58.457199 Epoch 16  	Train Loss = 20.48761 Val Loss = 21.55474
2024-04-22 15:35:26.145934 Epoch 17  	Train Loss = 20.38250 Val Loss = 20.92801
2024-04-22 15:46:53.992041 Epoch 18  	Train Loss = 20.40054 Val Loss = 20.94305
2024-04-22 15:58:21.996262 Epoch 19  	Train Loss = 20.18303 Val Loss = 20.99479
2024-04-22 16:09:50.047839 Epoch 20  	Train Loss = 20.14779 Val Loss = 20.79361
2024-04-22 16:21:17.924850 Epoch 21  	Train Loss = 20.05738 Val Loss = 20.79505
2024-04-22 16:32:46.822456 Epoch 22  	Train Loss = 19.99296 Val Loss = 20.67237
2024-04-22 16:44:14.838610 Epoch 23  	Train Loss = 19.89544 Val Loss = 20.76645
2024-04-22 16:55:43.318338 Epoch 24  	Train Loss = 19.82859 Val Loss = 20.54309
2024-04-22 17:07:12.256251 Epoch 25  	Train Loss = 19.73257 Val Loss = 20.66403
2024-04-22 17:18:41.018376 Epoch 26  	Train Loss = 19.78074 Val Loss = 20.58776
2024-04-22 17:30:09.670569 Epoch 27  	Train Loss = 19.69436 Val Loss = 20.45368
2024-04-22 17:41:38.666498 Epoch 28  	Train Loss = 19.59858 Val Loss = 20.71019
2024-04-22 17:53:08.256801 Epoch 29  	Train Loss = 19.60196 Val Loss = 20.44259
2024-04-22 18:04:36.696471 Epoch 30  	Train Loss = 19.51282 Val Loss = 20.49701
2024-04-22 18:16:05.018812 Epoch 31  	Train Loss = 19.48493 Val Loss = 20.56440
2024-04-22 18:27:35.215913 Epoch 32  	Train Loss = 19.46255 Val Loss = 20.52767
2024-04-22 18:39:03.806895 Epoch 33  	Train Loss = 19.38855 Val Loss = 20.22236
2024-04-22 18:50:32.011268 Epoch 34  	Train Loss = 19.33903 Val Loss = 20.30330
2024-04-22 19:02:03.625152 Epoch 35  	Train Loss = 19.31306 Val Loss = 20.29217
2024-04-22 19:13:34.152319 Epoch 36  	Train Loss = 19.28135 Val Loss = 20.31796
2024-04-22 19:25:04.341276 Epoch 37  	Train Loss = 19.25661 Val Loss = 20.24031
2024-04-22 19:36:34.685058 Epoch 38  	Train Loss = 19.20623 Val Loss = 20.10269
2024-04-22 19:48:04.517670 Epoch 39  	Train Loss = 19.24363 Val Loss = 20.13064
2024-04-22 19:59:34.306117 Epoch 40  	Train Loss = 19.11020 Val Loss = 20.34293
2024-04-22 20:11:04.436376 Epoch 41  	Train Loss = 19.09905 Val Loss = 20.14017
2024-04-22 20:22:34.200741 Epoch 42  	Train Loss = 19.08924 Val Loss = 20.18743
2024-04-22 20:34:03.689437 Epoch 43  	Train Loss = 19.04819 Val Loss = 20.15252
2024-04-22 20:45:32.172131 Epoch 44  	Train Loss = 19.00910 Val Loss = 20.16292
2024-04-22 20:57:00.187296 Epoch 45  	Train Loss = 19.03061 Val Loss = 19.99821
2024-04-22 21:08:29.733658 Epoch 46  	Train Loss = 18.97020 Val Loss = 20.16368
2024-04-22 21:20:00.059554 Epoch 47  	Train Loss = 18.95408 Val Loss = 20.10369
2024-04-22 21:31:29.999161 Epoch 48  	Train Loss = 18.90733 Val Loss = 20.07796
2024-04-22 21:42:59.510592 Epoch 49  	Train Loss = 18.89246 Val Loss = 20.04505
2024-04-22 21:54:28.919637 Epoch 50  	Train Loss = 18.88759 Val Loss = 20.13766
2024-04-22 22:05:58.177294 Epoch 51  	Train Loss = 18.81648 Val Loss = 20.48656
2024-04-22 22:17:27.712695 Epoch 52  	Train Loss = 18.85197 Val Loss = 19.92575
2024-04-22 22:28:57.156501 Epoch 53  	Train Loss = 18.78277 Val Loss = 20.01680
2024-04-22 22:40:26.691660 Epoch 54  	Train Loss = 18.77760 Val Loss = 19.92610
2024-04-22 22:51:55.635375 Epoch 55  	Train Loss = 18.77199 Val Loss = 20.25581
2024-04-22 23:03:24.687187 Epoch 56  	Train Loss = 18.76275 Val Loss = 20.03770
2024-04-22 23:14:54.089344 Epoch 57  	Train Loss = 18.71626 Val Loss = 20.10540
2024-04-22 23:26:23.402290 Epoch 58  	Train Loss = 18.72068 Val Loss = 20.08434
2024-04-22 23:37:52.627164 Epoch 59  	Train Loss = 18.85391 Val Loss = 27.95027
2024-04-22 23:49:22.117047 Epoch 60  	Train Loss = 32.14602 Val Loss = 28.08864
2024-04-23 00:00:51.512371 Epoch 61  	Train Loss = 28.51606 Val Loss = 25.48012
2024-04-23 00:12:21.576260 Epoch 62  	Train Loss = 24.13006 Val Loss = 23.92222
2024-04-23 00:23:51.004123 Epoch 63  	Train Loss = 23.17823 Val Loss = 23.55661
2024-04-23 00:35:20.767032 Epoch 64  	Train Loss = 22.45913 Val Loss = 23.21120
2024-04-23 00:46:50.293270 Epoch 65  	Train Loss = 27.16209 Val Loss = 26.37158
2024-04-23 00:58:19.929963 Epoch 66  	Train Loss = 25.10080 Val Loss = 25.41907
2024-04-23 01:09:49.244212 Epoch 67  	Train Loss = 24.30977 Val Loss = 24.92475
Early stopping at epoch: 67
Best at epoch 52:
Train Loss = 18.85197
Train MAE = 19.22464, RMSE = 31.56536, MAPE = 8.64894
Val Loss = 19.92575
Val MAE = 20.45903, RMSE = 33.45168, MAPE = 9.24794
Model checkpoint saved to: ../saved_models/STWA/STWA-PEMS07-2024-04-22-12-20-04.pt
--------- Test ---------
All Steps (1-12) MAE = 20.81501, RMSE = 33.95045, MAPE = 9.02942
Step 1 MAE = 17.85890, RMSE = 28.41475, MAPE = 7.65214
Step 2 MAE = 18.76455, RMSE = 30.21463, MAPE = 8.12868
Step 3 MAE = 19.47644, RMSE = 31.48552, MAPE = 8.56011
Step 4 MAE = 19.99752, RMSE = 32.46578, MAPE = 8.67262
Step 5 MAE = 20.42175, RMSE = 33.28050, MAPE = 8.90274
Step 6 MAE = 20.84492, RMSE = 34.01524, MAPE = 8.96386
Step 7 MAE = 21.23650, RMSE = 34.66525, MAPE = 9.12587
Step 8 MAE = 21.54605, RMSE = 35.25904, MAPE = 9.24684
Step 9 MAE = 21.87468, RMSE = 35.81744, MAPE = 9.51231
Step 10 MAE = 22.19275, RMSE = 36.28188, MAPE = 9.73008
Step 11 MAE = 22.54238, RMSE = 36.81625, MAPE = 9.75800
Step 12 MAE = 23.02054, RMSE = 37.43697, MAPE = 10.09847
Inference time: 60.99 s
