{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cseadmin/dz/anaconda3/envs/torch1.11/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def fit_transform(self, data):\n",
    "        self.mean = data.mean()\n",
    "        self.std = data.std()\n",
    "        \n",
    "        return (data - self.mean) / self.std\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(666) # 64不能过 32能过\n",
    "np.random.seed(233) # 64能过 32不能过\n",
    "# np.random.seed(2333) # 64和32都能过\n",
    "\n",
    "samples=1280\n",
    "arr=np.zeros(samples)\n",
    "ran=np.random.random(samples*100)#*100\n",
    "\n",
    "arr=np.concatenate((arr, ran))\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    mean=arr.mean(), std=arr.std()\n",
    ")\n",
    "\n",
    "print((arr==0).sum())\n",
    "print(arr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "def test_numpy(arr, scaler):\n",
    "    arr_tf = scaler.transform(arr)\n",
    "    res = scaler.inverse_transform(arr_tf)\n",
    "    print((res==0).sum())\n",
    "    print(res.dtype)\n",
    "\n",
    "test_numpy(arr, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "def test_tensor_numpy(arr, scaler):\n",
    "    arr_tf = scaler.transform(arr)\n",
    "    arr_tensor=torch.FloatTensor(arr_tf)\n",
    "    arr_tensor_numpy=arr_tensor.numpy()\n",
    "    \n",
    "    res = scaler.inverse_transform(arr_tensor_numpy)\n",
    "    print((res==0).sum())\n",
    "    print(res.dtype)\n",
    "\n",
    "test_tensor_numpy(arr, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "def test_tensor_from_numpy_to_numpy(arr, scaler):\n",
    "    arr_tf = scaler.transform(arr)\n",
    "    arr_tensor_from_numpy=torch.from_numpy(arr_tf)#.float()\n",
    "    arr_tensor_from_numpy_to_numpy=arr_tensor_from_numpy.numpy()\n",
    "\n",
    "    res = scaler.inverse_transform(arr_tensor_from_numpy_to_numpy)\n",
    "    \n",
    "    print((res==0).sum())\n",
    "    print(res.dtype)\n",
    "\n",
    "test_tensor_from_numpy_to_numpy(arr, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "def test_tensor_cuda_numpy(arr, scaler):\n",
    "    arr_tf = scaler.transform(arr)\n",
    "    arr_tensor_cuda=torch.FloatTensor(arr_tf).cuda()\n",
    "    arr_tensor_cuda_numpy=arr_tensor_cuda.cpu().numpy()\n",
    "\n",
    "    res = scaler.inverse_transform(arr_tensor_cuda_numpy)\n",
    "    print((res==0).sum())\n",
    "    print(res.dtype)\n",
    "    \n",
    "test_tensor_cuda_numpy(arr, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "def test_dataloader(arr, scaler):\n",
    "    arr_tf = scaler.transform(arr)\n",
    "    arr_tensor=torch.FloatTensor(arr_tf)\n",
    "    dataset = torch.utils.data.TensorDataset(arr_tensor, arr_tensor)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=64, shuffle=False\n",
    "    )\n",
    "\n",
    "    all=[]\n",
    "    for batch, _ in loader:\n",
    "        batch = batch.cuda()\n",
    "\n",
    "        batch = batch.cpu().numpy()\n",
    "        all.append(batch)\n",
    "        \n",
    "    all = np.vstack(all).squeeze()\n",
    "\n",
    "    res = scaler.inverse_transform(all)\n",
    "    print((res==0).sum())\n",
    "    print(res.dtype)\n",
    "    \n",
    "test_dataloader(arr, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6832, 12, 207, 1)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def gen_xy(data, in_steps, out_steps, with_embeddings=False):\n",
    "    if data.ndim == 2:\n",
    "        data = data[:, :, np.newaxis]\n",
    "\n",
    "    all_steps = data.shape[0]\n",
    "    indices = [\n",
    "        (i, i + (in_steps + out_steps))\n",
    "        for i in range(all_steps - (in_steps + out_steps) + 1)\n",
    "    ]\n",
    "\n",
    "    x, y = [], []\n",
    "    for begin, end in indices:\n",
    "        x.append(data[begin : begin + in_steps])\n",
    "        y.append(data[begin + in_steps : end])\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if with_embeddings:\n",
    "        y = y[..., 0][..., np.newaxis]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "train_size=0.7\n",
    "val_size=0.1\n",
    "\n",
    "data_path=\"./data/METRLA\"\n",
    "dataset=\"METRLA\"\n",
    "\n",
    "data = np.load(os.path.join(data_path, f\"{dataset}.npy\")) # equivalent to read_hdf(xxx).values\n",
    "data = data[:, :, np.newaxis]\n",
    "\n",
    "all_steps = data.shape[0]\n",
    "split1 = int(all_steps * train_size)\n",
    "split2 = int(all_steps * (train_size + val_size))\n",
    "\n",
    "train_data = data[:split1]\n",
    "val_data = data[split1:split2]\n",
    "test_data = data[split2:]\n",
    "\n",
    "x_train, y_train = gen_xy(train_data, 12, 12, False)\n",
    "x_val, y_val = gen_xy(val_data, 12, 12, False)\n",
    "x_test, y_test = gen_xy(test_data, 12, 12, False)\n",
    "\n",
    "print(y_test.shape)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6850, 12, 207, 1)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "y_test_npz=np.load(os.path.join(data_path, \"test.npz\"))[\"y\"][..., :1]\n",
    "\n",
    "print(y_test_npz.shape)\n",
    "print(y_test_npz.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2060124\n",
      "2060359\n"
     ]
    }
   ],
   "source": [
    "print((y_test==0).sum())\n",
    "print((y_test_npz==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler(\n",
    "    mean=x_train[..., 0].mean(), std=x_train[..., 0].std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2060124\n",
      "float64\n",
      "2060359\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "test_numpy(y_test, scaler2)\n",
    "test_numpy(y_test_npz, scaler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "float32\n",
      "0\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "test_tensor_numpy(y_test, scaler2)\n",
    "test_tensor_numpy(y_test_npz, scaler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2060124\n",
      "float64\n",
      "2060359\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "test_tensor_from_numpy_to_numpy(y_test, scaler2)\n",
    "test_tensor_from_numpy_to_numpy(y_test_npz, scaler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "float32\n",
      "0\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "test_tensor_cuda_numpy(y_test, scaler2)\n",
    "test_tensor_cuda_numpy(y_test_npz, scaler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "float32\n",
      "0\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "test_dataloader(y_test, scaler2)\n",
    "test_dataloader(y_test_npz, scaler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(torch.from_numpy(np.zeros(1)).dtype)\n",
    "print(torch.from_numpy(np.zeros(1)).float().dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于训练模型时 StandardScaler.inverse_transform() 的调查报告\n",
    "\n",
    "*23.3.26 更新：float32 个锤子，重点是别 transform y_true !!!!!!!!!!!!!!!!!! 往下看往下看往下看有新的实验*\n",
    "\n",
    "#### 现象\n",
    "\n",
    "0 transform 过去，变成 tensor, 然后 numpy() 变回 ndarray 之后, 再 inverse_transform 回来, 就不是 0 而是一个很小的数\n",
    "\n",
    "y_true 在分母上，这一套下来原本的 0 值变成了一个超小的数，然后就会导致 MAPE 爆炸\n",
    "\n",
    "#### 原因\n",
    "\n",
    "torch 中的 tensor 为 float32 类型，而原始 numpy array 为 float64 类型\n",
    "\n",
    "无论是否在 cuda 上，在 numpy -> tensor -> numpy 的过程中，实际上执行了强制向下转型，相当于 double 强转 float，损失精度\n",
    "\n",
    "#### 证明\n",
    "\n",
    "* 看以上的 5 个 test\n",
    "    1. 就用原始 numpy\n",
    "    2. numpy transform 之后 转 tensor 再转回 numpy\n",
    "    3. numpy transform 之后 使用 torch.from_numpy() 转 tensor 再转回 numpy\n",
    "    4. numpy transform 之后 转 tensor 放到 cuda 上再放回 cpu 上 再转回 numpy\n",
    "    5. 模拟训练时的真实 dataloader\n",
    "\n",
    "    返回值为结果中 0 值的个数以及它的 dtype\n",
    "\n",
    "* 三组数据:\n",
    "    1. np.zeros(samples)\n",
    "    2. 我自己生成的 LA 的 y_test\n",
    "    3. generate_training_data.py 生成的 test.npz 中的 y\n",
    "\n",
    "* 期望的结果:\n",
    "\n",
    "    结果中 0 值的个数和一开始读取进来的原数据中 0 值个数相等\n",
    "\n",
    "* 错误结果：\n",
    "\n",
    "    结果中 0 值的个数为 0\n",
    "    \n",
    "* 结果\n",
    "\n",
    "    数据集 2，3 的 test 2 4 5 全挂，数据 1 玄学\n",
    "\n",
    "    数据 2，3 是可以稳定复现的，数据 1 因为牵扯到 random，会出现各种情况<br>\n",
    "    例如在上面的代码中我列出了 3 个 seed，分别能跑出三种不同的情况<br>\n",
    "    ```python\n",
    "    np.random.seed(666) # 64不能过 32能过\n",
    "    np.random.seed(233) # 64能过 32不能过\n",
    "    np.random.seed(2333) # 64和32都能过\n",
    "    ```\n",
    "    但是，当把 random 结果乘 100，也就是把随机数生成区间从 0~1 扩到 0~100 之后，这 5 个 test 就都能过了，猜测可能是一些超小的随机数在计算过程中 underflow 了<br>\n",
    "    不过也不能证明放大之后就一定行，也可能是我没找到特定的 seed\n",
    "\n",
    "    总之，数据1的结果涉及 random 的一些玄学，看来不可靠. 不过不重要，因为我们有可复现的两组确定的值\n",
    "\n",
    "* 补充\n",
    "\n",
    "    实际上数据3的结果也不算完全严谨，因为它和数据2用的是同一个scaler. 数据3在真实使用的过程中是没有任何问题的.<br>虽然都是0.2的测试集，但是2和3还是有一些小差异，结合上面的random玄学，或许就是这一点点差异导致了数据3在使用中没有碰到问题，但是并不代表问题不存在.<br>不过这并不影响这个实验结果的可信性，因为 scaler 本质就是一个线性变换和反变换，即使我把 mean 和 std 都设成随机数，也不会影响实验结论，因为问题本质是向下转型导致的精度丢失\n",
    "\n",
    "#### 特例\n",
    "\n",
    "`torch.from_numpy(arr)` 会创建 float64 类型的 tensor，然而没法用，因为模型里的参数都是 float32 类型的，不支持运算\n",
    "\n",
    "所以就会有 `torch.from_numpy(arr).float()` 这个写法了，`.float()` 的作用就是强转 float32\n",
    "\n",
    "综上，`torch.Tensor(arr) == torch.FloatTensor(arr) == torch.from_numpy(arr).float()` 这三者的返回值是一样的\n",
    "\n",
    "#### 结论\n",
    "\n",
    "1. 干脆别 transform y_true 就没这些破事了\n",
    "\n",
    "2. 当然还有一种选择是在读取原始 numpy 数组的时候就 `.astype(np.float32)` (已证明可行)，例如 `read_hdf(path).values.astype(np.float32)`<br>反正变 tensor 都要强转，不如从头开始转好，保证整个 data flow 中类型的一致性？\n",
    "\n",
    "再重新梳理一下整个过程，np.float64 的 0 先是被 transform 成了某个 np.float64 的值 m，然后在它转 tensor 的过程中变成了 torch.float32 类型，损失了精度. 因此，m 在 inverse_transform 的过程中很可能无法 map 回一个 np.float32 的 0. 然而，如果在开头就把这个 0 转成一个 np.float32 类型的 0，那么它 transform 成的 m 也就不会遭遇向下转型的问题，这个 m 一定可以 map 回一个 np.float32 的 0，和开头一致.\n",
    "\n",
    "对于方法1，虽然没有影响，但是 x 在 transform 之后转 tensor 的过程中也会客观存在向下转型的问题，只不过是不影响 y_pred\n",
    "\n",
    "因此，我认为方法2更能从本质上解决此问题\n",
    "\n",
    "当然，方法1+2应该更无懈可击\n",
    "\n",
    "#### 不要碰 sklearn 的 Scaler\n",
    "\n",
    "`sklearn.preprocessing.StandardScaler` 这位更是重量级，他连最基本的 test1 都过不了，下面请欣赏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.84217094e-14, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "trash_scaler=sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "np.random.seed(888)\n",
    "arr=np.zeros((10, 10))\n",
    "ran=np.random.random((10, 10))*1000 # 这里使劲放大 避免生成一些特别接近 0 的小数导致 underflow\n",
    "\n",
    "arr=np.concatenate((arr, ran))\n",
    "trash_scaler.fit(arr)\n",
    "\n",
    "trash_scaler.inverse_transform(trash_scaler.transform(arr))[:10] # 此结果可复现性跟 seed 有关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler3=StandardScaler(mean=arr.mean(), std=arr.std())\n",
    "scaler3.inverse_transform(scaler3.transform(arr))[:10] # 正常结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[859.56060606, 164.56949536, 483.47595916, 921.0272662 ,\n",
       "        428.55644132,  57.46009258, 925.00743443, 657.60153838,\n",
       "        132.95283812, 533.44892506],\n",
       "       [899.47760247, 248.36495841,  30.17181961,  72.44714713,\n",
       "        874.16449484, 558.43035064, 916.0473573 , 633.46044603,\n",
       "        283.25261043, 365.36880957],\n",
       "       [ 92.23385593, 372.51258294, 347.42277894, 705.17076602,\n",
       "        648.50903915,  40.90877305, 211.73176489,   1.48992314,\n",
       "        138.97165701, 211.82539406],\n",
       "       [ 26.09493297, 446.08735315, 239.1053078 , 954.4922192 ,\n",
       "        907.63182395, 862.49050287,  91.58743709, 977.45235499,\n",
       "        411.50139009, 458.30466961],\n",
       "       [525.90924761, 294.41554361, 972.11594431, 181.44419861,\n",
       "        303.40641894, 174.45413484, 527.56957655,  20.6929621 ,\n",
       "         63.54593412, 635.27231208],\n",
       "       [496.20334782,  14.1263981 , 627.22219182, 634.97506577,\n",
       "        108.14148996, 829.64259705, 517.75216617, 570.68344442,\n",
       "        546.33304626, 127.14921061],\n",
       "       [727.31796061, 940.10123979, 450.078109  , 876.50674311,\n",
       "        537.35564866, 495.68415078, 418.27208488, 851.00628476,\n",
       "        386.85271278, 606.89502955],\n",
       "       [217.84097166, 912.94433177, 658.43656035, 588.08589947,\n",
       "        188.62706265, 856.3980037 , 180.29326956, 948.51925776,\n",
       "        384.16340154, 251.3879254 ],\n",
       "       [967.4664386 , 770.48045352, 446.85195545, 198.13853631,\n",
       "        659.82266802, 230.24124558, 135.98433668, 601.44265305,\n",
       "        578.48927201, 856.2356408 ],\n",
       "       [357.64188766, 476.23814596, 545.92319952, 795.08298106,\n",
       "        144.62443292,  18.02918949, 385.32152866, 906.14554089,\n",
       "        866.29571122, 139.88735161]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran # 没有很小的数，不存在 underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239.5926788373776\n",
      "319.2670582859939\n"
     ]
    }
   ],
   "source": [
    "print(arr.mean())\n",
    "print(arr.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin=torch.nn.Linear(8, 16)\n",
    "\n",
    "lin.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm=torch.nn.LSTM(8, 16)\n",
    "\n",
    "lstm.weight_hh_l0.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb=torch.nn.Embedding(10, 8)\n",
    "\n",
    "emb.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randn=torch.randn(10, 10)\n",
    "\n",
    "randn.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2023.3.26 更新\n",
    "\n",
    "出大问题，貌似我上面做的关于 random 的实验，那三个种子三种不同情况是真的。0 transform 之后能不能 inverse 回 0 真就是玄学，和 float32 还是 64 无关。\n",
    "\n",
    "具体看下面 PEMS03 这个数据集，明明我转了 float32，但是在 numpy 这一步就挂了。。还没转 tensor 呢 没有什么强转丢精度问题。\n",
    "\n",
    "这样就体现出方法 1 的重要性了，千万别 transform y_true，就一定不会碰见这些恶心问题了。\n",
    "\n",
    "以及，我还是建议 `astype(float32)`，消除丢精度的问题（虽然现在看来没这么重要了。。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5258789e-05"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pems03=np.load(\"./data/PEMS03/PEMS03.npz\")[\"data\"].astype(np.float32)\n",
    "print(pems03.min())\n",
    "\n",
    "scaler4=StandardScaler(mean=pems03.mean(), std=pems03.std())\n",
    "\n",
    "trans03=scaler4.transform(pems03)\n",
    "inverse03=scaler4.inverse_transform(trans03)\n",
    "\n",
    "inverse03.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5258789e-05"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor03=torch.FloatTensor(trans03)\n",
    "tensor03_tonp=tensor03.numpy()\n",
    "print(tensor03_tonp.dtype)\n",
    "tensor03_tonp_inverse=scaler4.inverse_transform(tensor03_tonp)\n",
    "\n",
    "tensor03_tonp_inverse.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2473868, -1.2473868, -1.2473868, ..., -1.2473868, -1.2473868,\n",
       "       -1.2473868], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans03[pems03==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5258789e-05, 1.5258789e-05, 1.5258789e-05, ..., 1.5258789e-05,\n",
       "       1.5258789e-05, 1.5258789e-05], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse03[pems03==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2473868"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler4.transform(np.float32(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5258789e-05"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler4.inverse_transform(np.float32(-1.2473868))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2473868"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans03[pems03==0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.842170943040401e-14"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pems03=np.load(\"./data/PEMS03/PEMS03.npz\")[\"data\"].astype(np.float64)\n",
    "scaler4=StandardScaler(mean=pems03.mean(), std=pems03.std())\n",
    "\n",
    "trans03=scaler4.transform(pems03)\n",
    "inverse03=scaler4.inverse_transform(trans03)\n",
    "\n",
    "inverse03.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何解决 PEMS03 的这个问题：原数据保持 float64，mean 和 std 用 float32 算，然后莫名其妙就行了\n",
    "\n",
    "按照之前的尿性，这个方法铁定也是玄学，没有普适性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pems03=np.load(\"./data/PEMS03/PEMS03.npz\")[\"data\"].astype(np.float64)\n",
    "scaler4=StandardScaler(mean=pems03.astype(np.float32).mean(), std=pems03.astype(np.float32).std())\n",
    "\n",
    "trans03=scaler4.transform(pems03)\n",
    "inverse03=scaler4.inverse_transform(trans03)\n",
    "\n",
    "inverse03.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
